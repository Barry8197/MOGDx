{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2887404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Library Import \n",
      "\n",
      "Using cuda device\n",
      "Total = 6.4Gb \t Reserved = 0.0Gb \t Allocated = 0.0Gb\n",
      "StratifiedKFold(n_splits=10, random_state=None, shuffle=True)\n",
      "GCN_MME("
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Barry\\OneDrive - University of Edinburgh\\PhD_Research\\Year3\\Python\\MOGDx3.0\\MOGDx\\MOGDx.py\", line 266, in <module>\n",
      "    main(args)\n",
      "  File \"C:\\Users\\Barry\\OneDrive - University of Edinburgh\\PhD_Research\\Year3\\Python\\MOGDx3.0\\MOGDx\\MOGDx.py\", line 196, in main\n",
      "    pd.DataFrame({'Actual' :node_actual , 'Predicted' : node_predictions}).to_csv(args.output + '/Predictions.csv')\n",
      "NameError: name 'node_actual' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1076, num_edges=18312,\n",
      "      ndata_schemes={'idx': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(5,), dtype=torch.int64), 'feat': Scheme(shape=(30459,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.6634 | Train Acc. 0.1240 | Validation Acc. 0.0928 \n",
      "Epoch 00005 | Loss 0.9342 | Train Acc. 0.7997 | Validation Acc. 0.2216 \n",
      "Epoch 00010 | Loss 0.7450 | Train Acc. 0.8256 | Validation Acc. 0.0773 \n",
      "Epoch 00015 | Loss 0.6081 | Train Acc. 0.8463 | Validation Acc. 0.0773 \n",
      "Epoch 00020 | Loss 0.5238 | Train Acc. 0.8320 | Validation Acc. 0.4330 \n",
      "Epoch 00025 | Loss 0.4514 | Train Acc. 0.8540 | Validation Acc. 0.7165 \n",
      "Epoch 00030 | Loss 0.4195 | Train Acc. 0.8475 | Validation Acc. 0.7629 \n",
      "Epoch 00035 | Loss 0.3757 | Train Acc. 0.8605 | Validation Acc. 0.7629 \n",
      "Epoch 00040 | Loss 0.3646 | Train Acc. 0.8618 | Validation Acc. 0.7526 \n",
      "Epoch 00045 | Loss 0.3633 | Train Acc. 0.8605 | Validation Acc. 0.7526 \n",
      "Epoch 00050 | Loss 0.3457 | Train Acc. 0.8669 | Validation Acc. 0.7577 \n",
      "Epoch 00055 | Loss 0.3340 | Train Acc. 0.8669 | Validation Acc. 0.7732 \n",
      "Epoch 00060 | Loss 0.3446 | Train Acc. 0.8708 | Validation Acc. 0.7835 \n",
      "Epoch 00065 | Loss 0.3292 | Train Acc. 0.8682 | Validation Acc. 0.7732 \n",
      "Epoch 00070 | Loss 0.3174 | Train Acc. 0.8747 | Validation Acc. 0.7629 \n",
      "Epoch 00075 | Loss 0.3080 | Train Acc. 0.8811 | Validation Acc. 0.7577 \n",
      "Epoch 00080 | Loss 0.3102 | Train Acc. 0.8760 | Validation Acc. 0.7629 \n",
      "Epoch 00085 | Loss 0.3107 | Train Acc. 0.8773 | Validation Acc. 0.7526 \n",
      "Epoch 00090 | Loss 0.3185 | Train Acc. 0.8669 | Validation Acc. 0.7629 \n",
      "Epoch 00095 | Loss 0.3079 | Train Acc. 0.8630 | Validation Acc. 0.7423 \n",
      "Epoch 00100 | Loss 0.3059 | Train Acc. 0.8760 | Validation Acc. 0.7629 \n",
      "Epoch 00105 | Loss 0.3060 | Train Acc. 0.8708 | Validation Acc. 0.7732 \n",
      "Epoch 00110 | Loss 0.3006 | Train Acc. 0.8773 | Validation Acc. 0.8041 \n",
      "Epoch 00115 | Loss 0.3052 | Train Acc. 0.8876 | Validation Acc. 0.8144 \n",
      "Epoch 00120 | Loss 0.2999 | Train Acc. 0.8773 | Validation Acc. 0.8402 \n",
      "Epoch 00125 | Loss 0.2925 | Train Acc. 0.8798 | Validation Acc. 0.8505 \n",
      "Epoch 00130 | Loss 0.2907 | Train Acc. 0.8786 | Validation Acc. 0.8402 \n",
      "Epoch 00135 | Loss 0.2868 | Train Acc. 0.8824 | Validation Acc. 0.8351 \n",
      "Epoch 00140 | Loss 0.2832 | Train Acc. 0.8863 | Validation Acc. 0.8454 \n",
      "Epoch 00145 | Loss 0.2915 | Train Acc. 0.8786 | Validation Acc. 0.8505 \n",
      "Epoch 00150 | Loss 0.2816 | Train Acc. 0.8889 | Validation Acc. 0.8608 \n",
      "Epoch 00155 | Loss 0.2770 | Train Acc. 0.8953 | Validation Acc. 0.8814 \n",
      "Epoch 00160 | Loss 0.2752 | Train Acc. 0.8992 | Validation Acc. 0.8608 \n",
      "Epoch 00165 | Loss 0.2776 | Train Acc. 0.8863 | Validation Acc. 0.8505 \n",
      "Epoch 00170 | Loss 0.2737 | Train Acc. 0.8915 | Validation Acc. 0.8505 \n",
      "Epoch 00175 | Loss 0.2826 | Train Acc. 0.8811 | Validation Acc. 0.8402 \n",
      "Epoch 00180 | Loss 0.2803 | Train Acc. 0.8798 | Validation Acc. 0.8402 \n",
      "Epoch 00185 | Loss 0.2756 | Train Acc. 0.8915 | Validation Acc. 0.8351 \n",
      "Epoch 00190 | Loss 0.2682 | Train Acc. 0.8979 | Validation Acc. 0.8041 \n",
      "Epoch 00195 | Loss 0.2712 | Train Acc. 0.8915 | Validation Acc. 0.7887 \n",
      "Epoch 00200 | Loss 0.2721 | Train Acc. 0.8850 | Validation Acc. 0.7784 \n",
      "Epoch 00205 | Loss 0.2580 | Train Acc. 0.8992 | Validation Acc. 0.7887 \n",
      "Epoch 00210 | Loss 0.2696 | Train Acc. 0.8953 | Validation Acc. 0.8144 \n",
      "Epoch 00215 | Loss 0.2598 | Train Acc. 0.8941 | Validation Acc. 0.8351 \n",
      "Epoch 00220 | Loss 0.2585 | Train Acc. 0.8928 | Validation Acc. 0.8247 \n",
      "Epoch 00225 | Loss 0.2597 | Train Acc. 0.9005 | Validation Acc. 0.8351 \n",
      "Epoch 00230 | Loss 0.2723 | Train Acc. 0.8876 | Validation Acc. 0.8351 \n",
      "Epoch 00235 | Loss 0.2722 | Train Acc. 0.8863 | Validation Acc. 0.8402 \n",
      "Epoch 00240 | Loss 0.2532 | Train Acc. 0.9005 | Validation Acc. 0.8351 \n",
      "Epoch 00245 | Loss 0.2586 | Train Acc. 0.8902 | Validation Acc. 0.8351 \n",
      "Epoch 00250 | Loss 0.2568 | Train Acc. 0.8941 | Validation Acc. 0.8505 \n",
      "Epoch 00255 | Loss 0.2554 | Train Acc. 0.8941 | Validation Acc. 0.8454 \n",
      "Epoch 00260 | Loss 0.2775 | Train Acc. 0.8824 | Validation Acc. 0.8557 \n",
      "Epoch 00265 | Loss 0.2602 | Train Acc. 0.8953 | Validation Acc. 0.8608 \n",
      "Epoch 00270 | Loss 0.2555 | Train Acc. 0.9044 | Validation Acc. 0.8454 \n",
      "Epoch 00275 | Loss 0.2540 | Train Acc. 0.8992 | Validation Acc. 0.8557 \n",
      "Epoch 00280 | Loss 0.2569 | Train Acc. 0.8966 | Validation Acc. 0.8660 \n",
      "Epoch 00285 | Loss 0.2596 | Train Acc. 0.8953 | Validation Acc. 0.8454 \n",
      "Epoch 00290 | Loss 0.2652 | Train Acc. 0.8979 | Validation Acc. 0.8608 \n",
      "Epoch 00295 | Loss 0.2632 | Train Acc. 0.8966 | Validation Acc. 0.8557 \n",
      "Epoch 00300 | Loss 0.2568 | Train Acc. 0.8953 | Validation Acc. 0.8660 \n",
      "Epoch 00305 | Loss 0.2465 | Train Acc. 0.8979 | Validation Acc. 0.8711 \n",
      "Epoch 00310 | Loss 0.2490 | Train Acc. 0.9057 | Validation Acc. 0.8711 \n",
      "Epoch 00315 | Loss 0.2593 | Train Acc. 0.9005 | Validation Acc. 0.8711 \n",
      "Epoch 00320 | Loss 0.2466 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00325 | Loss 0.2535 | Train Acc. 0.9096 | Validation Acc. 0.8763 \n",
      "Epoch 00330 | Loss 0.2510 | Train Acc. 0.9005 | Validation Acc. 0.8711 \n",
      "Epoch 00335 | Loss 0.2557 | Train Acc. 0.8928 | Validation Acc. 0.8814 \n",
      "Epoch 00340 | Loss 0.2526 | Train Acc. 0.8941 | Validation Acc. 0.8763 \n",
      "Epoch 00345 | Loss 0.2423 | Train Acc. 0.9031 | Validation Acc. 0.8866 \n",
      "Epoch 00350 | Loss 0.2524 | Train Acc. 0.8966 | Validation Acc. 0.8814 \n",
      "Epoch 00355 | Loss 0.2580 | Train Acc. 0.9018 | Validation Acc. 0.8866 \n",
      "Epoch 00360 | Loss 0.2482 | Train Acc. 0.8941 | Validation Acc. 0.8969 \n",
      "Epoch 00365 | Loss 0.2525 | Train Acc. 0.9031 | Validation Acc. 0.8814 \n",
      "Epoch 00370 | Loss 0.2552 | Train Acc. 0.9070 | Validation Acc. 0.8814 \n",
      "Epoch 00375 | Loss 0.2510 | Train Acc. 0.8992 | Validation Acc. 0.8763 \n",
      "Epoch 00380 | Loss 0.2438 | Train Acc. 0.9070 | Validation Acc. 0.8711 \n",
      "Epoch 00385 | Loss 0.2585 | Train Acc. 0.8979 | Validation Acc. 0.8711 \n",
      "Epoch 00390 | Loss 0.2430 | Train Acc. 0.8992 | Validation Acc. 0.8866 \n",
      "Epoch 00395 | Loss 0.2528 | Train Acc. 0.8953 | Validation Acc. 0.8763 \n",
      "Epoch 00400 | Loss 0.2477 | Train Acc. 0.9070 | Validation Acc. 0.8866 \n",
      "Epoch 00405 | Loss 0.2610 | Train Acc. 0.9005 | Validation Acc. 0.8866 \n",
      "Epoch 00410 | Loss 0.2593 | Train Acc. 0.8966 | Validation Acc. 0.8918 \n",
      "Epoch 00415 | Loss 0.2530 | Train Acc. 0.9018 | Validation Acc. 0.8814 \n",
      "Epoch 00420 | Loss 0.2568 | Train Acc. 0.8915 | Validation Acc. 0.8866 \n",
      "Epoch 00425 | Loss 0.2504 | Train Acc. 0.8966 | Validation Acc. 0.8866 \n",
      "Epoch 00430 | Loss 0.2529 | Train Acc. 0.9005 | Validation Acc. 0.8866 \n",
      "Epoch 00435 | Loss 0.2503 | Train Acc. 0.8992 | Validation Acc. 0.8814 \n",
      "Epoch 00440 | Loss 0.2511 | Train Acc. 0.8928 | Validation Acc. 0.8763 \n",
      "Epoch 00445 | Loss 0.2560 | Train Acc. 0.9031 | Validation Acc. 0.8763 \n",
      "Epoch 00450 | Loss 0.2437 | Train Acc. 0.8992 | Validation Acc. 0.8763 \n",
      "Epoch 00455 | Loss 0.2635 | Train Acc. 0.8928 | Validation Acc. 0.8763 \n",
      "Epoch 00460 | Loss 0.2412 | Train Acc. 0.9031 | Validation Acc. 0.8711 \n",
      "Epoch 00465 | Loss 0.2495 | Train Acc. 0.8992 | Validation Acc. 0.8711 \n",
      "Epoch 00470 | Loss 0.2435 | Train Acc. 0.9005 | Validation Acc. 0.8763 \n",
      "Epoch 00475 | Loss 0.2534 | Train Acc. 0.8979 | Validation Acc. 0.8763 \n",
      "Epoch 00480 | Loss 0.2599 | Train Acc. 0.8902 | Validation Acc. 0.8969 \n",
      "Epoch 00485 | Loss 0.2494 | Train Acc. 0.8928 | Validation Acc. 0.8557 \n",
      "Epoch 00490 | Loss 0.2558 | Train Acc. 0.8953 | Validation Acc. 0.8918 \n",
      "Epoch 00495 | Loss 0.2415 | Train Acc. 0.9070 | Validation Acc. 0.8763 \n",
      "Epoch 00500 | Loss 0.2521 | Train Acc. 0.9044 | Validation Acc. 0.8814 \n",
      "Epoch 00505 | Loss 0.2548 | Train Acc. 0.9005 | Validation Acc. 0.8814 \n",
      "Epoch 00510 | Loss 0.2388 | Train Acc. 0.9057 | Validation Acc. 0.8711 \n",
      "Epoch 00515 | Loss 0.2562 | Train Acc. 0.8953 | Validation Acc. 0.8763 \n",
      "Epoch 00520 | Loss 0.2484 | Train Acc. 0.8966 | Validation Acc. 0.8763 \n",
      "Epoch 00525 | Loss 0.2457 | Train Acc. 0.9018 | Validation Acc. 0.8763 \n",
      "Epoch 00530 | Loss 0.2563 | Train Acc. 0.8941 | Validation Acc. 0.8608 \n",
      "Epoch 00535 | Loss 0.2554 | Train Acc. 0.8941 | Validation Acc. 0.8814 \n",
      "Epoch 00540 | Loss 0.2523 | Train Acc. 0.8941 | Validation Acc. 0.8814 \n",
      "Epoch 00545 | Loss 0.2545 | Train Acc. 0.8941 | Validation Acc. 0.8763 \n",
      "Epoch 00550 | Loss 0.2491 | Train Acc. 0.9018 | Validation Acc. 0.8866 \n",
      "Epoch 00555 | Loss 0.2502 | Train Acc. 0.8966 | Validation Acc. 0.8763 \n",
      "Epoch 00560 | Loss 0.2469 | Train Acc. 0.9005 | Validation Acc. 0.8763 \n",
      "Epoch 00565 | Loss 0.2384 | Train Acc. 0.9057 | Validation Acc. 0.8763 \n",
      "Epoch 00570 | Loss 0.2565 | Train Acc. 0.8941 | Validation Acc. 0.8711 \n",
      "Epoch 00575 | Loss 0.2484 | Train Acc. 0.8915 | Validation Acc. 0.8660 \n",
      "Epoch 00580 | Loss 0.2460 | Train Acc. 0.8966 | Validation Acc. 0.8814 \n",
      "Epoch 00585 | Loss 0.2446 | Train Acc. 0.9109 | Validation Acc. 0.8814 \n",
      "Epoch 00590 | Loss 0.2451 | Train Acc. 0.9005 | Validation Acc. 0.8711 \n",
      "Epoch 00595 | Loss 0.2443 | Train Acc. 0.9005 | Validation Acc. 0.8866 \n",
      "Epoch 00600 | Loss 0.2554 | Train Acc. 0.8928 | Validation Acc. 0.8763 \n",
      "Epoch 00605 | Loss 0.2447 | Train Acc. 0.8979 | Validation Acc. 0.8763 \n",
      "Epoch 00610 | Loss 0.2431 | Train Acc. 0.9070 | Validation Acc. 0.8814 \n",
      "Epoch 00615 | Loss 0.2521 | Train Acc. 0.8992 | Validation Acc. 0.8711 \n",
      "Epoch 00620 | Loss 0.2487 | Train Acc. 0.9005 | Validation Acc. 0.8918 \n",
      "Epoch 00625 | Loss 0.2410 | Train Acc. 0.9083 | Validation Acc. 0.8763 \n",
      "Epoch 00630 | Loss 0.2543 | Train Acc. 0.9057 | Validation Acc. 0.8866 \n",
      "Epoch 00635 | Loss 0.2351 | Train Acc. 0.9057 | Validation Acc. 0.8866 \n",
      "Epoch 00640 | Loss 0.2434 | Train Acc. 0.9044 | Validation Acc. 0.8660 \n",
      "Epoch 00645 | Loss 0.2436 | Train Acc. 0.9044 | Validation Acc. 0.8763 \n",
      "Epoch 00650 | Loss 0.2442 | Train Acc. 0.9057 | Validation Acc. 0.8763 \n",
      "Epoch 00655 | Loss 0.2511 | Train Acc. 0.8992 | Validation Acc. 0.8814 \n",
      "Epoch 00660 | Loss 0.2420 | Train Acc. 0.9018 | Validation Acc. 0.8711 \n",
      "Epoch 00665 | Loss 0.2441 | Train Acc. 0.9044 | Validation Acc. 0.8814 \n",
      "Epoch 00670 | Loss 0.2494 | Train Acc. 0.9005 | Validation Acc. 0.8969 \n",
      "Epoch 00675 | Loss 0.2585 | Train Acc. 0.9044 | Validation Acc. 0.8763 \n",
      "Epoch 00680 | Loss 0.2456 | Train Acc. 0.9044 | Validation Acc. 0.8814 \n",
      "Epoch 00685 | Loss 0.2432 | Train Acc. 0.9031 | Validation Acc. 0.8866 \n",
      "Epoch 00690 | Loss 0.2499 | Train Acc. 0.9044 | Validation Acc. 0.8814 \n",
      "Epoch 00695 | Loss 0.2472 | Train Acc. 0.9057 | Validation Acc. 0.8763 \n",
      "Epoch 00700 | Loss 0.2505 | Train Acc. 0.8979 | Validation Acc. 0.8763 \n",
      "Epoch 00705 | Loss 0.2558 | Train Acc. 0.9005 | Validation Acc. 0.8814 \n",
      "Epoch 00710 | Loss 0.2455 | Train Acc. 0.9044 | Validation Acc. 0.8763 \n",
      "Epoch 00715 | Loss 0.2545 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00720 | Loss 0.2539 | Train Acc. 0.9005 | Validation Acc. 0.8814 \n",
      "Epoch 00725 | Loss 0.2492 | Train Acc. 0.8992 | Validation Acc. 0.8814 \n",
      "Epoch 00730 | Loss 0.2452 | Train Acc. 0.8966 | Validation Acc. 0.8866 \n",
      "Epoch 00735 | Loss 0.2557 | Train Acc. 0.8953 | Validation Acc. 0.8866 \n",
      "Epoch 00740 | Loss 0.2560 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00745 | Loss 0.2448 | Train Acc. 0.8889 | Validation Acc. 0.8814 \n",
      "Epoch 00750 | Loss 0.2514 | Train Acc. 0.8953 | Validation Acc. 0.8763 \n",
      "Epoch 00755 | Loss 0.2501 | Train Acc. 0.8902 | Validation Acc. 0.8763 \n",
      "Epoch 00760 | Loss 0.2534 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00765 | Loss 0.2578 | Train Acc. 0.9018 | Validation Acc. 0.8711 \n",
      "Epoch 00770 | Loss 0.2443 | Train Acc. 0.9057 | Validation Acc. 0.8660 \n",
      "Epoch 00775 | Loss 0.2423 | Train Acc. 0.9070 | Validation Acc. 0.8660 \n",
      "Epoch 00780 | Loss 0.2466 | Train Acc. 0.9005 | Validation Acc. 0.8866 \n",
      "Epoch 00785 | Loss 0.2518 | Train Acc. 0.8979 | Validation Acc. 0.8660 \n",
      "Epoch 00790 | Loss 0.2533 | Train Acc. 0.8979 | Validation Acc. 0.8763 \n",
      "Epoch 00795 | Loss 0.2398 | Train Acc. 0.9005 | Validation Acc. 0.8866 \n",
      "Epoch 00800 | Loss 0.2450 | Train Acc. 0.9031 | Validation Acc. 0.8866 \n",
      "Epoch 00805 | Loss 0.2417 | Train Acc. 0.9057 | Validation Acc. 0.8866 \n",
      "Epoch 00810 | Loss 0.2493 | Train Acc. 0.9096 | Validation Acc. 0.8660 \n",
      "Epoch 00815 | Loss 0.2645 | Train Acc. 0.9018 | Validation Acc. 0.8660 \n",
      "Epoch 00820 | Loss 0.2488 | Train Acc. 0.9018 | Validation Acc. 0.8814 \n",
      "Epoch 00825 | Loss 0.2527 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 00830 | Loss 0.2530 | Train Acc. 0.8915 | Validation Acc. 0.8711 \n",
      "Epoch 00835 | Loss 0.2438 | Train Acc. 0.9070 | Validation Acc. 0.8814 \n",
      "Epoch 00840 | Loss 0.2443 | Train Acc. 0.9018 | Validation Acc. 0.8660 \n",
      "Epoch 00845 | Loss 0.2390 | Train Acc. 0.9083 | Validation Acc. 0.8814 \n",
      "Epoch 00850 | Loss 0.2538 | Train Acc. 0.8915 | Validation Acc. 0.8711 \n",
      "Epoch 00855 | Loss 0.2490 | Train Acc. 0.9005 | Validation Acc. 0.8711 \n",
      "Epoch 00860 | Loss 0.2446 | Train Acc. 0.9018 | Validation Acc. 0.8814 \n",
      "Epoch 00865 | Loss 0.2466 | Train Acc. 0.9005 | Validation Acc. 0.8711 \n",
      "Epoch 00870 | Loss 0.2487 | Train Acc. 0.9083 | Validation Acc. 0.8763 \n",
      "Epoch 00875 | Loss 0.2457 | Train Acc. 0.9109 | Validation Acc. 0.8711 \n",
      "Epoch 00880 | Loss 0.2482 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00885 | Loss 0.2556 | Train Acc. 0.8979 | Validation Acc. 0.8763 \n",
      "Epoch 00890 | Loss 0.2501 | Train Acc. 0.8992 | Validation Acc. 0.8866 \n",
      "Epoch 00895 | Loss 0.2492 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 00900 | Loss 0.2480 | Train Acc. 0.8992 | Validation Acc. 0.8866 \n",
      "Epoch 00905 | Loss 0.2476 | Train Acc. 0.9083 | Validation Acc. 0.8918 \n",
      "Epoch 00910 | Loss 0.2451 | Train Acc. 0.9109 | Validation Acc. 0.8814 \n",
      "Epoch 00915 | Loss 0.2576 | Train Acc. 0.8979 | Validation Acc. 0.8660 \n",
      "Epoch 00920 | Loss 0.2499 | Train Acc. 0.9096 | Validation Acc. 0.8763 \n",
      "Epoch 00925 | Loss 0.2535 | Train Acc. 0.8979 | Validation Acc. 0.8660 \n",
      "Epoch 00930 | Loss 0.2548 | Train Acc. 0.8966 | Validation Acc. 0.8814 \n",
      "Epoch 00935 | Loss 0.2445 | Train Acc. 0.9134 | Validation Acc. 0.8814 \n",
      "Epoch 00940 | Loss 0.2474 | Train Acc. 0.9018 | Validation Acc. 0.8866 \n",
      "Epoch 00945 | Loss 0.2449 | Train Acc. 0.8979 | Validation Acc. 0.8969 \n",
      "Epoch 00950 | Loss 0.2428 | Train Acc. 0.9005 | Validation Acc. 0.8660 \n",
      "Epoch 00955 | Loss 0.2550 | Train Acc. 0.9018 | Validation Acc. 0.8814 \n",
      "Epoch 00960 | Loss 0.2468 | Train Acc. 0.9044 | Validation Acc. 0.8711 \n",
      "Epoch 00965 | Loss 0.2361 | Train Acc. 0.9109 | Validation Acc. 0.8608 \n",
      "Epoch 00970 | Loss 0.2401 | Train Acc. 0.8979 | Validation Acc. 0.8711 \n",
      "Epoch 00975 | Loss 0.2474 | Train Acc. 0.8979 | Validation Acc. 0.8918 \n",
      "Epoch 00980 | Loss 0.2507 | Train Acc. 0.9044 | Validation Acc. 0.8660 \n",
      "Epoch 00985 | Loss 0.2487 | Train Acc. 0.8928 | Validation Acc. 0.8711 \n",
      "Epoch 00990 | Loss 0.2474 | Train Acc. 0.8941 | Validation Acc. 0.8763 \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "Fold : 1 | Test Accuracy = 0.8426 | F1 = 0.8096 \n",
      "Total = 6.4Gb \t Reserved = 1.0Gb \t Allocated = 0.3Gb\n",
      "Clearing gpu memory\n",
      "Total = 6.4Gb \t Reserved = 0.3Gb \t Allocated = 0.3Gb\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1076, num_edges=18312,\n",
      "      ndata_schemes={'idx': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(5,), dtype=torch.int64), 'feat': Scheme(shape=(30459,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.6874 | Train Acc. 0.1150 | Validation Acc. 0.0361 \n",
      "Epoch 00005 | Loss 0.8414 | Train Acc. 0.8165 | Validation Acc. 0.2680 \n",
      "Epoch 00010 | Loss 0.6732 | Train Acc. 0.8230 | Validation Acc. 0.7010 \n",
      "Epoch 00015 | Loss 0.5602 | Train Acc. 0.8411 | Validation Acc. 0.7732 \n",
      "Epoch 00020 | Loss 0.5080 | Train Acc. 0.8424 | Validation Acc. 0.7577 \n",
      "Epoch 00025 | Loss 0.4434 | Train Acc. 0.8501 | Validation Acc. 0.7423 \n",
      "Epoch 00030 | Loss 0.4150 | Train Acc. 0.8475 | Validation Acc. 0.7680 \n",
      "Epoch 00035 | Loss 0.3881 | Train Acc. 0.8605 | Validation Acc. 0.7732 \n",
      "Epoch 00040 | Loss 0.3727 | Train Acc. 0.8630 | Validation Acc. 0.7680 \n",
      "Epoch 00045 | Loss 0.3688 | Train Acc. 0.8553 | Validation Acc. 0.7577 \n",
      "Epoch 00050 | Loss 0.3527 | Train Acc. 0.8630 | Validation Acc. 0.7526 \n",
      "Epoch 00055 | Loss 0.3686 | Train Acc. 0.8579 | Validation Acc. 0.7474 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00060 | Loss 0.3417 | Train Acc. 0.8592 | Validation Acc. 0.7990 \n",
      "Epoch 00065 | Loss 0.3489 | Train Acc. 0.8656 | Validation Acc. 0.7835 \n",
      "Epoch 00070 | Loss 0.3379 | Train Acc. 0.8618 | Validation Acc. 0.7526 \n",
      "Epoch 00075 | Loss 0.3286 | Train Acc. 0.8708 | Validation Acc. 0.7474 \n",
      "Epoch 00080 | Loss 0.3433 | Train Acc. 0.8656 | Validation Acc. 0.7165 \n",
      "Epoch 00085 | Loss 0.3252 | Train Acc. 0.8747 | Validation Acc. 0.7320 \n",
      "Epoch 00090 | Loss 0.3230 | Train Acc. 0.8669 | Validation Acc. 0.7320 \n",
      "Epoch 00095 | Loss 0.3196 | Train Acc. 0.8760 | Validation Acc. 0.7577 \n",
      "Epoch 00100 | Loss 0.3141 | Train Acc. 0.8773 | Validation Acc. 0.7474 \n",
      "Epoch 00105 | Loss 0.3241 | Train Acc. 0.8721 | Validation Acc. 0.7990 \n",
      "Epoch 00110 | Loss 0.3172 | Train Acc. 0.8798 | Validation Acc. 0.7990 \n",
      "Epoch 00115 | Loss 0.3147 | Train Acc. 0.8760 | Validation Acc. 0.8351 \n",
      "Epoch 00120 | Loss 0.3116 | Train Acc. 0.8786 | Validation Acc. 0.8402 \n",
      "Epoch 00125 | Loss 0.3154 | Train Acc. 0.8811 | Validation Acc. 0.8247 \n",
      "Epoch 00130 | Loss 0.3057 | Train Acc. 0.8811 | Validation Acc. 0.8402 \n",
      "Epoch 00135 | Loss 0.3043 | Train Acc. 0.8889 | Validation Acc. 0.8454 \n",
      "Epoch 00140 | Loss 0.3103 | Train Acc. 0.8760 | Validation Acc. 0.8505 \n",
      "Epoch 00145 | Loss 0.3025 | Train Acc. 0.8837 | Validation Acc. 0.8454 \n",
      "Epoch 00150 | Loss 0.2933 | Train Acc. 0.8863 | Validation Acc. 0.8402 \n",
      "Epoch 00155 | Loss 0.2904 | Train Acc. 0.8941 | Validation Acc. 0.8608 \n",
      "Epoch 00160 | Loss 0.2975 | Train Acc. 0.8824 | Validation Acc. 0.8608 \n",
      "Epoch 00165 | Loss 0.2981 | Train Acc. 0.8798 | Validation Acc. 0.8711 \n",
      "Epoch 00170 | Loss 0.3034 | Train Acc. 0.8798 | Validation Acc. 0.8505 \n",
      "Epoch 00175 | Loss 0.2927 | Train Acc. 0.8876 | Validation Acc. 0.8660 \n",
      "Epoch 00180 | Loss 0.2905 | Train Acc. 0.8941 | Validation Acc. 0.8608 \n",
      "Epoch 00185 | Loss 0.2876 | Train Acc. 0.8928 | Validation Acc. 0.8505 \n",
      "Epoch 00190 | Loss 0.2885 | Train Acc. 0.8850 | Validation Acc. 0.8454 \n",
      "Epoch 00195 | Loss 0.2785 | Train Acc. 0.8889 | Validation Acc. 0.8247 \n",
      "Epoch 00200 | Loss 0.2856 | Train Acc. 0.8889 | Validation Acc. 0.8454 \n",
      "Epoch 00205 | Loss 0.2896 | Train Acc. 0.9031 | Validation Acc. 0.8608 \n",
      "Epoch 00210 | Loss 0.2830 | Train Acc. 0.8837 | Validation Acc. 0.8711 \n",
      "Epoch 00215 | Loss 0.2799 | Train Acc. 0.8863 | Validation Acc. 0.8660 \n",
      "Epoch 00220 | Loss 0.2807 | Train Acc. 0.8837 | Validation Acc. 0.8505 \n",
      "Epoch 00225 | Loss 0.2781 | Train Acc. 0.8876 | Validation Acc. 0.8763 \n",
      "Epoch 00230 | Loss 0.2879 | Train Acc. 0.8979 | Validation Acc. 0.8557 \n",
      "Epoch 00235 | Loss 0.2833 | Train Acc. 0.8928 | Validation Acc. 0.8454 \n",
      "Epoch 00240 | Loss 0.2764 | Train Acc. 0.8928 | Validation Acc. 0.8660 \n",
      "Epoch 00245 | Loss 0.2744 | Train Acc. 0.8915 | Validation Acc. 0.8505 \n",
      "Epoch 00250 | Loss 0.2703 | Train Acc. 0.8941 | Validation Acc. 0.8557 \n",
      "Epoch 00255 | Loss 0.2869 | Train Acc. 0.8941 | Validation Acc. 0.8814 \n",
      "Epoch 00260 | Loss 0.2711 | Train Acc. 0.8966 | Validation Acc. 0.8711 \n",
      "Epoch 00265 | Loss 0.2811 | Train Acc. 0.8889 | Validation Acc. 0.8608 \n",
      "Epoch 00270 | Loss 0.2686 | Train Acc. 0.9018 | Validation Acc. 0.8711 \n",
      "Epoch 00275 | Loss 0.2641 | Train Acc. 0.8941 | Validation Acc. 0.8814 \n",
      "Epoch 00280 | Loss 0.2743 | Train Acc. 0.8850 | Validation Acc. 0.8660 \n",
      "Epoch 00285 | Loss 0.2663 | Train Acc. 0.8979 | Validation Acc. 0.8711 \n",
      "Epoch 00290 | Loss 0.2736 | Train Acc. 0.9005 | Validation Acc. 0.8505 \n",
      "Epoch 00295 | Loss 0.2718 | Train Acc. 0.8966 | Validation Acc. 0.8763 \n",
      "Epoch 00300 | Loss 0.2679 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00305 | Loss 0.2645 | Train Acc. 0.8902 | Validation Acc. 0.8557 \n",
      "Epoch 00310 | Loss 0.2621 | Train Acc. 0.8850 | Validation Acc. 0.8608 \n",
      "Epoch 00315 | Loss 0.2605 | Train Acc. 0.8979 | Validation Acc. 0.8660 \n",
      "Epoch 00320 | Loss 0.2700 | Train Acc. 0.8992 | Validation Acc. 0.8557 \n",
      "Epoch 00325 | Loss 0.2644 | Train Acc. 0.8979 | Validation Acc. 0.8608 \n",
      "Epoch 00330 | Loss 0.2733 | Train Acc. 0.8850 | Validation Acc. 0.8608 \n",
      "Epoch 00335 | Loss 0.2774 | Train Acc. 0.8863 | Validation Acc. 0.8711 \n",
      "Epoch 00340 | Loss 0.2658 | Train Acc. 0.8966 | Validation Acc. 0.8711 \n",
      "Epoch 00345 | Loss 0.2587 | Train Acc. 0.9005 | Validation Acc. 0.8608 \n",
      "Epoch 00350 | Loss 0.2655 | Train Acc. 0.8824 | Validation Acc. 0.8660 \n",
      "Epoch 00355 | Loss 0.2606 | Train Acc. 0.9044 | Validation Acc. 0.8763 \n",
      "Epoch 00360 | Loss 0.2696 | Train Acc. 0.8941 | Validation Acc. 0.8763 \n",
      "Epoch 00365 | Loss 0.2601 | Train Acc. 0.9005 | Validation Acc. 0.8505 \n",
      "Epoch 00370 | Loss 0.2596 | Train Acc. 0.9018 | Validation Acc. 0.8711 \n",
      "Epoch 00375 | Loss 0.2655 | Train Acc. 0.8863 | Validation Acc. 0.8711 \n",
      "Epoch 00380 | Loss 0.2577 | Train Acc. 0.9044 | Validation Acc. 0.8660 \n",
      "Epoch 00385 | Loss 0.2674 | Train Acc. 0.8915 | Validation Acc. 0.8608 \n",
      "Epoch 00390 | Loss 0.2634 | Train Acc. 0.8928 | Validation Acc. 0.8866 \n",
      "Epoch 00395 | Loss 0.2603 | Train Acc. 0.8915 | Validation Acc. 0.8711 \n",
      "Epoch 00400 | Loss 0.2591 | Train Acc. 0.9018 | Validation Acc. 0.8557 \n",
      "Epoch 00405 | Loss 0.2635 | Train Acc. 0.9057 | Validation Acc. 0.8608 \n",
      "Epoch 00410 | Loss 0.2663 | Train Acc. 0.9005 | Validation Acc. 0.8505 \n",
      "Epoch 00415 | Loss 0.2629 | Train Acc. 0.8992 | Validation Acc. 0.8557 \n",
      "Epoch 00420 | Loss 0.2657 | Train Acc. 0.8992 | Validation Acc. 0.8660 \n",
      "Epoch 00425 | Loss 0.2578 | Train Acc. 0.9005 | Validation Acc. 0.8711 \n",
      "Epoch 00430 | Loss 0.2641 | Train Acc. 0.8979 | Validation Acc. 0.8660 \n",
      "Epoch 00435 | Loss 0.2703 | Train Acc. 0.9018 | Validation Acc. 0.8660 \n",
      "Epoch 00440 | Loss 0.2602 | Train Acc. 0.9044 | Validation Acc. 0.8711 \n",
      "Epoch 00445 | Loss 0.2616 | Train Acc. 0.9005 | Validation Acc. 0.8711 \n",
      "Epoch 00450 | Loss 0.2595 | Train Acc. 0.8953 | Validation Acc. 0.8660 \n",
      "Epoch 00455 | Loss 0.2562 | Train Acc. 0.9083 | Validation Acc. 0.8505 \n",
      "Epoch 00460 | Loss 0.2543 | Train Acc. 0.9018 | Validation Acc. 0.8763 \n",
      "Epoch 00465 | Loss 0.2619 | Train Acc. 0.8915 | Validation Acc. 0.8608 \n",
      "Epoch 00470 | Loss 0.2687 | Train Acc. 0.8889 | Validation Acc. 0.8608 \n",
      "Epoch 00475 | Loss 0.2550 | Train Acc. 0.8966 | Validation Acc. 0.8660 \n",
      "Epoch 00480 | Loss 0.2587 | Train Acc. 0.9018 | Validation Acc. 0.8660 \n",
      "Epoch 00485 | Loss 0.2649 | Train Acc. 0.8941 | Validation Acc. 0.8711 \n",
      "Epoch 00490 | Loss 0.2700 | Train Acc. 0.8966 | Validation Acc. 0.8608 \n",
      "Epoch 00495 | Loss 0.2644 | Train Acc. 0.8966 | Validation Acc. 0.8763 \n",
      "Epoch 00500 | Loss 0.2570 | Train Acc. 0.9005 | Validation Acc. 0.8763 \n",
      "Epoch 00505 | Loss 0.2667 | Train Acc. 0.8992 | Validation Acc. 0.8660 \n",
      "Epoch 00510 | Loss 0.2610 | Train Acc. 0.8915 | Validation Acc. 0.8608 \n",
      "Epoch 00515 | Loss 0.2754 | Train Acc. 0.8889 | Validation Acc. 0.8660 \n",
      "Epoch 00520 | Loss 0.2598 | Train Acc. 0.8941 | Validation Acc. 0.8557 \n",
      "Epoch 00525 | Loss 0.2605 | Train Acc. 0.9031 | Validation Acc. 0.8660 \n",
      "Epoch 00530 | Loss 0.2572 | Train Acc. 0.9018 | Validation Acc. 0.8557 \n",
      "Epoch 00535 | Loss 0.2620 | Train Acc. 0.9057 | Validation Acc. 0.8608 \n",
      "Epoch 00540 | Loss 0.2555 | Train Acc. 0.9057 | Validation Acc. 0.8711 \n",
      "Epoch 00545 | Loss 0.2666 | Train Acc. 0.8953 | Validation Acc. 0.8608 \n",
      "Epoch 00550 | Loss 0.2533 | Train Acc. 0.9057 | Validation Acc. 0.8608 \n",
      "Epoch 00555 | Loss 0.2587 | Train Acc. 0.8953 | Validation Acc. 0.8660 \n",
      "Epoch 00560 | Loss 0.2626 | Train Acc. 0.8889 | Validation Acc. 0.8608 \n",
      "Epoch 00565 | Loss 0.2619 | Train Acc. 0.8953 | Validation Acc. 0.8557 \n",
      "Epoch 00570 | Loss 0.2599 | Train Acc. 0.9044 | Validation Acc. 0.8660 \n",
      "Epoch 00575 | Loss 0.2499 | Train Acc. 0.9044 | Validation Acc. 0.8505 \n",
      "Epoch 00580 | Loss 0.2638 | Train Acc. 0.8941 | Validation Acc. 0.8557 \n",
      "Epoch 00585 | Loss 0.2661 | Train Acc. 0.8966 | Validation Acc. 0.8557 \n",
      "Epoch 00590 | Loss 0.2604 | Train Acc. 0.8953 | Validation Acc. 0.8557 \n",
      "Epoch 00595 | Loss 0.2627 | Train Acc. 0.9005 | Validation Acc. 0.8660 \n",
      "Epoch 00600 | Loss 0.2671 | Train Acc. 0.9018 | Validation Acc. 0.8660 \n",
      "Epoch 00605 | Loss 0.2603 | Train Acc. 0.8992 | Validation Acc. 0.8660 \n",
      "Epoch 00610 | Loss 0.2598 | Train Acc. 0.8979 | Validation Acc. 0.8711 \n",
      "Epoch 00615 | Loss 0.2620 | Train Acc. 0.8992 | Validation Acc. 0.8660 \n",
      "Epoch 00620 | Loss 0.2617 | Train Acc. 0.8941 | Validation Acc. 0.8660 \n",
      "Epoch 00625 | Loss 0.2604 | Train Acc. 0.8966 | Validation Acc. 0.8608 \n",
      "Epoch 00630 | Loss 0.2600 | Train Acc. 0.9121 | Validation Acc. 0.8608 \n",
      "Epoch 00635 | Loss 0.2614 | Train Acc. 0.9005 | Validation Acc. 0.8608 \n",
      "Epoch 00640 | Loss 0.2542 | Train Acc. 0.8979 | Validation Acc. 0.8660 \n",
      "Epoch 00645 | Loss 0.2587 | Train Acc. 0.8953 | Validation Acc. 0.8660 \n",
      "Epoch 00650 | Loss 0.2604 | Train Acc. 0.9031 | Validation Acc. 0.8660 \n",
      "Epoch 00655 | Loss 0.2647 | Train Acc. 0.9057 | Validation Acc. 0.8711 \n",
      "Epoch 00660 | Loss 0.2581 | Train Acc. 0.8966 | Validation Acc. 0.8711 \n",
      "Epoch 00665 | Loss 0.2549 | Train Acc. 0.9018 | Validation Acc. 0.8711 \n",
      "Epoch 00670 | Loss 0.2623 | Train Acc. 0.8992 | Validation Acc. 0.8660 \n",
      "Epoch 00675 | Loss 0.2691 | Train Acc. 0.8941 | Validation Acc. 0.8660 \n",
      "Epoch 00680 | Loss 0.2617 | Train Acc. 0.9044 | Validation Acc. 0.8711 \n",
      "Epoch 00685 | Loss 0.2630 | Train Acc. 0.9044 | Validation Acc. 0.8608 \n",
      "Epoch 00690 | Loss 0.2557 | Train Acc. 0.8902 | Validation Acc. 0.8608 \n",
      "Epoch 00695 | Loss 0.2606 | Train Acc. 0.8966 | Validation Acc. 0.8660 \n",
      "Epoch 00700 | Loss 0.2611 | Train Acc. 0.8992 | Validation Acc. 0.8557 \n",
      "Epoch 00705 | Loss 0.2591 | Train Acc. 0.8902 | Validation Acc. 0.8711 \n",
      "Epoch 00710 | Loss 0.2638 | Train Acc. 0.8902 | Validation Acc. 0.8557 \n",
      "Epoch 00715 | Loss 0.2670 | Train Acc. 0.8928 | Validation Acc. 0.8711 \n",
      "Epoch 00720 | Loss 0.2629 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00725 | Loss 0.2544 | Train Acc. 0.9031 | Validation Acc. 0.8660 \n",
      "Epoch 00730 | Loss 0.2605 | Train Acc. 0.8915 | Validation Acc. 0.8505 \n",
      "Epoch 00735 | Loss 0.2611 | Train Acc. 0.9018 | Validation Acc. 0.8660 \n",
      "Epoch 00740 | Loss 0.2569 | Train Acc. 0.8979 | Validation Acc. 0.8660 \n",
      "Epoch 00745 | Loss 0.2606 | Train Acc. 0.9018 | Validation Acc. 0.8660 \n",
      "Epoch 00750 | Loss 0.2517 | Train Acc. 0.9057 | Validation Acc. 0.8711 \n",
      "Epoch 00755 | Loss 0.2568 | Train Acc. 0.8979 | Validation Acc. 0.8763 \n",
      "Epoch 00760 | Loss 0.2613 | Train Acc. 0.8915 | Validation Acc. 0.8711 \n",
      "Epoch 00765 | Loss 0.2596 | Train Acc. 0.8966 | Validation Acc. 0.8608 \n",
      "Epoch 00770 | Loss 0.2649 | Train Acc. 0.8941 | Validation Acc. 0.8608 \n",
      "Epoch 00775 | Loss 0.2626 | Train Acc. 0.9005 | Validation Acc. 0.8557 \n",
      "Epoch 00780 | Loss 0.2645 | Train Acc. 0.8941 | Validation Acc. 0.8660 \n",
      "Epoch 00785 | Loss 0.2592 | Train Acc. 0.8941 | Validation Acc. 0.8711 \n",
      "Epoch 00790 | Loss 0.2599 | Train Acc. 0.8966 | Validation Acc. 0.8660 \n",
      "Epoch 00795 | Loss 0.2690 | Train Acc. 0.8928 | Validation Acc. 0.8711 \n",
      "Epoch 00800 | Loss 0.2558 | Train Acc. 0.9070 | Validation Acc. 0.8557 \n",
      "Epoch 00805 | Loss 0.2567 | Train Acc. 0.9070 | Validation Acc. 0.8608 \n",
      "Epoch 00810 | Loss 0.2653 | Train Acc. 0.8966 | Validation Acc. 0.8711 \n",
      "Epoch 00815 | Loss 0.2641 | Train Acc. 0.9031 | Validation Acc. 0.8763 \n",
      "Epoch 00820 | Loss 0.2636 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00825 | Loss 0.2569 | Train Acc. 0.9070 | Validation Acc. 0.8608 \n",
      "Epoch 00830 | Loss 0.2676 | Train Acc. 0.9005 | Validation Acc. 0.8660 \n",
      "Epoch 00835 | Loss 0.2565 | Train Acc. 0.8966 | Validation Acc. 0.8660 \n",
      "Epoch 00840 | Loss 0.2614 | Train Acc. 0.9005 | Validation Acc. 0.8660 \n",
      "Epoch 00845 | Loss 0.2604 | Train Acc. 0.8992 | Validation Acc. 0.8557 \n",
      "Epoch 00850 | Loss 0.2599 | Train Acc. 0.8992 | Validation Acc. 0.8763 \n",
      "Epoch 00855 | Loss 0.2571 | Train Acc. 0.8992 | Validation Acc. 0.8660 \n",
      "Epoch 00860 | Loss 0.2596 | Train Acc. 0.9031 | Validation Acc. 0.8763 \n",
      "Epoch 00865 | Loss 0.2618 | Train Acc. 0.8966 | Validation Acc. 0.8608 \n",
      "Epoch 00870 | Loss 0.2655 | Train Acc. 0.8953 | Validation Acc. 0.8557 \n",
      "Epoch 00875 | Loss 0.2574 | Train Acc. 0.8953 | Validation Acc. 0.8660 \n",
      "Epoch 00880 | Loss 0.2599 | Train Acc. 0.9005 | Validation Acc. 0.8557 \n",
      "Epoch 00885 | Loss 0.2570 | Train Acc. 0.8979 | Validation Acc. 0.8711 \n",
      "Epoch 00890 | Loss 0.2604 | Train Acc. 0.9005 | Validation Acc. 0.8608 \n",
      "Epoch 00895 | Loss 0.2631 | Train Acc. 0.9057 | Validation Acc. 0.8660 \n",
      "Epoch 00900 | Loss 0.2573 | Train Acc. 0.9070 | Validation Acc. 0.8660 \n",
      "Epoch 00905 | Loss 0.2619 | Train Acc. 0.8915 | Validation Acc. 0.8711 \n",
      "Epoch 00910 | Loss 0.2680 | Train Acc. 0.8773 | Validation Acc. 0.8763 \n",
      "Epoch 00915 | Loss 0.2674 | Train Acc. 0.8966 | Validation Acc. 0.8660 \n",
      "Epoch 00920 | Loss 0.2676 | Train Acc. 0.8915 | Validation Acc. 0.8557 \n",
      "Epoch 00925 | Loss 0.2651 | Train Acc. 0.8979 | Validation Acc. 0.8557 \n",
      "Epoch 00930 | Loss 0.2550 | Train Acc. 0.9083 | Validation Acc. 0.8660 \n",
      "Epoch 00935 | Loss 0.2551 | Train Acc. 0.9005 | Validation Acc. 0.8608 \n",
      "Epoch 00940 | Loss 0.2601 | Train Acc. 0.8992 | Validation Acc. 0.8608 \n",
      "Epoch 00945 | Loss 0.2558 | Train Acc. 0.9031 | Validation Acc. 0.8711 \n",
      "Epoch 00950 | Loss 0.2657 | Train Acc. 0.8928 | Validation Acc. 0.8711 \n",
      "Epoch 00955 | Loss 0.2589 | Train Acc. 0.9031 | Validation Acc. 0.8608 \n",
      "Epoch 00960 | Loss 0.2666 | Train Acc. 0.8928 | Validation Acc. 0.8608 \n",
      "Epoch 00965 | Loss 0.2622 | Train Acc. 0.8928 | Validation Acc. 0.8814 \n",
      "Epoch 00970 | Loss 0.2557 | Train Acc. 0.9031 | Validation Acc. 0.8763 \n",
      "Epoch 00975 | Loss 0.2692 | Train Acc. 0.8953 | Validation Acc. 0.8608 \n",
      "Epoch 00980 | Loss 0.2649 | Train Acc. 0.9057 | Validation Acc. 0.8711 \n",
      "Epoch 00985 | Loss 0.2600 | Train Acc. 0.8928 | Validation Acc. 0.8660 \n",
      "Epoch 00990 | Loss 0.2650 | Train Acc. 0.8889 | Validation Acc. 0.8711 \n",
      "Epoch 00995 | Loss 0.2697 | Train Acc. 0.8902 | Validation Acc. 0.8763 \n",
      "Epoch 01000 | Loss 0.2585 | Train Acc. 0.9031 | Validation Acc. 0.8711 \n",
      "Epoch 01005 | Loss 0.2578 | Train Acc. 0.9070 | Validation Acc. 0.8557 \n",
      "Epoch 01010 | Loss 0.2542 | Train Acc. 0.9018 | Validation Acc. 0.8660 \n",
      "Epoch 01015 | Loss 0.2592 | Train Acc. 0.8941 | Validation Acc. 0.8660 \n",
      "Epoch 01020 | Loss 0.2652 | Train Acc. 0.8941 | Validation Acc. 0.8660 \n",
      "Epoch 01025 | Loss 0.2625 | Train Acc. 0.8953 | Validation Acc. 0.8505 \n",
      "Epoch 01030 | Loss 0.2677 | Train Acc. 0.8928 | Validation Acc. 0.8711 \n",
      "Epoch 01035 | Loss 0.2582 | Train Acc. 0.8902 | Validation Acc. 0.8608 \n",
      "Epoch 01040 | Loss 0.2619 | Train Acc. 0.9031 | Validation Acc. 0.8660 \n",
      "Epoch 01045 | Loss 0.2555 | Train Acc. 0.9018 | Validation Acc. 0.8660 \n",
      "Epoch 01050 | Loss 0.2634 | Train Acc. 0.9031 | Validation Acc. 0.8505 \n",
      "Epoch 01055 | Loss 0.2670 | Train Acc. 0.8915 | Validation Acc. 0.8557 \n",
      "Epoch 01060 | Loss 0.2528 | Train Acc. 0.9070 | Validation Acc. 0.8660 \n",
      "Epoch 01065 | Loss 0.2569 | Train Acc. 0.9057 | Validation Acc. 0.8608 \n",
      "Epoch 01070 | Loss 0.2599 | Train Acc. 0.8992 | Validation Acc. 0.8608 \n",
      "Epoch 01075 | Loss 0.2534 | Train Acc. 0.9031 | Validation Acc. 0.8711 \n",
      "Epoch 01080 | Loss 0.2664 | Train Acc. 0.8941 | Validation Acc. 0.8660 \n",
      "Epoch 01085 | Loss 0.2623 | Train Acc. 0.9070 | Validation Acc. 0.8505 \n",
      "Epoch 01090 | Loss 0.2735 | Train Acc. 0.8863 | Validation Acc. 0.8660 \n",
      "Epoch 01095 | Loss 0.2620 | Train Acc. 0.8863 | Validation Acc. 0.8711 \n",
      "Epoch 01100 | Loss 0.2610 | Train Acc. 0.8966 | Validation Acc. 0.8711 \n",
      "Epoch 01105 | Loss 0.2575 | Train Acc. 0.8941 | Validation Acc. 0.8711 \n",
      "Epoch 01110 | Loss 0.2583 | Train Acc. 0.8992 | Validation Acc. 0.8660 \n",
      "Epoch 01115 | Loss 0.2522 | Train Acc. 0.9031 | Validation Acc. 0.8660 \n",
      "Epoch 01120 | Loss 0.2564 | Train Acc. 0.8992 | Validation Acc. 0.8660 \n",
      "Epoch 01125 | Loss 0.2532 | Train Acc. 0.8992 | Validation Acc. 0.8660 \n",
      "Epoch 01130 | Loss 0.2483 | Train Acc. 0.9005 | Validation Acc. 0.8608 \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "Fold : 2 | Test Accuracy = 0.8796 | F1 = 0.8566 \n",
      "Total = 6.4Gb \t Reserved = 1.1Gb \t Allocated = 0.3Gb\n",
      "Clearing gpu memory\n",
      "Total = 6.4Gb \t Reserved = 0.3Gb \t Allocated = 0.3Gb\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1076, num_edges=18312,\n",
      "      ndata_schemes={'idx': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(5,), dtype=torch.int64), 'feat': Scheme(shape=(30459,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.6782 | Train Acc. 0.1615 | Validation Acc. 0.2165 \n",
      "Epoch 00005 | Loss 0.8715 | Train Acc. 0.7894 | Validation Acc. 0.1959 \n",
      "Epoch 00010 | Loss 0.7198 | Train Acc. 0.8269 | Validation Acc. 0.2010 \n",
      "Epoch 00015 | Loss 0.5953 | Train Acc. 0.8217 | Validation Acc. 0.4897 \n",
      "Epoch 00020 | Loss 0.5197 | Train Acc. 0.8295 | Validation Acc. 0.7629 \n",
      "Epoch 00025 | Loss 0.4563 | Train Acc. 0.8424 | Validation Acc. 0.7680 \n",
      "Epoch 00030 | Loss 0.4074 | Train Acc. 0.8514 | Validation Acc. 0.7990 \n",
      "Epoch 00035 | Loss 0.4046 | Train Acc. 0.8514 | Validation Acc. 0.7835 \n",
      "Epoch 00040 | Loss 0.3767 | Train Acc. 0.8501 | Validation Acc. 0.7732 \n",
      "Epoch 00045 | Loss 0.3805 | Train Acc. 0.8553 | Validation Acc. 0.7474 \n",
      "Epoch 00050 | Loss 0.3691 | Train Acc. 0.8669 | Validation Acc. 0.7577 \n",
      "Epoch 00055 | Loss 0.3508 | Train Acc. 0.8527 | Validation Acc. 0.7680 \n",
      "Epoch 00060 | Loss 0.3576 | Train Acc. 0.8553 | Validation Acc. 0.7784 \n",
      "Epoch 00065 | Loss 0.3488 | Train Acc. 0.8656 | Validation Acc. 0.7887 \n",
      "Epoch 00070 | Loss 0.3445 | Train Acc. 0.8605 | Validation Acc. 0.7835 \n",
      "Epoch 00075 | Loss 0.3529 | Train Acc. 0.8488 | Validation Acc. 0.7990 \n",
      "Epoch 00080 | Loss 0.3260 | Train Acc. 0.8682 | Validation Acc. 0.7887 \n",
      "Epoch 00085 | Loss 0.3269 | Train Acc. 0.8747 | Validation Acc. 0.7784 \n",
      "Epoch 00090 | Loss 0.3310 | Train Acc. 0.8695 | Validation Acc. 0.7784 \n",
      "Epoch 00095 | Loss 0.3266 | Train Acc. 0.8605 | Validation Acc. 0.7887 \n",
      "Epoch 00100 | Loss 0.3206 | Train Acc. 0.8760 | Validation Acc. 0.8351 \n",
      "Epoch 00105 | Loss 0.3266 | Train Acc. 0.8760 | Validation Acc. 0.8660 \n",
      "Epoch 00110 | Loss 0.3264 | Train Acc. 0.8721 | Validation Acc. 0.8660 \n",
      "Epoch 00115 | Loss 0.3135 | Train Acc. 0.8747 | Validation Acc. 0.8557 \n",
      "Epoch 00120 | Loss 0.3137 | Train Acc. 0.8850 | Validation Acc. 0.8608 \n",
      "Epoch 00125 | Loss 0.3139 | Train Acc. 0.8734 | Validation Acc. 0.8505 \n",
      "Epoch 00130 | Loss 0.3083 | Train Acc. 0.8798 | Validation Acc. 0.8711 \n",
      "Epoch 00135 | Loss 0.3087 | Train Acc. 0.8721 | Validation Acc. 0.8763 \n",
      "Epoch 00140 | Loss 0.3102 | Train Acc. 0.8811 | Validation Acc. 0.8866 \n",
      "Epoch 00145 | Loss 0.2916 | Train Acc. 0.8824 | Validation Acc. 0.8557 \n",
      "Epoch 00150 | Loss 0.3043 | Train Acc. 0.8773 | Validation Acc. 0.8814 \n",
      "Epoch 00155 | Loss 0.2965 | Train Acc. 0.8850 | Validation Acc. 0.8763 \n",
      "Epoch 00160 | Loss 0.2997 | Train Acc. 0.8837 | Validation Acc. 0.8454 \n",
      "Epoch 00165 | Loss 0.2999 | Train Acc. 0.8786 | Validation Acc. 0.8814 \n",
      "Epoch 00170 | Loss 0.2986 | Train Acc. 0.8863 | Validation Acc. 0.8814 \n",
      "Epoch 00175 | Loss 0.2862 | Train Acc. 0.8889 | Validation Acc. 0.8608 \n",
      "Epoch 00180 | Loss 0.2865 | Train Acc. 0.8928 | Validation Acc. 0.7887 \n",
      "Epoch 00185 | Loss 0.3028 | Train Acc. 0.8876 | Validation Acc. 0.7474 \n",
      "Epoch 00190 | Loss 0.2877 | Train Acc. 0.8837 | Validation Acc. 0.7113 \n",
      "Epoch 00195 | Loss 0.2824 | Train Acc. 0.8876 | Validation Acc. 0.7938 \n",
      "Epoch 00200 | Loss 0.2807 | Train Acc. 0.8837 | Validation Acc. 0.8144 \n",
      "Epoch 00205 | Loss 0.2846 | Train Acc. 0.8863 | Validation Acc. 0.8505 \n",
      "Epoch 00210 | Loss 0.2842 | Train Acc. 0.8850 | Validation Acc. 0.8660 \n",
      "Epoch 00215 | Loss 0.2824 | Train Acc. 0.8876 | Validation Acc. 0.8660 \n",
      "Epoch 00220 | Loss 0.2725 | Train Acc. 0.8902 | Validation Acc. 0.8351 \n",
      "Epoch 00225 | Loss 0.2779 | Train Acc. 0.8915 | Validation Acc. 0.8711 \n",
      "Epoch 00230 | Loss 0.2760 | Train Acc. 0.8966 | Validation Acc. 0.8505 \n",
      "Epoch 00235 | Loss 0.2795 | Train Acc. 0.8837 | Validation Acc. 0.8247 \n",
      "Epoch 00240 | Loss 0.2738 | Train Acc. 0.9005 | Validation Acc. 0.8247 \n",
      "Epoch 00245 | Loss 0.2771 | Train Acc. 0.8850 | Validation Acc. 0.8351 \n",
      "Epoch 00250 | Loss 0.2795 | Train Acc. 0.8889 | Validation Acc. 0.8093 \n",
      "Epoch 00255 | Loss 0.2732 | Train Acc. 0.8915 | Validation Acc. 0.8711 \n",
      "Epoch 00260 | Loss 0.2746 | Train Acc. 0.8915 | Validation Acc. 0.8660 \n",
      "Epoch 00265 | Loss 0.2721 | Train Acc. 0.8915 | Validation Acc. 0.8763 \n",
      "Epoch 00270 | Loss 0.2687 | Train Acc. 0.8824 | Validation Acc. 0.8866 \n",
      "Epoch 00275 | Loss 0.2881 | Train Acc. 0.8811 | Validation Acc. 0.8866 \n",
      "Epoch 00280 | Loss 0.2698 | Train Acc. 0.9057 | Validation Acc. 0.8711 \n",
      "Epoch 00285 | Loss 0.2637 | Train Acc. 0.9044 | Validation Acc. 0.8660 \n",
      "Epoch 00290 | Loss 0.2752 | Train Acc. 0.8928 | Validation Acc. 0.8814 \n",
      "Epoch 00295 | Loss 0.2780 | Train Acc. 0.8876 | Validation Acc. 0.8711 \n",
      "Epoch 00300 | Loss 0.2725 | Train Acc. 0.8928 | Validation Acc. 0.8814 \n",
      "Epoch 00305 | Loss 0.2696 | Train Acc. 0.8928 | Validation Acc. 0.8814 \n",
      "Epoch 00310 | Loss 0.2591 | Train Acc. 0.9070 | Validation Acc. 0.8763 \n",
      "Epoch 00315 | Loss 0.2791 | Train Acc. 0.8889 | Validation Acc. 0.8866 \n",
      "Epoch 00320 | Loss 0.2678 | Train Acc. 0.9031 | Validation Acc. 0.8763 \n",
      "Epoch 00325 | Loss 0.2653 | Train Acc. 0.8953 | Validation Acc. 0.8763 \n",
      "Epoch 00330 | Loss 0.2615 | Train Acc. 0.8953 | Validation Acc. 0.8969 \n",
      "Epoch 00335 | Loss 0.2728 | Train Acc. 0.8876 | Validation Acc. 0.8866 \n",
      "Epoch 00340 | Loss 0.2754 | Train Acc. 0.8966 | Validation Acc. 0.8866 \n",
      "Epoch 00345 | Loss 0.2635 | Train Acc. 0.8915 | Validation Acc. 0.8763 \n",
      "Epoch 00350 | Loss 0.2700 | Train Acc. 0.8876 | Validation Acc. 0.8918 \n",
      "Epoch 00355 | Loss 0.2670 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 00360 | Loss 0.2613 | Train Acc. 0.8966 | Validation Acc. 0.8763 \n",
      "Epoch 00365 | Loss 0.2566 | Train Acc. 0.8902 | Validation Acc. 0.8763 \n",
      "Epoch 00370 | Loss 0.2602 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00375 | Loss 0.2636 | Train Acc. 0.8928 | Validation Acc. 0.8814 \n",
      "Epoch 00380 | Loss 0.2702 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 00385 | Loss 0.2681 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00390 | Loss 0.2670 | Train Acc. 0.8941 | Validation Acc. 0.8763 \n",
      "Epoch 00395 | Loss 0.2627 | Train Acc. 0.8928 | Validation Acc. 0.8763 \n",
      "Epoch 00400 | Loss 0.2652 | Train Acc. 0.9018 | Validation Acc. 0.8711 \n",
      "Epoch 00405 | Loss 0.2689 | Train Acc. 0.9005 | Validation Acc. 0.8969 \n",
      "Epoch 00410 | Loss 0.2622 | Train Acc. 0.8992 | Validation Acc. 0.8918 \n",
      "Epoch 00415 | Loss 0.2655 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00420 | Loss 0.2626 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00425 | Loss 0.2595 | Train Acc. 0.9005 | Validation Acc. 0.8918 \n",
      "Epoch 00430 | Loss 0.2709 | Train Acc. 0.9018 | Validation Acc. 0.8763 \n",
      "Epoch 00435 | Loss 0.2685 | Train Acc. 0.8979 | Validation Acc. 0.8763 \n",
      "Epoch 00440 | Loss 0.2601 | Train Acc. 0.8953 | Validation Acc. 0.8660 \n",
      "Epoch 00445 | Loss 0.2648 | Train Acc. 0.8850 | Validation Acc. 0.8763 \n",
      "Epoch 00450 | Loss 0.2670 | Train Acc. 0.8902 | Validation Acc. 0.8814 \n",
      "Epoch 00455 | Loss 0.2647 | Train Acc. 0.8863 | Validation Acc. 0.8814 \n",
      "Epoch 00460 | Loss 0.2664 | Train Acc. 0.8876 | Validation Acc. 0.8711 \n",
      "Epoch 00465 | Loss 0.2729 | Train Acc. 0.8966 | Validation Acc. 0.8814 \n",
      "Epoch 00470 | Loss 0.2723 | Train Acc. 0.8953 | Validation Acc. 0.8660 \n",
      "Epoch 00475 | Loss 0.2622 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 00480 | Loss 0.2621 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00485 | Loss 0.2600 | Train Acc. 0.8992 | Validation Acc. 0.8763 \n",
      "Epoch 00490 | Loss 0.2602 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00495 | Loss 0.2637 | Train Acc. 0.8889 | Validation Acc. 0.8814 \n",
      "Epoch 00500 | Loss 0.2749 | Train Acc. 0.8966 | Validation Acc. 0.8763 \n",
      "Epoch 00505 | Loss 0.2583 | Train Acc. 0.8966 | Validation Acc. 0.8711 \n",
      "Epoch 00510 | Loss 0.2583 | Train Acc. 0.9005 | Validation Acc. 0.8866 \n",
      "Epoch 00515 | Loss 0.2691 | Train Acc. 0.8928 | Validation Acc. 0.8711 \n",
      "Epoch 00520 | Loss 0.2695 | Train Acc. 0.8966 | Validation Acc. 0.8763 \n",
      "Epoch 00525 | Loss 0.2571 | Train Acc. 0.8979 | Validation Acc. 0.8763 \n",
      "Epoch 00530 | Loss 0.2597 | Train Acc. 0.8850 | Validation Acc. 0.8814 \n",
      "Epoch 00535 | Loss 0.2584 | Train Acc. 0.9031 | Validation Acc. 0.8711 \n",
      "Epoch 00540 | Loss 0.2690 | Train Acc. 0.8966 | Validation Acc. 0.8814 \n",
      "Epoch 00545 | Loss 0.2626 | Train Acc. 0.8966 | Validation Acc. 0.8763 \n",
      "Epoch 00550 | Loss 0.2699 | Train Acc. 0.8992 | Validation Acc. 0.8814 \n",
      "Epoch 00555 | Loss 0.2743 | Train Acc. 0.8941 | Validation Acc. 0.8814 \n",
      "Epoch 00560 | Loss 0.2663 | Train Acc. 0.8902 | Validation Acc. 0.8866 \n",
      "Epoch 00565 | Loss 0.2648 | Train Acc. 0.9005 | Validation Acc. 0.8814 \n",
      "Epoch 00570 | Loss 0.2562 | Train Acc. 0.8966 | Validation Acc. 0.8711 \n",
      "Epoch 00575 | Loss 0.2572 | Train Acc. 0.9057 | Validation Acc. 0.8660 \n",
      "Epoch 00580 | Loss 0.2712 | Train Acc. 0.8850 | Validation Acc. 0.8660 \n",
      "Epoch 00585 | Loss 0.2634 | Train Acc. 0.9018 | Validation Acc. 0.8711 \n",
      "Epoch 00590 | Loss 0.2608 | Train Acc. 0.8889 | Validation Acc. 0.8814 \n",
      "Epoch 00595 | Loss 0.2619 | Train Acc. 0.8992 | Validation Acc. 0.8763 \n",
      "Epoch 00600 | Loss 0.2584 | Train Acc. 0.8953 | Validation Acc. 0.8814 \n",
      "Epoch 00605 | Loss 0.2619 | Train Acc. 0.8915 | Validation Acc. 0.8814 \n",
      "Epoch 00610 | Loss 0.2510 | Train Acc. 0.9044 | Validation Acc. 0.8814 \n",
      "Epoch 00615 | Loss 0.2666 | Train Acc. 0.8979 | Validation Acc. 0.8763 \n",
      "Epoch 00620 | Loss 0.2660 | Train Acc. 0.8902 | Validation Acc. 0.8660 \n",
      "Epoch 00625 | Loss 0.2625 | Train Acc. 0.9018 | Validation Acc. 0.8866 \n",
      "Epoch 00630 | Loss 0.2619 | Train Acc. 0.9031 | Validation Acc. 0.8660 \n",
      "Epoch 00635 | Loss 0.2729 | Train Acc. 0.8953 | Validation Acc. 0.8866 \n",
      "Epoch 00640 | Loss 0.2615 | Train Acc. 0.9031 | Validation Acc. 0.8918 \n",
      "Epoch 00645 | Loss 0.2681 | Train Acc. 0.8850 | Validation Acc. 0.8866 \n",
      "Epoch 00650 | Loss 0.2585 | Train Acc. 0.8941 | Validation Acc. 0.8866 \n",
      "Epoch 00655 | Loss 0.2642 | Train Acc. 0.9083 | Validation Acc. 0.8866 \n",
      "Epoch 00660 | Loss 0.2629 | Train Acc. 0.8979 | Validation Acc. 0.8711 \n",
      "Epoch 00665 | Loss 0.2669 | Train Acc. 0.8992 | Validation Acc. 0.8711 \n",
      "Epoch 00670 | Loss 0.2626 | Train Acc. 0.9044 | Validation Acc. 0.8763 \n",
      "Epoch 00675 | Loss 0.2658 | Train Acc. 0.9005 | Validation Acc. 0.8814 \n",
      "Epoch 00680 | Loss 0.2717 | Train Acc. 0.8863 | Validation Acc. 0.8918 \n",
      "Epoch 00685 | Loss 0.2642 | Train Acc. 0.8941 | Validation Acc. 0.8814 \n",
      "Epoch 00690 | Loss 0.2681 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00695 | Loss 0.2638 | Train Acc. 0.9018 | Validation Acc. 0.8814 \n",
      "Epoch 00700 | Loss 0.2517 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00705 | Loss 0.2724 | Train Acc. 0.8915 | Validation Acc. 0.8918 \n",
      "Epoch 00710 | Loss 0.2640 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 00715 | Loss 0.2641 | Train Acc. 0.8979 | Validation Acc. 0.8918 \n",
      "Epoch 00720 | Loss 0.2738 | Train Acc. 0.8811 | Validation Acc. 0.8763 \n",
      "Epoch 00725 | Loss 0.2625 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 00730 | Loss 0.2594 | Train Acc. 0.9031 | Validation Acc. 0.8814 \n",
      "Epoch 00735 | Loss 0.2677 | Train Acc. 0.8992 | Validation Acc. 0.8814 \n",
      "Epoch 00740 | Loss 0.2566 | Train Acc. 0.9096 | Validation Acc. 0.8763 \n",
      "Epoch 00745 | Loss 0.2697 | Train Acc. 0.8992 | Validation Acc. 0.8918 \n",
      "Epoch 00750 | Loss 0.2690 | Train Acc. 0.8850 | Validation Acc. 0.8866 \n",
      "Epoch 00755 | Loss 0.2707 | Train Acc. 0.8941 | Validation Acc. 0.8763 \n",
      "Epoch 00760 | Loss 0.2557 | Train Acc. 0.9018 | Validation Acc. 0.8866 \n",
      "Epoch 00765 | Loss 0.2603 | Train Acc. 0.9018 | Validation Acc. 0.8866 \n",
      "Epoch 00770 | Loss 0.2703 | Train Acc. 0.8902 | Validation Acc. 0.8814 \n",
      "Epoch 00775 | Loss 0.2699 | Train Acc. 0.9005 | Validation Acc. 0.8866 \n",
      "Epoch 00780 | Loss 0.2638 | Train Acc. 0.9070 | Validation Acc. 0.8814 \n",
      "Epoch 00785 | Loss 0.2679 | Train Acc. 0.8928 | Validation Acc. 0.8814 \n",
      "Epoch 00790 | Loss 0.2659 | Train Acc. 0.8941 | Validation Acc. 0.8814 \n",
      "Epoch 00795 | Loss 0.2556 | Train Acc. 0.9109 | Validation Acc. 0.8660 \n",
      "Epoch 00800 | Loss 0.2618 | Train Acc. 0.8979 | Validation Acc. 0.8763 \n",
      "Epoch 00805 | Loss 0.2666 | Train Acc. 0.9005 | Validation Acc. 0.8814 \n",
      "Epoch 00810 | Loss 0.2524 | Train Acc. 0.9005 | Validation Acc. 0.8814 \n",
      "Epoch 00815 | Loss 0.2648 | Train Acc. 0.8928 | Validation Acc. 0.8711 \n",
      "Epoch 00820 | Loss 0.2613 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00825 | Loss 0.2684 | Train Acc. 0.8863 | Validation Acc. 0.8866 \n",
      "Epoch 00830 | Loss 0.2647 | Train Acc. 0.8979 | Validation Acc. 0.8763 \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "Fold : 3 | Test Accuracy = 0.8796 | F1 = 0.8526 \n",
      "Total = 6.4Gb \t Reserved = 1.1Gb \t Allocated = 0.4Gb\n",
      "Clearing gpu memory\n",
      "Total = 6.4Gb \t Reserved = 0.3Gb \t Allocated = 0.3Gb\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1076, num_edges=18312,\n",
      "      ndata_schemes={'idx': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(5,), dtype=torch.int64), 'feat': Scheme(shape=(30459,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.5315 | Train Acc. 0.2545 | Validation Acc. 0.6959 \n",
      "Epoch 00005 | Loss 0.6159 | Train Acc. 0.8372 | Validation Acc. 0.3299 \n",
      "Epoch 00010 | Loss 0.5248 | Train Acc. 0.8372 | Validation Acc. 0.5052 \n",
      "Epoch 00015 | Loss 0.4620 | Train Acc. 0.8424 | Validation Acc. 0.6289 \n",
      "Epoch 00020 | Loss 0.4072 | Train Acc. 0.8566 | Validation Acc. 0.7165 \n",
      "Epoch 00025 | Loss 0.3823 | Train Acc. 0.8643 | Validation Acc. 0.7526 \n",
      "Epoch 00030 | Loss 0.3522 | Train Acc. 0.8708 | Validation Acc. 0.7784 \n",
      "Epoch 00035 | Loss 0.3428 | Train Acc. 0.8708 | Validation Acc. 0.8144 \n",
      "Epoch 00040 | Loss 0.3445 | Train Acc. 0.8747 | Validation Acc. 0.8299 \n",
      "Epoch 00045 | Loss 0.3256 | Train Acc. 0.8721 | Validation Acc. 0.8196 \n",
      "Epoch 00050 | Loss 0.3201 | Train Acc. 0.8708 | Validation Acc. 0.7938 \n",
      "Epoch 00055 | Loss 0.3083 | Train Acc. 0.8876 | Validation Acc. 0.8093 \n",
      "Epoch 00060 | Loss 0.3050 | Train Acc. 0.8824 | Validation Acc. 0.8041 \n",
      "Epoch 00065 | Loss 0.3102 | Train Acc. 0.8734 | Validation Acc. 0.7835 \n",
      "Epoch 00070 | Loss 0.2938 | Train Acc. 0.8824 | Validation Acc. 0.7835 \n",
      "Epoch 00075 | Loss 0.2984 | Train Acc. 0.8811 | Validation Acc. 0.7938 \n",
      "Epoch 00080 | Loss 0.3005 | Train Acc. 0.8928 | Validation Acc. 0.7887 \n",
      "Epoch 00085 | Loss 0.2937 | Train Acc. 0.8786 | Validation Acc. 0.7784 \n",
      "Epoch 00090 | Loss 0.2872 | Train Acc. 0.8876 | Validation Acc. 0.7629 \n",
      "Epoch 00095 | Loss 0.2787 | Train Acc. 0.8863 | Validation Acc. 0.7577 \n",
      "Epoch 00100 | Loss 0.2743 | Train Acc. 0.8863 | Validation Acc. 0.8144 \n",
      "Epoch 00105 | Loss 0.2748 | Train Acc. 0.8928 | Validation Acc. 0.8299 \n",
      "Epoch 00110 | Loss 0.2783 | Train Acc. 0.8953 | Validation Acc. 0.8196 \n",
      "Epoch 00115 | Loss 0.2558 | Train Acc. 0.8966 | Validation Acc. 0.8557 \n",
      "Epoch 00120 | Loss 0.2678 | Train Acc. 0.8863 | Validation Acc. 0.8454 \n",
      "Epoch 00125 | Loss 0.2687 | Train Acc. 0.8992 | Validation Acc. 0.8196 \n",
      "Epoch 00130 | Loss 0.2597 | Train Acc. 0.9044 | Validation Acc. 0.8351 \n",
      "Epoch 00135 | Loss 0.2601 | Train Acc. 0.8966 | Validation Acc. 0.8505 \n",
      "Epoch 00140 | Loss 0.2466 | Train Acc. 0.8966 | Validation Acc. 0.8814 \n",
      "Epoch 00145 | Loss 0.2478 | Train Acc. 0.8966 | Validation Acc. 0.8557 \n",
      "Epoch 00150 | Loss 0.2578 | Train Acc. 0.9044 | Validation Acc. 0.8608 \n",
      "Epoch 00155 | Loss 0.2512 | Train Acc. 0.9018 | Validation Acc. 0.8505 \n",
      "Epoch 00160 | Loss 0.2488 | Train Acc. 0.9109 | Validation Acc. 0.8454 \n",
      "Epoch 00165 | Loss 0.2484 | Train Acc. 0.9018 | Validation Acc. 0.8454 \n",
      "Epoch 00170 | Loss 0.2411 | Train Acc. 0.9083 | Validation Acc. 0.8196 \n",
      "Epoch 00175 | Loss 0.2355 | Train Acc. 0.9044 | Validation Acc. 0.7990 \n",
      "Epoch 00180 | Loss 0.2391 | Train Acc. 0.9121 | Validation Acc. 0.8093 \n",
      "Epoch 00185 | Loss 0.2446 | Train Acc. 0.9070 | Validation Acc. 0.7990 \n",
      "Epoch 00190 | Loss 0.2309 | Train Acc. 0.9044 | Validation Acc. 0.7990 \n",
      "Epoch 00195 | Loss 0.2317 | Train Acc. 0.9031 | Validation Acc. 0.8351 \n",
      "Epoch 00200 | Loss 0.2304 | Train Acc. 0.9109 | Validation Acc. 0.8454 \n",
      "Epoch 00205 | Loss 0.2303 | Train Acc. 0.9199 | Validation Acc. 0.8351 \n",
      "Epoch 00210 | Loss 0.2351 | Train Acc. 0.9031 | Validation Acc. 0.8351 \n",
      "Epoch 00215 | Loss 0.2267 | Train Acc. 0.9173 | Validation Acc. 0.8402 \n",
      "Epoch 00220 | Loss 0.2233 | Train Acc. 0.9212 | Validation Acc. 0.8196 \n",
      "Epoch 00225 | Loss 0.2215 | Train Acc. 0.9147 | Validation Acc. 0.8247 \n",
      "Epoch 00230 | Loss 0.2211 | Train Acc. 0.9083 | Validation Acc. 0.8196 \n",
      "Epoch 00235 | Loss 0.2271 | Train Acc. 0.9147 | Validation Acc. 0.8351 \n",
      "Epoch 00240 | Loss 0.2220 | Train Acc. 0.9212 | Validation Acc. 0.8454 \n",
      "Epoch 00245 | Loss 0.2202 | Train Acc. 0.9109 | Validation Acc. 0.8144 \n",
      "Epoch 00250 | Loss 0.2190 | Train Acc. 0.9160 | Validation Acc. 0.8299 \n",
      "Epoch 00255 | Loss 0.2098 | Train Acc. 0.9147 | Validation Acc. 0.8247 \n",
      "Epoch 00260 | Loss 0.2195 | Train Acc. 0.9238 | Validation Acc. 0.8454 \n",
      "Epoch 00265 | Loss 0.2148 | Train Acc. 0.9199 | Validation Acc. 0.8196 \n",
      "Epoch 00270 | Loss 0.2094 | Train Acc. 0.9276 | Validation Acc. 0.8557 \n",
      "Epoch 00275 | Loss 0.2149 | Train Acc. 0.9225 | Validation Acc. 0.8351 \n",
      "Epoch 00280 | Loss 0.2167 | Train Acc. 0.9173 | Validation Acc. 0.8505 \n",
      "Epoch 00285 | Loss 0.2011 | Train Acc. 0.9328 | Validation Acc. 0.8557 \n",
      "Epoch 00290 | Loss 0.2114 | Train Acc. 0.9186 | Validation Acc. 0.8351 \n",
      "Epoch 00295 | Loss 0.2132 | Train Acc. 0.9186 | Validation Acc. 0.8247 \n",
      "Epoch 00300 | Loss 0.2110 | Train Acc. 0.9212 | Validation Acc. 0.8299 \n",
      "Epoch 00305 | Loss 0.2110 | Train Acc. 0.9225 | Validation Acc. 0.8505 \n",
      "Epoch 00310 | Loss 0.2115 | Train Acc. 0.9212 | Validation Acc. 0.8505 \n",
      "Epoch 00315 | Loss 0.2229 | Train Acc. 0.9186 | Validation Acc. 0.8351 \n",
      "Epoch 00320 | Loss 0.2052 | Train Acc. 0.9315 | Validation Acc. 0.8402 \n",
      "Epoch 00325 | Loss 0.2177 | Train Acc. 0.9173 | Validation Acc. 0.8402 \n",
      "Epoch 00330 | Loss 0.2208 | Train Acc. 0.9147 | Validation Acc. 0.8505 \n",
      "Epoch 00335 | Loss 0.2053 | Train Acc. 0.9160 | Validation Acc. 0.8608 \n",
      "Epoch 00340 | Loss 0.2121 | Train Acc. 0.9225 | Validation Acc. 0.8505 \n",
      "Epoch 00345 | Loss 0.2174 | Train Acc. 0.9173 | Validation Acc. 0.8505 \n",
      "Epoch 00350 | Loss 0.2138 | Train Acc. 0.9121 | Validation Acc. 0.8402 \n",
      "Epoch 00355 | Loss 0.2082 | Train Acc. 0.9147 | Validation Acc. 0.8660 \n",
      "Epoch 00360 | Loss 0.2030 | Train Acc. 0.9238 | Validation Acc. 0.8557 \n",
      "Epoch 00365 | Loss 0.2060 | Train Acc. 0.9264 | Validation Acc. 0.8351 \n",
      "Epoch 00370 | Loss 0.2066 | Train Acc. 0.9251 | Validation Acc. 0.8505 \n",
      "Epoch 00375 | Loss 0.2027 | Train Acc. 0.9160 | Validation Acc. 0.8505 \n",
      "Epoch 00380 | Loss 0.2055 | Train Acc. 0.9276 | Validation Acc. 0.8402 \n",
      "Epoch 00385 | Loss 0.2177 | Train Acc. 0.9134 | Validation Acc. 0.8660 \n",
      "Epoch 00390 | Loss 0.2050 | Train Acc. 0.9238 | Validation Acc. 0.8402 \n",
      "Epoch 00395 | Loss 0.2043 | Train Acc. 0.9186 | Validation Acc. 0.8608 \n",
      "Epoch 00400 | Loss 0.2111 | Train Acc. 0.9238 | Validation Acc. 0.8454 \n",
      "Epoch 00405 | Loss 0.2067 | Train Acc. 0.9315 | Validation Acc. 0.8505 \n",
      "Epoch 00410 | Loss 0.2017 | Train Acc. 0.9251 | Validation Acc. 0.8608 \n",
      "Epoch 00415 | Loss 0.1985 | Train Acc. 0.9264 | Validation Acc. 0.8505 \n",
      "Epoch 00420 | Loss 0.2029 | Train Acc. 0.9238 | Validation Acc. 0.8454 \n",
      "Epoch 00425 | Loss 0.2172 | Train Acc. 0.9186 | Validation Acc. 0.8711 \n",
      "Epoch 00430 | Loss 0.2075 | Train Acc. 0.9289 | Validation Acc. 0.8299 \n",
      "Epoch 00435 | Loss 0.2026 | Train Acc. 0.9328 | Validation Acc. 0.8402 \n",
      "Epoch 00440 | Loss 0.2063 | Train Acc. 0.9289 | Validation Acc. 0.8351 \n",
      "Epoch 00445 | Loss 0.1998 | Train Acc. 0.9328 | Validation Acc. 0.8608 \n",
      "Epoch 00450 | Loss 0.2039 | Train Acc. 0.9186 | Validation Acc. 0.8402 \n",
      "Epoch 00455 | Loss 0.1979 | Train Acc. 0.9264 | Validation Acc. 0.8505 \n",
      "Epoch 00460 | Loss 0.2073 | Train Acc. 0.9173 | Validation Acc. 0.8608 \n",
      "Epoch 00465 | Loss 0.2049 | Train Acc. 0.9264 | Validation Acc. 0.8711 \n",
      "Epoch 00470 | Loss 0.2056 | Train Acc. 0.9238 | Validation Acc. 0.8557 \n",
      "Epoch 00475 | Loss 0.2128 | Train Acc. 0.9251 | Validation Acc. 0.8660 \n",
      "Epoch 00480 | Loss 0.2089 | Train Acc. 0.9225 | Validation Acc. 0.8402 \n",
      "Epoch 00485 | Loss 0.2071 | Train Acc. 0.9160 | Validation Acc. 0.8608 \n",
      "Epoch 00490 | Loss 0.2009 | Train Acc. 0.9264 | Validation Acc. 0.8608 \n",
      "Epoch 00495 | Loss 0.2075 | Train Acc. 0.9276 | Validation Acc. 0.8608 \n",
      "Epoch 00500 | Loss 0.2077 | Train Acc. 0.9173 | Validation Acc. 0.8351 \n",
      "Epoch 00505 | Loss 0.1989 | Train Acc. 0.9276 | Validation Acc. 0.8454 \n",
      "Epoch 00510 | Loss 0.2034 | Train Acc. 0.9251 | Validation Acc. 0.8608 \n",
      "Epoch 00515 | Loss 0.2100 | Train Acc. 0.9134 | Validation Acc. 0.8454 \n",
      "Epoch 00520 | Loss 0.2054 | Train Acc. 0.9238 | Validation Acc. 0.8402 \n",
      "Epoch 00525 | Loss 0.1998 | Train Acc. 0.9264 | Validation Acc. 0.8711 \n",
      "Epoch 00530 | Loss 0.2122 | Train Acc. 0.9212 | Validation Acc. 0.8660 \n",
      "Epoch 00535 | Loss 0.2093 | Train Acc. 0.9289 | Validation Acc. 0.8505 \n",
      "Epoch 00540 | Loss 0.2039 | Train Acc. 0.9238 | Validation Acc. 0.8711 \n",
      "Epoch 00545 | Loss 0.2100 | Train Acc. 0.9173 | Validation Acc. 0.8505 \n",
      "Epoch 00550 | Loss 0.2071 | Train Acc. 0.9264 | Validation Acc. 0.8608 \n",
      "Epoch 00555 | Loss 0.2059 | Train Acc. 0.9251 | Validation Acc. 0.8505 \n",
      "Epoch 00560 | Loss 0.1960 | Train Acc. 0.9289 | Validation Acc. 0.8660 \n",
      "Epoch 00565 | Loss 0.2046 | Train Acc. 0.9251 | Validation Acc. 0.8454 \n",
      "Epoch 00570 | Loss 0.2072 | Train Acc. 0.9238 | Validation Acc. 0.8660 \n",
      "Epoch 00575 | Loss 0.2076 | Train Acc. 0.9212 | Validation Acc. 0.8505 \n",
      "Epoch 00580 | Loss 0.2009 | Train Acc. 0.9302 | Validation Acc. 0.8557 \n",
      "Epoch 00585 | Loss 0.2021 | Train Acc. 0.9212 | Validation Acc. 0.8505 \n",
      "Epoch 00590 | Loss 0.1988 | Train Acc. 0.9238 | Validation Acc. 0.8505 \n",
      "Epoch 00595 | Loss 0.1982 | Train Acc. 0.9276 | Validation Acc. 0.8505 \n",
      "Epoch 00600 | Loss 0.2091 | Train Acc. 0.9251 | Validation Acc. 0.8505 \n",
      "Epoch 00605 | Loss 0.1995 | Train Acc. 0.9328 | Validation Acc. 0.8660 \n",
      "Epoch 00610 | Loss 0.2186 | Train Acc. 0.9238 | Validation Acc. 0.8608 \n",
      "Epoch 00615 | Loss 0.2132 | Train Acc. 0.9173 | Validation Acc. 0.8299 \n",
      "Epoch 00620 | Loss 0.2038 | Train Acc. 0.9212 | Validation Acc. 0.8505 \n",
      "Epoch 00625 | Loss 0.2117 | Train Acc. 0.9173 | Validation Acc. 0.8557 \n",
      "Epoch 00630 | Loss 0.2093 | Train Acc. 0.9225 | Validation Acc. 0.8454 \n",
      "Epoch 00635 | Loss 0.2042 | Train Acc. 0.9289 | Validation Acc. 0.8660 \n",
      "Epoch 00640 | Loss 0.2078 | Train Acc. 0.9186 | Validation Acc. 0.8454 \n",
      "Epoch 00645 | Loss 0.2034 | Train Acc. 0.9276 | Validation Acc. 0.8454 \n",
      "Epoch 00650 | Loss 0.2143 | Train Acc. 0.9199 | Validation Acc. 0.8557 \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "Fold : 4 | Test Accuracy = 0.8519 | F1 = 0.8381 \n",
      "Total = 6.4Gb \t Reserved = 1.1Gb \t Allocated = 0.4Gb\n",
      "Clearing gpu memory\n",
      "Total = 6.4Gb \t Reserved = 0.3Gb \t Allocated = 0.3Gb\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1076, num_edges=18312,\n",
      "      ndata_schemes={'idx': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(5,), dtype=torch.int64), 'feat': Scheme(shape=(30459,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.7228 | Train Acc. 0.0646 | Validation Acc. 0.2113 \n",
      "Epoch 00005 | Loss 1.0146 | Train Acc. 0.6886 | Validation Acc. 0.1392 \n",
      "Epoch 00010 | Loss 0.8087 | Train Acc. 0.7894 | Validation Acc. 0.2577 \n",
      "Epoch 00015 | Loss 0.6687 | Train Acc. 0.8230 | Validation Acc. 0.5670 \n",
      "Epoch 00020 | Loss 0.5693 | Train Acc. 0.8398 | Validation Acc. 0.6804 \n",
      "Epoch 00025 | Loss 0.4891 | Train Acc. 0.8450 | Validation Acc. 0.7268 \n",
      "Epoch 00030 | Loss 0.4402 | Train Acc. 0.8437 | Validation Acc. 0.7526 \n",
      "Epoch 00035 | Loss 0.4118 | Train Acc. 0.8463 | Validation Acc. 0.7423 \n",
      "Epoch 00040 | Loss 0.3894 | Train Acc. 0.8540 | Validation Acc. 0.7371 \n",
      "Epoch 00045 | Loss 0.3710 | Train Acc. 0.8437 | Validation Acc. 0.7423 \n",
      "Epoch 00050 | Loss 0.3592 | Train Acc. 0.8566 | Validation Acc. 0.7784 \n",
      "Epoch 00055 | Loss 0.3471 | Train Acc. 0.8682 | Validation Acc. 0.8247 \n",
      "Epoch 00060 | Loss 0.3597 | Train Acc. 0.8566 | Validation Acc. 0.8402 \n",
      "Epoch 00065 | Loss 0.3397 | Train Acc. 0.8682 | Validation Acc. 0.8351 \n",
      "Epoch 00070 | Loss 0.3432 | Train Acc. 0.8682 | Validation Acc. 0.8402 \n",
      "Epoch 00075 | Loss 0.3409 | Train Acc. 0.8618 | Validation Acc. 0.8454 \n",
      "Epoch 00080 | Loss 0.3272 | Train Acc. 0.8721 | Validation Acc. 0.8351 \n",
      "Epoch 00085 | Loss 0.3358 | Train Acc. 0.8605 | Validation Acc. 0.8196 \n",
      "Epoch 00090 | Loss 0.3192 | Train Acc. 0.8734 | Validation Acc. 0.8196 \n",
      "Epoch 00095 | Loss 0.3282 | Train Acc. 0.8669 | Validation Acc. 0.8144 \n",
      "Epoch 00100 | Loss 0.3297 | Train Acc. 0.8630 | Validation Acc. 0.8351 \n",
      "Epoch 00105 | Loss 0.3130 | Train Acc. 0.8760 | Validation Acc. 0.8505 \n",
      "Epoch 00110 | Loss 0.3045 | Train Acc. 0.8798 | Validation Acc. 0.8299 \n",
      "Epoch 00115 | Loss 0.3113 | Train Acc. 0.8695 | Validation Acc. 0.8505 \n",
      "Epoch 00120 | Loss 0.3104 | Train Acc. 0.8760 | Validation Acc. 0.8505 \n",
      "Epoch 00125 | Loss 0.3074 | Train Acc. 0.8786 | Validation Acc. 0.8505 \n",
      "Epoch 00130 | Loss 0.3028 | Train Acc. 0.8811 | Validation Acc. 0.8608 \n",
      "Epoch 00135 | Loss 0.3105 | Train Acc. 0.8760 | Validation Acc. 0.8505 \n",
      "Epoch 00140 | Loss 0.3027 | Train Acc. 0.8786 | Validation Acc. 0.8557 \n",
      "Epoch 00145 | Loss 0.2957 | Train Acc. 0.8786 | Validation Acc. 0.8505 \n",
      "Epoch 00150 | Loss 0.3017 | Train Acc. 0.8798 | Validation Acc. 0.8454 \n",
      "Epoch 00155 | Loss 0.3005 | Train Acc. 0.8811 | Validation Acc. 0.8608 \n",
      "Epoch 00160 | Loss 0.2917 | Train Acc. 0.8811 | Validation Acc. 0.8557 \n",
      "Epoch 00165 | Loss 0.2995 | Train Acc. 0.8773 | Validation Acc. 0.8660 \n",
      "Epoch 00170 | Loss 0.2975 | Train Acc. 0.8747 | Validation Acc. 0.8660 \n",
      "Epoch 00175 | Loss 0.2856 | Train Acc. 0.8863 | Validation Acc. 0.8557 \n",
      "Epoch 00180 | Loss 0.2950 | Train Acc. 0.8811 | Validation Acc. 0.8454 \n",
      "Epoch 00185 | Loss 0.2932 | Train Acc. 0.8798 | Validation Acc. 0.8505 \n",
      "Epoch 00190 | Loss 0.2997 | Train Acc. 0.8786 | Validation Acc. 0.8505 \n",
      "Epoch 00195 | Loss 0.2829 | Train Acc. 0.8889 | Validation Acc. 0.8557 \n",
      "Epoch 00200 | Loss 0.2740 | Train Acc. 0.8863 | Validation Acc. 0.8557 \n",
      "Epoch 00205 | Loss 0.2815 | Train Acc. 0.8837 | Validation Acc. 0.8660 \n",
      "Epoch 00210 | Loss 0.2795 | Train Acc. 0.8889 | Validation Acc. 0.8711 \n",
      "Epoch 00215 | Loss 0.2722 | Train Acc. 0.8915 | Validation Acc. 0.8711 \n",
      "Epoch 00220 | Loss 0.2825 | Train Acc. 0.8824 | Validation Acc. 0.8402 \n",
      "Epoch 00225 | Loss 0.2852 | Train Acc. 0.8811 | Validation Acc. 0.8505 \n",
      "Epoch 00230 | Loss 0.2764 | Train Acc. 0.9005 | Validation Acc. 0.8454 \n",
      "Epoch 00235 | Loss 0.2831 | Train Acc. 0.8876 | Validation Acc. 0.8608 \n",
      "Epoch 00240 | Loss 0.2865 | Train Acc. 0.8786 | Validation Acc. 0.8454 \n",
      "Epoch 00245 | Loss 0.2858 | Train Acc. 0.8889 | Validation Acc. 0.8454 \n",
      "Epoch 00250 | Loss 0.2797 | Train Acc. 0.8876 | Validation Acc. 0.8454 \n",
      "Epoch 00255 | Loss 0.2865 | Train Acc. 0.8889 | Validation Acc. 0.8351 \n",
      "Epoch 00260 | Loss 0.2739 | Train Acc. 0.8928 | Validation Acc. 0.8557 \n",
      "Epoch 00265 | Loss 0.2839 | Train Acc. 0.8760 | Validation Acc. 0.8402 \n",
      "Epoch 00270 | Loss 0.2713 | Train Acc. 0.8953 | Validation Acc. 0.8660 \n",
      "Epoch 00275 | Loss 0.2727 | Train Acc. 0.9005 | Validation Acc. 0.8557 \n",
      "Epoch 00280 | Loss 0.2666 | Train Acc. 0.8966 | Validation Acc. 0.8660 \n",
      "Epoch 00285 | Loss 0.2755 | Train Acc. 0.8915 | Validation Acc. 0.8660 \n",
      "Epoch 00290 | Loss 0.2754 | Train Acc. 0.8863 | Validation Acc. 0.8608 \n",
      "Epoch 00295 | Loss 0.2744 | Train Acc. 0.8889 | Validation Acc. 0.8608 \n",
      "Epoch 00300 | Loss 0.2685 | Train Acc. 0.9005 | Validation Acc. 0.8608 \n",
      "Epoch 00305 | Loss 0.2699 | Train Acc. 0.9018 | Validation Acc. 0.8608 \n",
      "Epoch 00310 | Loss 0.2681 | Train Acc. 0.8979 | Validation Acc. 0.8557 \n",
      "Epoch 00315 | Loss 0.2704 | Train Acc. 0.8966 | Validation Acc. 0.8711 \n",
      "Epoch 00320 | Loss 0.2797 | Train Acc. 0.8941 | Validation Acc. 0.8660 \n",
      "Epoch 00325 | Loss 0.2794 | Train Acc. 0.8915 | Validation Acc. 0.8660 \n",
      "Epoch 00330 | Loss 0.2695 | Train Acc. 0.8915 | Validation Acc. 0.8608 \n",
      "Epoch 00335 | Loss 0.2757 | Train Acc. 0.8850 | Validation Acc. 0.8608 \n",
      "Epoch 00340 | Loss 0.2789 | Train Acc. 0.8837 | Validation Acc. 0.8660 \n",
      "Epoch 00345 | Loss 0.2701 | Train Acc. 0.8876 | Validation Acc. 0.8711 \n",
      "Epoch 00350 | Loss 0.2691 | Train Acc. 0.8953 | Validation Acc. 0.8660 \n",
      "Epoch 00355 | Loss 0.2682 | Train Acc. 0.8837 | Validation Acc. 0.8763 \n",
      "Epoch 00360 | Loss 0.2653 | Train Acc. 0.8992 | Validation Acc. 0.8660 \n",
      "Epoch 00365 | Loss 0.2703 | Train Acc. 0.8902 | Validation Acc. 0.8711 \n",
      "Epoch 00370 | Loss 0.2687 | Train Acc. 0.8966 | Validation Acc. 0.8763 \n",
      "Epoch 00375 | Loss 0.2681 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00380 | Loss 0.2634 | Train Acc. 0.8928 | Validation Acc. 0.8711 \n",
      "Epoch 00385 | Loss 0.2864 | Train Acc. 0.8915 | Validation Acc. 0.8660 \n",
      "Epoch 00390 | Loss 0.2769 | Train Acc. 0.8824 | Validation Acc. 0.8711 \n",
      "Epoch 00395 | Loss 0.2805 | Train Acc. 0.8902 | Validation Acc. 0.8608 \n",
      "Epoch 00400 | Loss 0.2670 | Train Acc. 0.8928 | Validation Acc. 0.8660 \n",
      "Epoch 00405 | Loss 0.2644 | Train Acc. 0.8941 | Validation Acc. 0.8711 \n",
      "Epoch 00410 | Loss 0.2791 | Train Acc. 0.8915 | Validation Acc. 0.8608 \n",
      "Epoch 00415 | Loss 0.2709 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00420 | Loss 0.2670 | Train Acc. 0.8966 | Validation Acc. 0.8660 \n",
      "Epoch 00425 | Loss 0.2798 | Train Acc. 0.8876 | Validation Acc. 0.8763 \n",
      "Epoch 00430 | Loss 0.2669 | Train Acc. 0.8953 | Validation Acc. 0.8608 \n",
      "Epoch 00435 | Loss 0.2695 | Train Acc. 0.8876 | Validation Acc. 0.8660 \n",
      "Epoch 00440 | Loss 0.2698 | Train Acc. 0.8863 | Validation Acc. 0.8660 \n",
      "Epoch 00445 | Loss 0.2718 | Train Acc. 0.8953 | Validation Acc. 0.8608 \n",
      "Epoch 00450 | Loss 0.2711 | Train Acc. 0.8902 | Validation Acc. 0.8608 \n",
      "Epoch 00455 | Loss 0.2748 | Train Acc. 0.8863 | Validation Acc. 0.8763 \n",
      "Epoch 00460 | Loss 0.2670 | Train Acc. 0.8876 | Validation Acc. 0.8660 \n",
      "Epoch 00465 | Loss 0.2734 | Train Acc. 0.8902 | Validation Acc. 0.8711 \n",
      "Epoch 00470 | Loss 0.2681 | Train Acc. 0.8941 | Validation Acc. 0.8660 \n",
      "Epoch 00475 | Loss 0.2646 | Train Acc. 0.8953 | Validation Acc. 0.8660 \n",
      "Epoch 00480 | Loss 0.2686 | Train Acc. 0.8979 | Validation Acc. 0.8608 \n",
      "Epoch 00485 | Loss 0.2682 | Train Acc. 0.8941 | Validation Acc. 0.8660 \n",
      "Epoch 00490 | Loss 0.2631 | Train Acc. 0.9057 | Validation Acc. 0.8763 \n",
      "Epoch 00495 | Loss 0.2744 | Train Acc. 0.8941 | Validation Acc. 0.8660 \n",
      "Epoch 00500 | Loss 0.2781 | Train Acc. 0.8889 | Validation Acc. 0.8660 \n",
      "Epoch 00505 | Loss 0.2647 | Train Acc. 0.8966 | Validation Acc. 0.8660 \n",
      "Epoch 00510 | Loss 0.2643 | Train Acc. 0.8837 | Validation Acc. 0.8608 \n",
      "Epoch 00515 | Loss 0.2587 | Train Acc. 0.8966 | Validation Acc. 0.8608 \n",
      "Epoch 00520 | Loss 0.2663 | Train Acc. 0.8953 | Validation Acc. 0.8660 \n",
      "Epoch 00525 | Loss 0.2731 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00530 | Loss 0.2638 | Train Acc. 0.8915 | Validation Acc. 0.8557 \n",
      "Epoch 00535 | Loss 0.2605 | Train Acc. 0.8979 | Validation Acc. 0.8660 \n",
      "Epoch 00540 | Loss 0.2648 | Train Acc. 0.8876 | Validation Acc. 0.8608 \n",
      "Epoch 00545 | Loss 0.2817 | Train Acc. 0.8876 | Validation Acc. 0.8608 \n",
      "Epoch 00550 | Loss 0.2706 | Train Acc. 0.8902 | Validation Acc. 0.8660 \n",
      "Epoch 00555 | Loss 0.2705 | Train Acc. 0.9057 | Validation Acc. 0.8660 \n",
      "Epoch 00560 | Loss 0.2638 | Train Acc. 0.8837 | Validation Acc. 0.8608 \n",
      "Epoch 00565 | Loss 0.2758 | Train Acc. 0.8941 | Validation Acc. 0.8660 \n",
      "Epoch 00570 | Loss 0.2694 | Train Acc. 0.8953 | Validation Acc. 0.8660 \n",
      "Epoch 00575 | Loss 0.2720 | Train Acc. 0.8915 | Validation Acc. 0.8557 \n",
      "Epoch 00580 | Loss 0.2585 | Train Acc. 0.9018 | Validation Acc. 0.8763 \n",
      "Epoch 00585 | Loss 0.2630 | Train Acc. 0.9005 | Validation Acc. 0.8660 \n",
      "Epoch 00590 | Loss 0.2635 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00595 | Loss 0.2697 | Train Acc. 0.8915 | Validation Acc. 0.8711 \n",
      "Epoch 00600 | Loss 0.2692 | Train Acc. 0.8915 | Validation Acc. 0.8763 \n",
      "Epoch 00605 | Loss 0.2654 | Train Acc. 0.8928 | Validation Acc. 0.8711 \n",
      "Epoch 00610 | Loss 0.2616 | Train Acc. 0.8876 | Validation Acc. 0.8711 \n",
      "Epoch 00615 | Loss 0.2681 | Train Acc. 0.9005 | Validation Acc. 0.8711 \n",
      "Epoch 00620 | Loss 0.2650 | Train Acc. 0.8992 | Validation Acc. 0.8660 \n",
      "Epoch 00625 | Loss 0.2623 | Train Acc. 0.8850 | Validation Acc. 0.8711 \n",
      "Epoch 00630 | Loss 0.2723 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00635 | Loss 0.2618 | Train Acc. 0.8992 | Validation Acc. 0.8763 \n",
      "Epoch 00640 | Loss 0.2586 | Train Acc. 0.8915 | Validation Acc. 0.8660 \n",
      "Epoch 00645 | Loss 0.2650 | Train Acc. 0.8850 | Validation Acc. 0.8608 \n",
      "Epoch 00650 | Loss 0.2632 | Train Acc. 0.8928 | Validation Acc. 0.8711 \n",
      "Epoch 00655 | Loss 0.2735 | Train Acc. 0.8966 | Validation Acc. 0.8711 \n",
      "Epoch 00660 | Loss 0.2696 | Train Acc. 0.8966 | Validation Acc. 0.8608 \n",
      "Epoch 00665 | Loss 0.2599 | Train Acc. 0.8915 | Validation Acc. 0.8660 \n",
      "Epoch 00670 | Loss 0.2719 | Train Acc. 0.8850 | Validation Acc. 0.8763 \n",
      "Epoch 00675 | Loss 0.2736 | Train Acc. 0.8889 | Validation Acc. 0.8608 \n",
      "Epoch 00680 | Loss 0.2778 | Train Acc. 0.8876 | Validation Acc. 0.8660 \n",
      "Epoch 00685 | Loss 0.2732 | Train Acc. 0.8915 | Validation Acc. 0.8711 \n",
      "Epoch 00690 | Loss 0.2714 | Train Acc. 0.8863 | Validation Acc. 0.8660 \n",
      "Epoch 00695 | Loss 0.2552 | Train Acc. 0.8992 | Validation Acc. 0.8711 \n",
      "Epoch 00700 | Loss 0.2679 | Train Acc. 0.8902 | Validation Acc. 0.8660 \n",
      "Epoch 00705 | Loss 0.2656 | Train Acc. 0.8889 | Validation Acc. 0.8660 \n",
      "Epoch 00710 | Loss 0.2692 | Train Acc. 0.8889 | Validation Acc. 0.8660 \n",
      "Epoch 00715 | Loss 0.2673 | Train Acc. 0.9044 | Validation Acc. 0.8660 \n",
      "Epoch 00720 | Loss 0.2689 | Train Acc. 0.8902 | Validation Acc. 0.8711 \n",
      "Epoch 00725 | Loss 0.2725 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00730 | Loss 0.2751 | Train Acc. 0.8928 | Validation Acc. 0.8660 \n",
      "Epoch 00735 | Loss 0.2594 | Train Acc. 0.9096 | Validation Acc. 0.8711 \n",
      "Epoch 00740 | Loss 0.2705 | Train Acc. 0.8928 | Validation Acc. 0.8660 \n",
      "Epoch 00745 | Loss 0.2649 | Train Acc. 0.8992 | Validation Acc. 0.8763 \n",
      "Epoch 00750 | Loss 0.2686 | Train Acc. 0.8915 | Validation Acc. 0.8608 \n",
      "Epoch 00755 | Loss 0.2750 | Train Acc. 0.8863 | Validation Acc. 0.8711 \n",
      "Epoch 00760 | Loss 0.2743 | Train Acc. 0.8889 | Validation Acc. 0.8660 \n",
      "Epoch 00765 | Loss 0.2628 | Train Acc. 0.8824 | Validation Acc. 0.8711 \n",
      "Epoch 00770 | Loss 0.2768 | Train Acc. 0.8928 | Validation Acc. 0.8608 \n",
      "Epoch 00775 | Loss 0.2715 | Train Acc. 0.8915 | Validation Acc. 0.8608 \n",
      "Epoch 00780 | Loss 0.2627 | Train Acc. 0.8915 | Validation Acc. 0.8711 \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "Fold : 5 | Test Accuracy = 0.8519 | F1 = 0.8202 \n",
      "Total = 6.4Gb \t Reserved = 1.1Gb \t Allocated = 0.4Gb\n",
      "Clearing gpu memory\n",
      "Total = 6.4Gb \t Reserved = 0.3Gb \t Allocated = 0.3Gb\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1076, num_edges=18312,\n",
      "      ndata_schemes={'idx': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(5,), dtype=torch.int64), 'feat': Scheme(shape=(30459,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.6883 | Train Acc. 0.0284 | Validation Acc. 0.0979 \n",
      "Epoch 00005 | Loss 1.0441 | Train Acc. 0.6873 | Validation Acc. 0.1443 \n",
      "Epoch 00010 | Loss 0.8831 | Train Acc. 0.8062 | Validation Acc. 0.4588 \n",
      "Epoch 00015 | Loss 0.7264 | Train Acc. 0.8295 | Validation Acc. 0.7010 \n",
      "Epoch 00020 | Loss 0.6438 | Train Acc. 0.8359 | Validation Acc. 0.7732 \n",
      "Epoch 00025 | Loss 0.5673 | Train Acc. 0.8346 | Validation Acc. 0.7784 \n",
      "Epoch 00030 | Loss 0.4969 | Train Acc. 0.8372 | Validation Acc. 0.8144 \n",
      "Epoch 00035 | Loss 0.4664 | Train Acc. 0.8424 | Validation Acc. 0.8351 \n",
      "Epoch 00040 | Loss 0.4269 | Train Acc. 0.8385 | Validation Acc. 0.8454 \n",
      "Epoch 00045 | Loss 0.3917 | Train Acc. 0.8579 | Validation Acc. 0.8196 \n",
      "Epoch 00050 | Loss 0.3856 | Train Acc. 0.8579 | Validation Acc. 0.8144 \n",
      "Epoch 00055 | Loss 0.3796 | Train Acc. 0.8540 | Validation Acc. 0.8196 \n",
      "Epoch 00060 | Loss 0.3777 | Train Acc. 0.8605 | Validation Acc. 0.8299 \n",
      "Epoch 00065 | Loss 0.3548 | Train Acc. 0.8682 | Validation Acc. 0.8402 \n",
      "Epoch 00070 | Loss 0.3539 | Train Acc. 0.8630 | Validation Acc. 0.8402 \n",
      "Epoch 00075 | Loss 0.3428 | Train Acc. 0.8708 | Validation Acc. 0.8351 \n",
      "Epoch 00080 | Loss 0.3504 | Train Acc. 0.8527 | Validation Acc. 0.8299 \n",
      "Epoch 00085 | Loss 0.3444 | Train Acc. 0.8669 | Validation Acc. 0.8247 \n",
      "Epoch 00090 | Loss 0.3329 | Train Acc. 0.8695 | Validation Acc. 0.8041 \n",
      "Epoch 00095 | Loss 0.3227 | Train Acc. 0.8695 | Validation Acc. 0.8299 \n",
      "Epoch 00100 | Loss 0.3238 | Train Acc. 0.8760 | Validation Acc. 0.8196 \n",
      "Epoch 00105 | Loss 0.3201 | Train Acc. 0.8811 | Validation Acc. 0.8041 \n",
      "Epoch 00110 | Loss 0.3311 | Train Acc. 0.8643 | Validation Acc. 0.8144 \n",
      "Epoch 00115 | Loss 0.3296 | Train Acc. 0.8721 | Validation Acc. 0.8402 \n",
      "Epoch 00120 | Loss 0.3188 | Train Acc. 0.8734 | Validation Acc. 0.8196 \n",
      "Epoch 00125 | Loss 0.3162 | Train Acc. 0.8708 | Validation Acc. 0.8247 \n",
      "Epoch 00130 | Loss 0.3091 | Train Acc. 0.8760 | Validation Acc. 0.8351 \n",
      "Epoch 00135 | Loss 0.3011 | Train Acc. 0.8760 | Validation Acc. 0.8144 \n",
      "Epoch 00140 | Loss 0.3084 | Train Acc. 0.8760 | Validation Acc. 0.8505 \n",
      "Epoch 00145 | Loss 0.2953 | Train Acc. 0.8760 | Validation Acc. 0.8454 \n",
      "Epoch 00150 | Loss 0.3038 | Train Acc. 0.8747 | Validation Acc. 0.8402 \n",
      "Epoch 00155 | Loss 0.2890 | Train Acc. 0.8773 | Validation Acc. 0.8763 \n",
      "Epoch 00160 | Loss 0.3075 | Train Acc. 0.8786 | Validation Acc. 0.8866 \n",
      "Epoch 00165 | Loss 0.2956 | Train Acc. 0.8734 | Validation Acc. 0.9021 \n",
      "Epoch 00170 | Loss 0.2956 | Train Acc. 0.8850 | Validation Acc. 0.9124 \n",
      "Epoch 00175 | Loss 0.2855 | Train Acc. 0.8786 | Validation Acc. 0.9124 \n",
      "Epoch 00180 | Loss 0.2901 | Train Acc. 0.8798 | Validation Acc. 0.9072 \n",
      "Epoch 00185 | Loss 0.2946 | Train Acc. 0.8863 | Validation Acc. 0.8969 \n",
      "Epoch 00190 | Loss 0.2874 | Train Acc. 0.8863 | Validation Acc. 0.9072 \n",
      "Epoch 00195 | Loss 0.2904 | Train Acc. 0.8798 | Validation Acc. 0.9021 \n",
      "Epoch 00200 | Loss 0.2851 | Train Acc. 0.8941 | Validation Acc. 0.8918 \n",
      "Epoch 00205 | Loss 0.2844 | Train Acc. 0.8837 | Validation Acc. 0.9021 \n",
      "Epoch 00210 | Loss 0.2810 | Train Acc. 0.8889 | Validation Acc. 0.8866 \n",
      "Epoch 00215 | Loss 0.2742 | Train Acc. 0.8928 | Validation Acc. 0.9124 \n",
      "Epoch 00220 | Loss 0.2820 | Train Acc. 0.8876 | Validation Acc. 0.9021 \n",
      "Epoch 00225 | Loss 0.2894 | Train Acc. 0.8773 | Validation Acc. 0.8969 \n",
      "Epoch 00230 | Loss 0.2662 | Train Acc. 0.8850 | Validation Acc. 0.8969 \n",
      "Epoch 00235 | Loss 0.2792 | Train Acc. 0.8953 | Validation Acc. 0.8711 \n",
      "Epoch 00240 | Loss 0.2821 | Train Acc. 0.8850 | Validation Acc. 0.8814 \n",
      "Epoch 00245 | Loss 0.2821 | Train Acc. 0.8915 | Validation Acc. 0.8608 \n",
      "Epoch 00250 | Loss 0.2854 | Train Acc. 0.8837 | Validation Acc. 0.8608 \n",
      "Epoch 00255 | Loss 0.2733 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00260 | Loss 0.2750 | Train Acc. 0.8928 | Validation Acc. 0.9072 \n",
      "Epoch 00265 | Loss 0.2760 | Train Acc. 0.8941 | Validation Acc. 0.9021 \n",
      "Epoch 00270 | Loss 0.2730 | Train Acc. 0.8850 | Validation Acc. 0.8918 \n",
      "Epoch 00275 | Loss 0.2664 | Train Acc. 0.8966 | Validation Acc. 0.9072 \n",
      "Epoch 00280 | Loss 0.2750 | Train Acc. 0.8928 | Validation Acc. 0.9021 \n",
      "Epoch 00285 | Loss 0.2646 | Train Acc. 0.8863 | Validation Acc. 0.9021 \n",
      "Epoch 00290 | Loss 0.2724 | Train Acc. 0.8915 | Validation Acc. 0.8969 \n",
      "Epoch 00295 | Loss 0.2724 | Train Acc. 0.8902 | Validation Acc. 0.8969 \n",
      "Epoch 00300 | Loss 0.2569 | Train Acc. 0.8966 | Validation Acc. 0.8866 \n",
      "Epoch 00305 | Loss 0.2699 | Train Acc. 0.8979 | Validation Acc. 0.9072 \n",
      "Epoch 00310 | Loss 0.2723 | Train Acc. 0.8889 | Validation Acc. 0.8969 \n",
      "Epoch 00315 | Loss 0.2612 | Train Acc. 0.8953 | Validation Acc. 0.8969 \n",
      "Epoch 00320 | Loss 0.2677 | Train Acc. 0.8889 | Validation Acc. 0.9021 \n",
      "Epoch 00325 | Loss 0.2724 | Train Acc. 0.8966 | Validation Acc. 0.9072 \n",
      "Epoch 00330 | Loss 0.2726 | Train Acc. 0.8928 | Validation Acc. 0.9072 \n",
      "Epoch 00335 | Loss 0.2696 | Train Acc. 0.8966 | Validation Acc. 0.8918 \n",
      "Epoch 00340 | Loss 0.2566 | Train Acc. 0.8966 | Validation Acc. 0.8866 \n",
      "Epoch 00345 | Loss 0.2760 | Train Acc. 0.8902 | Validation Acc. 0.8969 \n",
      "Epoch 00350 | Loss 0.2591 | Train Acc. 0.8966 | Validation Acc. 0.8969 \n",
      "Epoch 00355 | Loss 0.2674 | Train Acc. 0.8941 | Validation Acc. 0.9021 \n",
      "Epoch 00360 | Loss 0.2599 | Train Acc. 0.8876 | Validation Acc. 0.8969 \n",
      "Epoch 00365 | Loss 0.2635 | Train Acc. 0.8928 | Validation Acc. 0.9072 \n",
      "Epoch 00370 | Loss 0.2627 | Train Acc. 0.8928 | Validation Acc. 0.8866 \n",
      "Epoch 00375 | Loss 0.2736 | Train Acc. 0.8915 | Validation Acc. 0.9021 \n",
      "Epoch 00380 | Loss 0.2614 | Train Acc. 0.8953 | Validation Acc. 0.8969 \n",
      "Epoch 00385 | Loss 0.2705 | Train Acc. 0.8902 | Validation Acc. 0.9072 \n",
      "Epoch 00390 | Loss 0.2685 | Train Acc. 0.8915 | Validation Acc. 0.8969 \n",
      "Epoch 00395 | Loss 0.2574 | Train Acc. 0.8928 | Validation Acc. 0.8969 \n",
      "Epoch 00400 | Loss 0.2739 | Train Acc. 0.8915 | Validation Acc. 0.9021 \n",
      "Epoch 00405 | Loss 0.2636 | Train Acc. 0.8953 | Validation Acc. 0.8969 \n",
      "Epoch 00410 | Loss 0.2589 | Train Acc. 0.9057 | Validation Acc. 0.8918 \n",
      "Epoch 00415 | Loss 0.2643 | Train Acc. 0.9057 | Validation Acc. 0.8969 \n",
      "Epoch 00420 | Loss 0.2584 | Train Acc. 0.8979 | Validation Acc. 0.8918 \n",
      "Epoch 00425 | Loss 0.2662 | Train Acc. 0.8915 | Validation Acc. 0.8918 \n",
      "Epoch 00430 | Loss 0.2582 | Train Acc. 0.8992 | Validation Acc. 0.8969 \n",
      "Epoch 00435 | Loss 0.2627 | Train Acc. 0.8941 | Validation Acc. 0.9021 \n",
      "Epoch 00440 | Loss 0.2666 | Train Acc. 0.8889 | Validation Acc. 0.8969 \n",
      "Epoch 00445 | Loss 0.2570 | Train Acc. 0.8941 | Validation Acc. 0.8866 \n",
      "Epoch 00450 | Loss 0.2631 | Train Acc. 0.8992 | Validation Acc. 0.8969 \n",
      "Epoch 00455 | Loss 0.2691 | Train Acc. 0.8953 | Validation Acc. 0.8918 \n",
      "Epoch 00460 | Loss 0.2640 | Train Acc. 0.9005 | Validation Acc. 0.8814 \n",
      "Epoch 00465 | Loss 0.2584 | Train Acc. 0.8966 | Validation Acc. 0.8866 \n",
      "Epoch 00470 | Loss 0.2702 | Train Acc. 0.8953 | Validation Acc. 0.8814 \n",
      "Epoch 00475 | Loss 0.2666 | Train Acc. 0.8928 | Validation Acc. 0.8814 \n",
      "Epoch 00480 | Loss 0.2599 | Train Acc. 0.8902 | Validation Acc. 0.8918 \n",
      "Epoch 00485 | Loss 0.2711 | Train Acc. 0.8863 | Validation Acc. 0.8969 \n",
      "Epoch 00490 | Loss 0.2616 | Train Acc. 0.9070 | Validation Acc. 0.9021 \n",
      "Epoch 00495 | Loss 0.2699 | Train Acc. 0.8876 | Validation Acc. 0.8866 \n",
      "Epoch 00500 | Loss 0.2580 | Train Acc. 0.8966 | Validation Acc. 0.8969 \n",
      "Epoch 00505 | Loss 0.2566 | Train Acc. 0.8902 | Validation Acc. 0.8969 \n",
      "Epoch 00510 | Loss 0.2708 | Train Acc. 0.8941 | Validation Acc. 0.8814 \n",
      "Epoch 00515 | Loss 0.2545 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 00520 | Loss 0.2701 | Train Acc. 0.9005 | Validation Acc. 0.8866 \n",
      "Epoch 00525 | Loss 0.2585 | Train Acc. 0.9005 | Validation Acc. 0.9072 \n",
      "Epoch 00530 | Loss 0.2663 | Train Acc. 0.8992 | Validation Acc. 0.9021 \n",
      "Epoch 00535 | Loss 0.2660 | Train Acc. 0.9031 | Validation Acc. 0.8866 \n",
      "Epoch 00540 | Loss 0.2657 | Train Acc. 0.8941 | Validation Acc. 0.8763 \n",
      "Epoch 00545 | Loss 0.2546 | Train Acc. 0.8966 | Validation Acc. 0.8918 \n",
      "Epoch 00550 | Loss 0.2715 | Train Acc. 0.8953 | Validation Acc. 0.8918 \n",
      "Epoch 00555 | Loss 0.2620 | Train Acc. 0.9005 | Validation Acc. 0.8918 \n",
      "Epoch 00560 | Loss 0.2700 | Train Acc. 0.8863 | Validation Acc. 0.9124 \n",
      "Epoch 00565 | Loss 0.2538 | Train Acc. 0.8889 | Validation Acc. 0.9021 \n",
      "Epoch 00570 | Loss 0.2564 | Train Acc. 0.9005 | Validation Acc. 0.8969 \n",
      "Epoch 00575 | Loss 0.2569 | Train Acc. 0.8941 | Validation Acc. 0.9021 \n",
      "Epoch 00580 | Loss 0.2626 | Train Acc. 0.8915 | Validation Acc. 0.8866 \n",
      "Epoch 00585 | Loss 0.2589 | Train Acc. 0.8928 | Validation Acc. 0.8918 \n",
      "Epoch 00590 | Loss 0.2695 | Train Acc. 0.9031 | Validation Acc. 0.8969 \n",
      "Epoch 00595 | Loss 0.2656 | Train Acc. 0.8979 | Validation Acc. 0.8969 \n",
      "Epoch 00600 | Loss 0.2592 | Train Acc. 0.8992 | Validation Acc. 0.8969 \n",
      "Epoch 00605 | Loss 0.2648 | Train Acc. 0.8953 | Validation Acc. 0.8918 \n",
      "Epoch 00610 | Loss 0.2645 | Train Acc. 0.8902 | Validation Acc. 0.8918 \n",
      "Epoch 00615 | Loss 0.2635 | Train Acc. 0.8941 | Validation Acc. 0.8969 \n",
      "Epoch 00620 | Loss 0.2530 | Train Acc. 0.9044 | Validation Acc. 0.8866 \n",
      "Epoch 00625 | Loss 0.2658 | Train Acc. 0.8966 | Validation Acc. 0.9021 \n",
      "Epoch 00630 | Loss 0.2565 | Train Acc. 0.8953 | Validation Acc. 0.8969 \n",
      "Epoch 00635 | Loss 0.2720 | Train Acc. 0.8876 | Validation Acc. 0.8918 \n",
      "Epoch 00640 | Loss 0.2610 | Train Acc. 0.8915 | Validation Acc. 0.8918 \n",
      "Epoch 00645 | Loss 0.2688 | Train Acc. 0.8953 | Validation Acc. 0.9021 \n",
      "Epoch 00650 | Loss 0.2588 | Train Acc. 0.8915 | Validation Acc. 0.9021 \n",
      "Epoch 00655 | Loss 0.2672 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 00660 | Loss 0.2569 | Train Acc. 0.8928 | Validation Acc. 0.9021 \n",
      "Epoch 00665 | Loss 0.2559 | Train Acc. 0.8953 | Validation Acc. 0.8918 \n",
      "Epoch 00670 | Loss 0.2636 | Train Acc. 0.8915 | Validation Acc. 0.9021 \n",
      "Epoch 00675 | Loss 0.2601 | Train Acc. 0.9057 | Validation Acc. 0.8918 \n",
      "Epoch 00680 | Loss 0.2658 | Train Acc. 0.8966 | Validation Acc. 0.9021 \n",
      "Epoch 00685 | Loss 0.2617 | Train Acc. 0.8992 | Validation Acc. 0.9021 \n",
      "Epoch 00690 | Loss 0.2602 | Train Acc. 0.8979 | Validation Acc. 0.8918 \n",
      "Epoch 00695 | Loss 0.2632 | Train Acc. 0.8941 | Validation Acc. 0.8866 \n",
      "Epoch 00700 | Loss 0.2658 | Train Acc. 0.8953 | Validation Acc. 0.8814 \n",
      "Epoch 00705 | Loss 0.2515 | Train Acc. 0.8966 | Validation Acc. 0.8866 \n",
      "Epoch 00710 | Loss 0.2659 | Train Acc. 0.8928 | Validation Acc. 0.8866 \n",
      "Epoch 00715 | Loss 0.2732 | Train Acc. 0.8863 | Validation Acc. 0.8866 \n",
      "Epoch 00720 | Loss 0.2589 | Train Acc. 0.8953 | Validation Acc. 0.8969 \n",
      "Epoch 00725 | Loss 0.2573 | Train Acc. 0.8928 | Validation Acc. 0.8918 \n",
      "Epoch 00730 | Loss 0.2562 | Train Acc. 0.8992 | Validation Acc. 0.8866 \n",
      "Epoch 00735 | Loss 0.2620 | Train Acc. 0.8889 | Validation Acc. 0.8763 \n",
      "Epoch 00740 | Loss 0.2682 | Train Acc. 0.8966 | Validation Acc. 0.8866 \n",
      "Epoch 00745 | Loss 0.2616 | Train Acc. 0.9005 | Validation Acc. 0.8918 \n",
      "Epoch 00750 | Loss 0.2568 | Train Acc. 0.8966 | Validation Acc. 0.8969 \n",
      "Epoch 00755 | Loss 0.2626 | Train Acc. 0.8941 | Validation Acc. 0.8969 \n",
      "Epoch 00760 | Loss 0.2613 | Train Acc. 0.8992 | Validation Acc. 0.9021 \n",
      "Epoch 00765 | Loss 0.2618 | Train Acc. 0.8966 | Validation Acc. 0.9021 \n",
      "Epoch 00770 | Loss 0.2617 | Train Acc. 0.8928 | Validation Acc. 0.8918 \n",
      "Epoch 00775 | Loss 0.2661 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 00780 | Loss 0.2604 | Train Acc. 0.8966 | Validation Acc. 0.8918 \n",
      "Epoch 00785 | Loss 0.2557 | Train Acc. 0.8953 | Validation Acc. 0.9175 \n",
      "Epoch 00790 | Loss 0.2580 | Train Acc. 0.9005 | Validation Acc. 0.8866 \n",
      "Epoch 00795 | Loss 0.2589 | Train Acc. 0.8915 | Validation Acc. 0.8866 \n",
      "Epoch 00800 | Loss 0.2643 | Train Acc. 0.8979 | Validation Acc. 0.8918 \n",
      "Epoch 00805 | Loss 0.2651 | Train Acc. 0.8928 | Validation Acc. 0.8969 \n",
      "Epoch 00810 | Loss 0.2529 | Train Acc. 0.8953 | Validation Acc. 0.8969 \n",
      "Epoch 00815 | Loss 0.2629 | Train Acc. 0.8889 | Validation Acc. 0.8969 \n",
      "Epoch 00820 | Loss 0.2636 | Train Acc. 0.9005 | Validation Acc. 0.8918 \n",
      "Epoch 00825 | Loss 0.2607 | Train Acc. 0.8941 | Validation Acc. 0.9021 \n",
      "Epoch 00830 | Loss 0.2540 | Train Acc. 0.9057 | Validation Acc. 0.8918 \n",
      "Epoch 00835 | Loss 0.2658 | Train Acc. 0.8915 | Validation Acc. 0.8918 \n",
      "Epoch 00840 | Loss 0.2588 | Train Acc. 0.8915 | Validation Acc. 0.8866 \n",
      "Epoch 00845 | Loss 0.2603 | Train Acc. 0.8889 | Validation Acc. 0.9021 \n",
      "Epoch 00850 | Loss 0.2625 | Train Acc. 0.9044 | Validation Acc. 0.8918 \n",
      "Epoch 00855 | Loss 0.2574 | Train Acc. 0.8979 | Validation Acc. 0.8969 \n",
      "Epoch 00860 | Loss 0.2675 | Train Acc. 0.8902 | Validation Acc. 0.9021 \n",
      "Epoch 00865 | Loss 0.2618 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 00870 | Loss 0.2643 | Train Acc. 0.8953 | Validation Acc. 0.8918 \n",
      "Epoch 00875 | Loss 0.2595 | Train Acc. 0.8992 | Validation Acc. 0.9072 \n",
      "Epoch 00880 | Loss 0.2635 | Train Acc. 0.8889 | Validation Acc. 0.8918 \n",
      "Epoch 00885 | Loss 0.2658 | Train Acc. 0.8966 | Validation Acc. 0.8969 \n",
      "Epoch 00890 | Loss 0.2646 | Train Acc. 0.8902 | Validation Acc. 0.8918 \n",
      "Epoch 00895 | Loss 0.2671 | Train Acc. 0.8941 | Validation Acc. 0.8969 \n",
      "Epoch 00900 | Loss 0.2638 | Train Acc. 0.8992 | Validation Acc. 0.8918 \n",
      "Epoch 00905 | Loss 0.2562 | Train Acc. 0.9005 | Validation Acc. 0.8918 \n",
      "Epoch 00910 | Loss 0.2583 | Train Acc. 0.8966 | Validation Acc. 0.9021 \n",
      "Epoch 00915 | Loss 0.2671 | Train Acc. 0.8979 | Validation Acc. 0.8918 \n",
      "Epoch 00920 | Loss 0.2598 | Train Acc. 0.8928 | Validation Acc. 0.9021 \n",
      "Epoch 00925 | Loss 0.2650 | Train Acc. 0.8941 | Validation Acc. 0.8969 \n",
      "Epoch 00930 | Loss 0.2617 | Train Acc. 0.9005 | Validation Acc. 0.8969 \n",
      "Epoch 00935 | Loss 0.2617 | Train Acc. 0.8928 | Validation Acc. 0.8969 \n",
      "Epoch 00940 | Loss 0.2557 | Train Acc. 0.8953 | Validation Acc. 0.9021 \n",
      "Epoch 00945 | Loss 0.2614 | Train Acc. 0.8941 | Validation Acc. 0.8814 \n",
      "Epoch 00950 | Loss 0.2524 | Train Acc. 0.9031 | Validation Acc. 0.8866 \n",
      "Epoch 00955 | Loss 0.2676 | Train Acc. 0.8953 | Validation Acc. 0.8814 \n",
      "Epoch 00960 | Loss 0.2593 | Train Acc. 0.8953 | Validation Acc. 0.8969 \n",
      "Epoch 00965 | Loss 0.2600 | Train Acc. 0.8953 | Validation Acc. 0.8969 \n",
      "Epoch 00970 | Loss 0.2606 | Train Acc. 0.8966 | Validation Acc. 0.8918 \n",
      "Epoch 00975 | Loss 0.2647 | Train Acc. 0.8966 | Validation Acc. 0.8918 \n",
      "Epoch 00980 | Loss 0.2613 | Train Acc. 0.8863 | Validation Acc. 0.8969 \n",
      "Epoch 00985 | Loss 0.2636 | Train Acc. 0.8850 | Validation Acc. 0.9072 \n",
      "Epoch 00990 | Loss 0.2681 | Train Acc. 0.9005 | Validation Acc. 0.8969 \n",
      "Epoch 00995 | Loss 0.2595 | Train Acc. 0.8915 | Validation Acc. 0.8969 \n",
      "Epoch 01000 | Loss 0.2599 | Train Acc. 0.8966 | Validation Acc. 0.8866 \n",
      "Epoch 01005 | Loss 0.2634 | Train Acc. 0.9018 | Validation Acc. 0.8918 \n",
      "Epoch 01010 | Loss 0.2591 | Train Acc. 0.8953 | Validation Acc. 0.8969 \n",
      "Epoch 01015 | Loss 0.2682 | Train Acc. 0.8979 | Validation Acc. 0.8918 \n",
      "Epoch 01020 | Loss 0.2576 | Train Acc. 0.8979 | Validation Acc. 0.9021 \n",
      "Epoch 01025 | Loss 0.2678 | Train Acc. 0.8811 | Validation Acc. 0.8866 \n",
      "Epoch 01030 | Loss 0.2657 | Train Acc. 0.8889 | Validation Acc. 0.8969 \n",
      "Epoch 01035 | Loss 0.2613 | Train Acc. 0.8992 | Validation Acc. 0.9124 \n",
      "Epoch 01040 | Loss 0.2574 | Train Acc. 0.8928 | Validation Acc. 0.8969 \n",
      "Epoch 01045 | Loss 0.2601 | Train Acc. 0.9044 | Validation Acc. 0.8763 \n",
      "Epoch 01050 | Loss 0.2676 | Train Acc. 0.8953 | Validation Acc. 0.9021 \n",
      "Epoch 01055 | Loss 0.2634 | Train Acc. 0.8915 | Validation Acc. 0.8918 \n",
      "Epoch 01060 | Loss 0.2621 | Train Acc. 0.8992 | Validation Acc. 0.8918 \n",
      "Epoch 01065 | Loss 0.2604 | Train Acc. 0.8979 | Validation Acc. 0.8969 \n",
      "Epoch 01070 | Loss 0.2597 | Train Acc. 0.9005 | Validation Acc. 0.8918 \n",
      "Epoch 01075 | Loss 0.2498 | Train Acc. 0.8992 | Validation Acc. 0.8866 \n",
      "Epoch 01080 | Loss 0.2631 | Train Acc. 0.8928 | Validation Acc. 0.8969 \n",
      "Epoch 01085 | Loss 0.2576 | Train Acc. 0.9031 | Validation Acc. 0.8814 \n",
      "Epoch 01090 | Loss 0.2617 | Train Acc. 0.8992 | Validation Acc. 0.9072 \n",
      "Epoch 01095 | Loss 0.2531 | Train Acc. 0.9005 | Validation Acc. 0.8969 \n",
      "Epoch 01100 | Loss 0.2637 | Train Acc. 0.8889 | Validation Acc. 0.8866 \n",
      "Epoch 01105 | Loss 0.2664 | Train Acc. 0.8941 | Validation Acc. 0.8918 \n",
      "Epoch 01110 | Loss 0.2582 | Train Acc. 0.8941 | Validation Acc. 0.8918 \n",
      "Epoch 01115 | Loss 0.2652 | Train Acc. 0.8850 | Validation Acc. 0.9021 \n",
      "Epoch 01120 | Loss 0.2684 | Train Acc. 0.8953 | Validation Acc. 0.8918 \n",
      "Epoch 01125 | Loss 0.2607 | Train Acc. 0.8915 | Validation Acc. 0.9021 \n",
      "Epoch 01130 | Loss 0.2575 | Train Acc. 0.9083 | Validation Acc. 0.8814 \n",
      "Epoch 01135 | Loss 0.2706 | Train Acc. 0.8915 | Validation Acc. 0.9072 \n",
      "Epoch 01140 | Loss 0.2615 | Train Acc. 0.8915 | Validation Acc. 0.8969 \n",
      "Epoch 01145 | Loss 0.2587 | Train Acc. 0.8915 | Validation Acc. 0.8866 \n",
      "Epoch 01150 | Loss 0.2603 | Train Acc. 0.8979 | Validation Acc. 0.9021 \n",
      "Epoch 01155 | Loss 0.2689 | Train Acc. 0.8966 | Validation Acc. 0.8918 \n",
      "Epoch 01160 | Loss 0.2650 | Train Acc. 0.8889 | Validation Acc. 0.8918 \n",
      "Epoch 01165 | Loss 0.2596 | Train Acc. 0.8928 | Validation Acc. 0.8866 \n",
      "Epoch 01170 | Loss 0.2621 | Train Acc. 0.8953 | Validation Acc. 0.9021 \n",
      "Epoch 01175 | Loss 0.2584 | Train Acc. 0.8966 | Validation Acc. 0.8969 \n",
      "Epoch 01180 | Loss 0.2647 | Train Acc. 0.8941 | Validation Acc. 0.8969 \n",
      "Epoch 01185 | Loss 0.2635 | Train Acc. 0.8966 | Validation Acc. 0.8918 \n",
      "Epoch 01190 | Loss 0.2591 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 01195 | Loss 0.2601 | Train Acc. 0.9044 | Validation Acc. 0.8918 \n",
      "Epoch 01200 | Loss 0.2595 | Train Acc. 0.8966 | Validation Acc. 0.9021 \n",
      "Epoch 01205 | Loss 0.2643 | Train Acc. 0.8889 | Validation Acc. 0.9021 \n",
      "Epoch 01210 | Loss 0.2621 | Train Acc. 0.8966 | Validation Acc. 0.9021 \n",
      "Epoch 01215 | Loss 0.2580 | Train Acc. 0.8979 | Validation Acc. 0.8969 \n",
      "Epoch 01220 | Loss 0.2663 | Train Acc. 0.8992 | Validation Acc. 0.8866 \n",
      "Epoch 01225 | Loss 0.2607 | Train Acc. 0.8992 | Validation Acc. 0.8969 \n",
      "Epoch 01230 | Loss 0.2679 | Train Acc. 0.8915 | Validation Acc. 0.8866 \n",
      "Epoch 01235 | Loss 0.2632 | Train Acc. 0.8979 | Validation Acc. 0.8814 \n",
      "Epoch 01240 | Loss 0.2564 | Train Acc. 0.8941 | Validation Acc. 0.8918 \n",
      "Epoch 01245 | Loss 0.2610 | Train Acc. 0.8941 | Validation Acc. 0.8814 \n",
      "Epoch 01250 | Loss 0.2576 | Train Acc. 0.8992 | Validation Acc. 0.8866 \n",
      "Epoch 01255 | Loss 0.2591 | Train Acc. 0.9031 | Validation Acc. 0.8969 \n",
      "Epoch 01260 | Loss 0.2667 | Train Acc. 0.8902 | Validation Acc. 0.8814 \n",
      "Epoch 01265 | Loss 0.2703 | Train Acc. 0.8876 | Validation Acc. 0.8918 \n",
      "Epoch 01270 | Loss 0.2553 | Train Acc. 0.8941 | Validation Acc. 0.8866 \n",
      "Epoch 01275 | Loss 0.2665 | Train Acc. 0.8941 | Validation Acc. 0.9021 \n",
      "Epoch 01280 | Loss 0.2546 | Train Acc. 0.8876 | Validation Acc. 0.8918 \n",
      "Epoch 01285 | Loss 0.2619 | Train Acc. 0.8928 | Validation Acc. 0.8969 \n",
      "Epoch 01290 | Loss 0.2615 | Train Acc. 0.8966 | Validation Acc. 0.8866 \n",
      "Epoch 01295 | Loss 0.2542 | Train Acc. 0.8953 | Validation Acc. 0.8969 \n",
      "Epoch 01300 | Loss 0.2572 | Train Acc. 0.8941 | Validation Acc. 0.8969 \n",
      "Epoch 01305 | Loss 0.2698 | Train Acc. 0.8876 | Validation Acc. 0.8969 \n",
      "Epoch 01310 | Loss 0.2625 | Train Acc. 0.8992 | Validation Acc. 0.8918 \n",
      "Epoch 01315 | Loss 0.2570 | Train Acc. 0.8966 | Validation Acc. 0.8918 \n",
      "Epoch 01320 | Loss 0.2554 | Train Acc. 0.8966 | Validation Acc. 0.8969 \n",
      "Epoch 01325 | Loss 0.2598 | Train Acc. 0.8928 | Validation Acc. 0.8969 \n",
      "Epoch 01330 | Loss 0.2607 | Train Acc. 0.8953 | Validation Acc. 0.8814 \n",
      "Epoch 01335 | Loss 0.2612 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 01340 | Loss 0.2690 | Train Acc. 0.9005 | Validation Acc. 0.8918 \n",
      "Epoch 01345 | Loss 0.2677 | Train Acc. 0.8941 | Validation Acc. 0.8918 \n",
      "Epoch 01350 | Loss 0.2705 | Train Acc. 0.8953 | Validation Acc. 0.9072 \n",
      "Epoch 01355 | Loss 0.2628 | Train Acc. 0.9005 | Validation Acc. 0.8918 \n",
      "Epoch 01360 | Loss 0.2529 | Train Acc. 0.8941 | Validation Acc. 0.8866 \n",
      "Epoch 01365 | Loss 0.2605 | Train Acc. 0.9031 | Validation Acc. 0.8969 \n",
      "Epoch 01370 | Loss 0.2736 | Train Acc. 0.8876 | Validation Acc. 0.9072 \n",
      "Epoch 01375 | Loss 0.2515 | Train Acc. 0.9018 | Validation Acc. 0.8814 \n",
      "Epoch 01380 | Loss 0.2640 | Train Acc. 0.8992 | Validation Acc. 0.8918 \n",
      "Epoch 01385 | Loss 0.2650 | Train Acc. 0.8876 | Validation Acc. 0.8814 \n",
      "Epoch 01390 | Loss 0.2707 | Train Acc. 0.8915 | Validation Acc. 0.8814 \n",
      "Epoch 01395 | Loss 0.2749 | Train Acc. 0.8928 | Validation Acc. 0.8814 \n",
      "Epoch 01400 | Loss 0.2619 | Train Acc. 0.8966 | Validation Acc. 0.9072 \n",
      "Epoch 01405 | Loss 0.2715 | Train Acc. 0.8902 | Validation Acc. 0.8866 \n",
      "Epoch 01410 | Loss 0.2603 | Train Acc. 0.9005 | Validation Acc. 0.8866 \n",
      "Epoch 01415 | Loss 0.2697 | Train Acc. 0.8902 | Validation Acc. 0.8814 \n",
      "Epoch 01420 | Loss 0.2622 | Train Acc. 0.8979 | Validation Acc. 0.8918 \n",
      "Epoch 01425 | Loss 0.2617 | Train Acc. 0.9005 | Validation Acc. 0.8814 \n",
      "Epoch 01430 | Loss 0.2617 | Train Acc. 0.9070 | Validation Acc. 0.9072 \n",
      "Epoch 01435 | Loss 0.2616 | Train Acc. 0.8824 | Validation Acc. 0.8866 \n",
      "Epoch 01440 | Loss 0.2655 | Train Acc. 0.8915 | Validation Acc. 0.8969 \n",
      "Epoch 01445 | Loss 0.2635 | Train Acc. 0.8979 | Validation Acc. 0.8918 \n",
      "Epoch 01450 | Loss 0.2545 | Train Acc. 0.8979 | Validation Acc. 0.8969 \n",
      "Epoch 01455 | Loss 0.2566 | Train Acc. 0.9070 | Validation Acc. 0.8814 \n",
      "Epoch 01460 | Loss 0.2556 | Train Acc. 0.9005 | Validation Acc. 0.9072 \n",
      "Epoch 01465 | Loss 0.2603 | Train Acc. 0.8863 | Validation Acc. 0.8918 \n",
      "Epoch 01470 | Loss 0.2591 | Train Acc. 0.9070 | Validation Acc. 0.8918 \n",
      "Epoch 01475 | Loss 0.2642 | Train Acc. 0.9018 | Validation Acc. 0.8866 \n",
      "Epoch 01480 | Loss 0.2705 | Train Acc. 0.8953 | Validation Acc. 0.8918 \n",
      "Epoch 01485 | Loss 0.2651 | Train Acc. 0.8876 | Validation Acc. 0.8866 \n",
      "Epoch 01490 | Loss 0.2613 | Train Acc. 0.8966 | Validation Acc. 0.8866 \n",
      "Epoch 01495 | Loss 0.2558 | Train Acc. 0.8979 | Validation Acc. 0.8866 \n",
      "Epoch 01500 | Loss 0.2519 | Train Acc. 0.8953 | Validation Acc. 0.8918 \n",
      "Epoch 01505 | Loss 0.2672 | Train Acc. 0.8979 | Validation Acc. 0.9072 \n",
      "Epoch 01510 | Loss 0.2647 | Train Acc. 0.8979 | Validation Acc. 0.8969 \n",
      "Epoch 01515 | Loss 0.2608 | Train Acc. 0.9044 | Validation Acc. 0.9021 \n",
      "Epoch 01520 | Loss 0.2570 | Train Acc. 0.8941 | Validation Acc. 0.8866 \n",
      "Epoch 01525 | Loss 0.2649 | Train Acc. 0.9018 | Validation Acc. 0.8969 \n",
      "Epoch 01530 | Loss 0.2696 | Train Acc. 0.8915 | Validation Acc. 0.8969 \n",
      "Epoch 01535 | Loss 0.2546 | Train Acc. 0.8953 | Validation Acc. 0.8866 \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "Fold : 6 | Test Accuracy = 0.8241 | F1 = 0.7926 \n",
      "Total = 6.4Gb \t Reserved = 1.1Gb \t Allocated = 0.4Gb\n",
      "Clearing gpu memory\n",
      "Total = 6.4Gb \t Reserved = 0.3Gb \t Allocated = 0.3Gb\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1076, num_edges=18312,\n",
      "      ndata_schemes={'idx': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(5,), dtype=torch.int64), 'feat': Scheme(shape=(30459,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.7151 | Train Acc. 0.1213 | Validation Acc. 0.2010 \n",
      "Epoch 00005 | Loss 0.9952 | Train Acc. 0.6374 | Validation Acc. 0.2062 \n",
      "Epoch 00010 | Loss 0.8407 | Train Acc. 0.8116 | Validation Acc. 0.2010 \n",
      "Epoch 00015 | Loss 0.6828 | Train Acc. 0.8452 | Validation Acc. 0.5670 \n",
      "Epoch 00020 | Loss 0.5964 | Train Acc. 0.8490 | Validation Acc. 0.6907 \n",
      "Epoch 00025 | Loss 0.5241 | Train Acc. 0.8439 | Validation Acc. 0.7474 \n",
      "Epoch 00030 | Loss 0.4584 | Train Acc. 0.8529 | Validation Acc. 0.7577 \n",
      "Epoch 00035 | Loss 0.4195 | Train Acc. 0.8555 | Validation Acc. 0.7680 \n",
      "Epoch 00040 | Loss 0.3892 | Train Acc. 0.8671 | Validation Acc. 0.7887 \n",
      "Epoch 00045 | Loss 0.3735 | Train Acc. 0.8671 | Validation Acc. 0.8196 \n",
      "Epoch 00050 | Loss 0.3524 | Train Acc. 0.8774 | Validation Acc. 0.8144 \n",
      "Epoch 00055 | Loss 0.3464 | Train Acc. 0.8723 | Validation Acc. 0.8299 \n",
      "Epoch 00060 | Loss 0.3299 | Train Acc. 0.8774 | Validation Acc. 0.8196 \n",
      "Epoch 00065 | Loss 0.3346 | Train Acc. 0.8710 | Validation Acc. 0.8247 \n",
      "Epoch 00070 | Loss 0.3266 | Train Acc. 0.8697 | Validation Acc. 0.8299 \n",
      "Epoch 00075 | Loss 0.3252 | Train Acc. 0.8632 | Validation Acc. 0.7784 \n",
      "Epoch 00080 | Loss 0.3151 | Train Acc. 0.8710 | Validation Acc. 0.7680 \n",
      "Epoch 00085 | Loss 0.3099 | Train Acc. 0.8774 | Validation Acc. 0.7629 \n",
      "Epoch 00090 | Loss 0.3137 | Train Acc. 0.8800 | Validation Acc. 0.7680 \n",
      "Epoch 00095 | Loss 0.3061 | Train Acc. 0.8787 | Validation Acc. 0.7680 \n",
      "Epoch 00100 | Loss 0.2980 | Train Acc. 0.8826 | Validation Acc. 0.7113 \n",
      "Epoch 00105 | Loss 0.2915 | Train Acc. 0.8890 | Validation Acc. 0.7526 \n",
      "Epoch 00110 | Loss 0.2917 | Train Acc. 0.8852 | Validation Acc. 0.7835 \n",
      "Epoch 00115 | Loss 0.2885 | Train Acc. 0.8813 | Validation Acc. 0.7990 \n",
      "Epoch 00120 | Loss 0.2858 | Train Acc. 0.8865 | Validation Acc. 0.7887 \n",
      "Epoch 00125 | Loss 0.2884 | Train Acc. 0.8813 | Validation Acc. 0.7732 \n",
      "Epoch 00130 | Loss 0.2971 | Train Acc. 0.8761 | Validation Acc. 0.7784 \n",
      "Epoch 00135 | Loss 0.2956 | Train Acc. 0.8787 | Validation Acc. 0.7835 \n",
      "Epoch 00140 | Loss 0.2932 | Train Acc. 0.8839 | Validation Acc. 0.7990 \n",
      "Epoch 00145 | Loss 0.2835 | Train Acc. 0.8852 | Validation Acc. 0.7835 \n",
      "Epoch 00150 | Loss 0.2702 | Train Acc. 0.8942 | Validation Acc. 0.7887 \n",
      "Epoch 00155 | Loss 0.2751 | Train Acc. 0.8865 | Validation Acc. 0.8196 \n",
      "Epoch 00160 | Loss 0.2725 | Train Acc. 0.8890 | Validation Acc. 0.8660 \n",
      "Epoch 00165 | Loss 0.2666 | Train Acc. 0.8903 | Validation Acc. 0.8454 \n",
      "Epoch 00170 | Loss 0.2658 | Train Acc. 0.8916 | Validation Acc. 0.8660 \n",
      "Epoch 00175 | Loss 0.2619 | Train Acc. 0.8955 | Validation Acc. 0.8660 \n",
      "Epoch 00180 | Loss 0.2671 | Train Acc. 0.8852 | Validation Acc. 0.8557 \n",
      "Epoch 00185 | Loss 0.2585 | Train Acc. 0.8877 | Validation Acc. 0.8763 \n",
      "Epoch 00190 | Loss 0.2588 | Train Acc. 0.8890 | Validation Acc. 0.8351 \n",
      "Epoch 00195 | Loss 0.2714 | Train Acc. 0.8865 | Validation Acc. 0.8351 \n",
      "Epoch 00200 | Loss 0.2651 | Train Acc. 0.8916 | Validation Acc. 0.8299 \n",
      "Epoch 00205 | Loss 0.2543 | Train Acc. 0.9019 | Validation Acc. 0.8711 \n",
      "Epoch 00210 | Loss 0.2644 | Train Acc. 0.8903 | Validation Acc. 0.8557 \n",
      "Epoch 00215 | Loss 0.2631 | Train Acc. 0.8877 | Validation Acc. 0.8608 \n",
      "Epoch 00220 | Loss 0.2562 | Train Acc. 0.8981 | Validation Acc. 0.8351 \n",
      "Epoch 00225 | Loss 0.2627 | Train Acc. 0.8929 | Validation Acc. 0.8402 \n",
      "Epoch 00230 | Loss 0.2552 | Train Acc. 0.8865 | Validation Acc. 0.8299 \n",
      "Epoch 00235 | Loss 0.2516 | Train Acc. 0.9045 | Validation Acc. 0.8041 \n",
      "Epoch 00240 | Loss 0.2535 | Train Acc. 0.8852 | Validation Acc. 0.8299 \n",
      "Epoch 00245 | Loss 0.2585 | Train Acc. 0.8955 | Validation Acc. 0.8299 \n",
      "Epoch 00250 | Loss 0.2520 | Train Acc. 0.8916 | Validation Acc. 0.8505 \n",
      "Epoch 00255 | Loss 0.2562 | Train Acc. 0.8955 | Validation Acc. 0.8454 \n",
      "Epoch 00260 | Loss 0.2641 | Train Acc. 0.8903 | Validation Acc. 0.8454 \n",
      "Epoch 00265 | Loss 0.2541 | Train Acc. 0.8994 | Validation Acc. 0.8402 \n",
      "Epoch 00270 | Loss 0.2542 | Train Acc. 0.8929 | Validation Acc. 0.8505 \n",
      "Epoch 00275 | Loss 0.2442 | Train Acc. 0.8981 | Validation Acc. 0.8711 \n",
      "Epoch 00280 | Loss 0.2518 | Train Acc. 0.9019 | Validation Acc. 0.8608 \n",
      "Epoch 00285 | Loss 0.2476 | Train Acc. 0.9006 | Validation Acc. 0.8505 \n",
      "Epoch 00290 | Loss 0.2560 | Train Acc. 0.8968 | Validation Acc. 0.8402 \n",
      "Epoch 00295 | Loss 0.2543 | Train Acc. 0.8929 | Validation Acc. 0.8608 \n",
      "Epoch 00300 | Loss 0.2501 | Train Acc. 0.8929 | Validation Acc. 0.8505 \n",
      "Epoch 00305 | Loss 0.2467 | Train Acc. 0.9058 | Validation Acc. 0.8505 \n",
      "Epoch 00310 | Loss 0.2499 | Train Acc. 0.8994 | Validation Acc. 0.8505 \n",
      "Epoch 00315 | Loss 0.2514 | Train Acc. 0.8981 | Validation Acc. 0.8505 \n",
      "Epoch 00320 | Loss 0.2420 | Train Acc. 0.9084 | Validation Acc. 0.8454 \n",
      "Epoch 00325 | Loss 0.2611 | Train Acc. 0.8968 | Validation Acc. 0.8299 \n",
      "Epoch 00330 | Loss 0.2456 | Train Acc. 0.8994 | Validation Acc. 0.8557 \n",
      "Epoch 00335 | Loss 0.2548 | Train Acc. 0.8994 | Validation Acc. 0.8402 \n",
      "Epoch 00340 | Loss 0.2508 | Train Acc. 0.9045 | Validation Acc. 0.8454 \n",
      "Epoch 00345 | Loss 0.2496 | Train Acc. 0.8968 | Validation Acc. 0.8454 \n",
      "Epoch 00350 | Loss 0.2513 | Train Acc. 0.8981 | Validation Acc. 0.8402 \n",
      "Epoch 00355 | Loss 0.2448 | Train Acc. 0.8942 | Validation Acc. 0.8454 \n",
      "Epoch 00360 | Loss 0.2494 | Train Acc. 0.8994 | Validation Acc. 0.8454 \n",
      "Epoch 00365 | Loss 0.2470 | Train Acc. 0.9019 | Validation Acc. 0.8505 \n",
      "Epoch 00370 | Loss 0.2461 | Train Acc. 0.9058 | Validation Acc. 0.8608 \n",
      "Epoch 00375 | Loss 0.2435 | Train Acc. 0.8955 | Validation Acc. 0.8402 \n",
      "Epoch 00380 | Loss 0.2444 | Train Acc. 0.9084 | Validation Acc. 0.8454 \n",
      "Epoch 00385 | Loss 0.2433 | Train Acc. 0.8968 | Validation Acc. 0.8608 \n",
      "Epoch 00390 | Loss 0.2429 | Train Acc. 0.9058 | Validation Acc. 0.8557 \n",
      "Epoch 00395 | Loss 0.2520 | Train Acc. 0.8890 | Validation Acc. 0.8299 \n",
      "Epoch 00400 | Loss 0.2433 | Train Acc. 0.8929 | Validation Acc. 0.8454 \n",
      "Epoch 00405 | Loss 0.2543 | Train Acc. 0.9019 | Validation Acc. 0.8351 \n",
      "Epoch 00410 | Loss 0.2490 | Train Acc. 0.8955 | Validation Acc. 0.8608 \n",
      "Epoch 00415 | Loss 0.2514 | Train Acc. 0.8890 | Validation Acc. 0.8557 \n",
      "Epoch 00420 | Loss 0.2472 | Train Acc. 0.8955 | Validation Acc. 0.8557 \n",
      "Epoch 00425 | Loss 0.2424 | Train Acc. 0.9032 | Validation Acc. 0.8505 \n",
      "Epoch 00430 | Loss 0.2457 | Train Acc. 0.8955 | Validation Acc. 0.8557 \n",
      "Epoch 00435 | Loss 0.2342 | Train Acc. 0.8955 | Validation Acc. 0.8557 \n",
      "Epoch 00440 | Loss 0.2545 | Train Acc. 0.8955 | Validation Acc. 0.8557 \n",
      "Epoch 00445 | Loss 0.2429 | Train Acc. 0.9045 | Validation Acc. 0.8454 \n",
      "Epoch 00450 | Loss 0.2468 | Train Acc. 0.8981 | Validation Acc. 0.8505 \n",
      "Epoch 00455 | Loss 0.2482 | Train Acc. 0.8916 | Validation Acc. 0.8660 \n",
      "Epoch 00460 | Loss 0.2446 | Train Acc. 0.9071 | Validation Acc. 0.8557 \n",
      "Epoch 00465 | Loss 0.2513 | Train Acc. 0.9045 | Validation Acc. 0.8608 \n",
      "Epoch 00470 | Loss 0.2551 | Train Acc. 0.8955 | Validation Acc. 0.8557 \n",
      "Epoch 00475 | Loss 0.2413 | Train Acc. 0.8981 | Validation Acc. 0.8505 \n",
      "Epoch 00480 | Loss 0.2441 | Train Acc. 0.8955 | Validation Acc. 0.8660 \n",
      "Epoch 00485 | Loss 0.2446 | Train Acc. 0.8916 | Validation Acc. 0.8505 \n",
      "Epoch 00490 | Loss 0.2496 | Train Acc. 0.8903 | Validation Acc. 0.8608 \n",
      "Epoch 00495 | Loss 0.2463 | Train Acc. 0.8981 | Validation Acc. 0.8505 \n",
      "Epoch 00500 | Loss 0.2460 | Train Acc. 0.8942 | Validation Acc. 0.8505 \n",
      "Epoch 00505 | Loss 0.2452 | Train Acc. 0.9032 | Validation Acc. 0.8608 \n",
      "Epoch 00510 | Loss 0.2455 | Train Acc. 0.8916 | Validation Acc. 0.8557 \n",
      "Epoch 00515 | Loss 0.2375 | Train Acc. 0.8981 | Validation Acc. 0.8608 \n",
      "Epoch 00520 | Loss 0.2387 | Train Acc. 0.8955 | Validation Acc. 0.8557 \n",
      "Epoch 00525 | Loss 0.2457 | Train Acc. 0.8916 | Validation Acc. 0.8505 \n",
      "Epoch 00530 | Loss 0.2471 | Train Acc. 0.9071 | Validation Acc. 0.8608 \n",
      "Epoch 00535 | Loss 0.2450 | Train Acc. 0.8890 | Validation Acc. 0.8505 \n",
      "Epoch 00540 | Loss 0.2400 | Train Acc. 0.9045 | Validation Acc. 0.8608 \n",
      "Epoch 00545 | Loss 0.2484 | Train Acc. 0.8942 | Validation Acc. 0.8402 \n",
      "Epoch 00550 | Loss 0.2371 | Train Acc. 0.9032 | Validation Acc. 0.8711 \n",
      "Epoch 00555 | Loss 0.2456 | Train Acc. 0.8968 | Validation Acc. 0.8505 \n",
      "Epoch 00560 | Loss 0.2524 | Train Acc. 0.8994 | Validation Acc. 0.8711 \n",
      "Epoch 00565 | Loss 0.2494 | Train Acc. 0.8942 | Validation Acc. 0.8608 \n",
      "Epoch 00570 | Loss 0.2453 | Train Acc. 0.8994 | Validation Acc. 0.8402 \n",
      "Epoch 00575 | Loss 0.2436 | Train Acc. 0.9071 | Validation Acc. 0.8660 \n",
      "Epoch 00580 | Loss 0.2394 | Train Acc. 0.9032 | Validation Acc. 0.8608 \n",
      "Epoch 00585 | Loss 0.2487 | Train Acc. 0.8994 | Validation Acc. 0.8608 \n",
      "Epoch 00590 | Loss 0.2388 | Train Acc. 0.9045 | Validation Acc. 0.8711 \n",
      "Epoch 00595 | Loss 0.2492 | Train Acc. 0.8994 | Validation Acc. 0.8608 \n",
      "Epoch 00600 | Loss 0.2327 | Train Acc. 0.9097 | Validation Acc. 0.8711 \n",
      "Epoch 00605 | Loss 0.2414 | Train Acc. 0.9071 | Validation Acc. 0.8660 \n",
      "Epoch 00610 | Loss 0.2446 | Train Acc. 0.8981 | Validation Acc. 0.8608 \n",
      "Epoch 00615 | Loss 0.2478 | Train Acc. 0.8981 | Validation Acc. 0.8660 \n",
      "Epoch 00620 | Loss 0.2478 | Train Acc. 0.8981 | Validation Acc. 0.8608 \n",
      "Epoch 00625 | Loss 0.2543 | Train Acc. 0.8968 | Validation Acc. 0.8763 \n",
      "Epoch 00630 | Loss 0.2554 | Train Acc. 0.9032 | Validation Acc. 0.8711 \n",
      "Epoch 00635 | Loss 0.2460 | Train Acc. 0.8916 | Validation Acc. 0.8402 \n",
      "Epoch 00640 | Loss 0.2477 | Train Acc. 0.8981 | Validation Acc. 0.8505 \n",
      "Epoch 00645 | Loss 0.2419 | Train Acc. 0.8968 | Validation Acc. 0.8557 \n",
      "Epoch 00650 | Loss 0.2419 | Train Acc. 0.8916 | Validation Acc. 0.8711 \n",
      "Epoch 00655 | Loss 0.2488 | Train Acc. 0.8865 | Validation Acc. 0.8608 \n",
      "Epoch 00660 | Loss 0.2462 | Train Acc. 0.8929 | Validation Acc. 0.8402 \n",
      "Epoch 00665 | Loss 0.2426 | Train Acc. 0.9006 | Validation Acc. 0.8557 \n",
      "Epoch 00670 | Loss 0.2373 | Train Acc. 0.9006 | Validation Acc. 0.8608 \n",
      "Epoch 00675 | Loss 0.2439 | Train Acc. 0.9058 | Validation Acc. 0.8711 \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "Fold : 7 | Test Accuracy = 0.8785 | F1 = 0.8526 \n",
      "Total = 6.4Gb \t Reserved = 1.1Gb \t Allocated = 0.4Gb\n",
      "Clearing gpu memory\n",
      "Total = 6.4Gb \t Reserved = 0.3Gb \t Allocated = 0.3Gb\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1076, num_edges=18312,\n",
      "      ndata_schemes={'idx': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(5,), dtype=torch.int64), 'feat': Scheme(shape=(30459,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.6366 | Train Acc. 0.0761 | Validation Acc. 0.4691 \n",
      "Epoch 00005 | Loss 0.7903 | Train Acc. 0.8077 | Validation Acc. 0.6392 \n",
      "Epoch 00010 | Loss 0.6238 | Train Acc. 0.8090 | Validation Acc. 0.7010 \n",
      "Epoch 00015 | Loss 0.5163 | Train Acc. 0.8219 | Validation Acc. 0.7680 \n",
      "Epoch 00020 | Loss 0.4714 | Train Acc. 0.8219 | Validation Acc. 0.7938 \n",
      "Epoch 00025 | Loss 0.4192 | Train Acc. 0.8297 | Validation Acc. 0.8093 \n",
      "Epoch 00030 | Loss 0.3969 | Train Acc. 0.8361 | Validation Acc. 0.8247 \n",
      "Epoch 00035 | Loss 0.3807 | Train Acc. 0.8490 | Validation Acc. 0.8041 \n",
      "Epoch 00040 | Loss 0.3479 | Train Acc. 0.8555 | Validation Acc. 0.7835 \n",
      "Epoch 00045 | Loss 0.3636 | Train Acc. 0.8465 | Validation Acc. 0.7835 \n",
      "Epoch 00050 | Loss 0.3436 | Train Acc. 0.8594 | Validation Acc. 0.7784 \n",
      "Epoch 00055 | Loss 0.3480 | Train Acc. 0.8568 | Validation Acc. 0.7732 \n",
      "Epoch 00060 | Loss 0.3379 | Train Acc. 0.8632 | Validation Acc. 0.7938 \n",
      "Epoch 00065 | Loss 0.3298 | Train Acc. 0.8710 | Validation Acc. 0.7835 \n",
      "Epoch 00070 | Loss 0.3337 | Train Acc. 0.8658 | Validation Acc. 0.7990 \n",
      "Epoch 00075 | Loss 0.3335 | Train Acc. 0.8581 | Validation Acc. 0.7938 \n",
      "Epoch 00080 | Loss 0.3124 | Train Acc. 0.8774 | Validation Acc. 0.8196 \n",
      "Epoch 00085 | Loss 0.3109 | Train Acc. 0.8761 | Validation Acc. 0.8144 \n",
      "Epoch 00090 | Loss 0.3148 | Train Acc. 0.8800 | Validation Acc. 0.7784 \n",
      "Epoch 00095 | Loss 0.3122 | Train Acc. 0.8735 | Validation Acc. 0.7526 \n",
      "Epoch 00100 | Loss 0.3017 | Train Acc. 0.8710 | Validation Acc. 0.7629 \n",
      "Epoch 00105 | Loss 0.3009 | Train Acc. 0.8774 | Validation Acc. 0.7835 \n",
      "Epoch 00110 | Loss 0.3070 | Train Acc. 0.8748 | Validation Acc. 0.8351 \n",
      "Epoch 00115 | Loss 0.3002 | Train Acc. 0.8723 | Validation Acc. 0.8196 \n",
      "Epoch 00120 | Loss 0.2909 | Train Acc. 0.8826 | Validation Acc. 0.8505 \n",
      "Epoch 00125 | Loss 0.3015 | Train Acc. 0.8723 | Validation Acc. 0.8299 \n",
      "Epoch 00130 | Loss 0.2966 | Train Acc. 0.8813 | Validation Acc. 0.8454 \n",
      "Epoch 00135 | Loss 0.2893 | Train Acc. 0.8813 | Validation Acc. 0.8711 \n",
      "Epoch 00140 | Loss 0.2827 | Train Acc. 0.8774 | Validation Acc. 0.8557 \n",
      "Epoch 00145 | Loss 0.2861 | Train Acc. 0.8839 | Validation Acc. 0.8763 \n",
      "Epoch 00150 | Loss 0.2927 | Train Acc. 0.8723 | Validation Acc. 0.8866 \n",
      "Epoch 00155 | Loss 0.2902 | Train Acc. 0.8748 | Validation Acc. 0.8969 \n",
      "Epoch 00160 | Loss 0.2890 | Train Acc. 0.8774 | Validation Acc. 0.8918 \n",
      "Epoch 00165 | Loss 0.2737 | Train Acc. 0.8890 | Validation Acc. 0.8918 \n",
      "Epoch 00170 | Loss 0.2851 | Train Acc. 0.8826 | Validation Acc. 0.9021 \n",
      "Epoch 00175 | Loss 0.2789 | Train Acc. 0.8852 | Validation Acc. 0.8918 \n",
      "Epoch 00180 | Loss 0.2795 | Train Acc. 0.8890 | Validation Acc. 0.8918 \n",
      "Epoch 00185 | Loss 0.2754 | Train Acc. 0.8761 | Validation Acc. 0.8866 \n",
      "Epoch 00190 | Loss 0.2757 | Train Acc. 0.8826 | Validation Acc. 0.8763 \n",
      "Epoch 00195 | Loss 0.2694 | Train Acc. 0.8839 | Validation Acc. 0.8505 \n",
      "Epoch 00200 | Loss 0.2688 | Train Acc. 0.9032 | Validation Acc. 0.8093 \n",
      "Epoch 00205 | Loss 0.2605 | Train Acc. 0.8994 | Validation Acc. 0.8505 \n",
      "Epoch 00210 | Loss 0.2702 | Train Acc. 0.8826 | Validation Acc. 0.8608 \n",
      "Epoch 00215 | Loss 0.2689 | Train Acc. 0.8865 | Validation Acc. 0.8711 \n",
      "Epoch 00220 | Loss 0.2680 | Train Acc. 0.8968 | Validation Acc. 0.8505 \n",
      "Epoch 00225 | Loss 0.2592 | Train Acc. 0.8942 | Validation Acc. 0.8608 \n",
      "Epoch 00230 | Loss 0.2625 | Train Acc. 0.8955 | Validation Acc. 0.8763 \n",
      "Epoch 00235 | Loss 0.2618 | Train Acc. 0.8968 | Validation Acc. 0.8918 \n",
      "Epoch 00240 | Loss 0.2708 | Train Acc. 0.8903 | Validation Acc. 0.8918 \n",
      "Epoch 00245 | Loss 0.2650 | Train Acc. 0.8877 | Validation Acc. 0.8763 \n",
      "Epoch 00250 | Loss 0.2581 | Train Acc. 0.9019 | Validation Acc. 0.8866 \n",
      "Epoch 00255 | Loss 0.2664 | Train Acc. 0.8929 | Validation Acc. 0.8918 \n",
      "Epoch 00260 | Loss 0.2563 | Train Acc. 0.8955 | Validation Acc. 0.8866 \n",
      "Epoch 00265 | Loss 0.2667 | Train Acc. 0.8968 | Validation Acc. 0.8969 \n",
      "Epoch 00270 | Loss 0.2478 | Train Acc. 0.8916 | Validation Acc. 0.8918 \n",
      "Epoch 00275 | Loss 0.2598 | Train Acc. 0.8942 | Validation Acc. 0.8918 \n",
      "Epoch 00280 | Loss 0.2565 | Train Acc. 0.8942 | Validation Acc. 0.8866 \n",
      "Epoch 00285 | Loss 0.2575 | Train Acc. 0.8826 | Validation Acc. 0.8763 \n",
      "Epoch 00290 | Loss 0.2677 | Train Acc. 0.8877 | Validation Acc. 0.8711 \n",
      "Epoch 00295 | Loss 0.2532 | Train Acc. 0.8929 | Validation Acc. 0.8866 \n",
      "Epoch 00300 | Loss 0.2535 | Train Acc. 0.8903 | Validation Acc. 0.8763 \n",
      "Epoch 00305 | Loss 0.2526 | Train Acc. 0.8994 | Validation Acc. 0.8866 \n",
      "Epoch 00310 | Loss 0.2564 | Train Acc. 0.8981 | Validation Acc. 0.8969 \n",
      "Epoch 00315 | Loss 0.2544 | Train Acc. 0.8968 | Validation Acc. 0.8918 \n",
      "Epoch 00320 | Loss 0.2485 | Train Acc. 0.9019 | Validation Acc. 0.8969 \n",
      "Epoch 00325 | Loss 0.2620 | Train Acc. 0.8916 | Validation Acc. 0.8814 \n",
      "Epoch 00330 | Loss 0.2591 | Train Acc. 0.8787 | Validation Acc. 0.8814 \n",
      "Epoch 00335 | Loss 0.2568 | Train Acc. 0.8955 | Validation Acc. 0.8814 \n",
      "Epoch 00340 | Loss 0.2506 | Train Acc. 0.8968 | Validation Acc. 0.8866 \n",
      "Epoch 00345 | Loss 0.2561 | Train Acc. 0.8903 | Validation Acc. 0.8969 \n",
      "Epoch 00350 | Loss 0.2593 | Train Acc. 0.8968 | Validation Acc. 0.8866 \n",
      "Epoch 00355 | Loss 0.2545 | Train Acc. 0.8929 | Validation Acc. 0.8866 \n",
      "Epoch 00360 | Loss 0.2490 | Train Acc. 0.8994 | Validation Acc. 0.8969 \n",
      "Epoch 00365 | Loss 0.2527 | Train Acc. 0.8942 | Validation Acc. 0.8814 \n",
      "Epoch 00370 | Loss 0.2501 | Train Acc. 0.9058 | Validation Acc. 0.8866 \n",
      "Epoch 00375 | Loss 0.2469 | Train Acc. 0.9045 | Validation Acc. 0.8763 \n",
      "Epoch 00380 | Loss 0.2669 | Train Acc. 0.8890 | Validation Acc. 0.8969 \n",
      "Epoch 00385 | Loss 0.2503 | Train Acc. 0.8968 | Validation Acc. 0.8918 \n",
      "Epoch 00390 | Loss 0.2550 | Train Acc. 0.8903 | Validation Acc. 0.8918 \n",
      "Epoch 00395 | Loss 0.2586 | Train Acc. 0.8968 | Validation Acc. 0.8763 \n",
      "Epoch 00400 | Loss 0.2585 | Train Acc. 0.8955 | Validation Acc. 0.8969 \n",
      "Epoch 00405 | Loss 0.2498 | Train Acc. 0.9058 | Validation Acc. 0.8918 \n",
      "Epoch 00410 | Loss 0.2532 | Train Acc. 0.8968 | Validation Acc. 0.8814 \n",
      "Epoch 00415 | Loss 0.2446 | Train Acc. 0.9006 | Validation Acc. 0.8866 \n",
      "Epoch 00420 | Loss 0.2562 | Train Acc. 0.8968 | Validation Acc. 0.9021 \n",
      "Epoch 00425 | Loss 0.2536 | Train Acc. 0.8903 | Validation Acc. 0.8969 \n",
      "Epoch 00430 | Loss 0.2551 | Train Acc. 0.8852 | Validation Acc. 0.8866 \n",
      "Epoch 00435 | Loss 0.2478 | Train Acc. 0.8916 | Validation Acc. 0.8711 \n",
      "Epoch 00440 | Loss 0.2560 | Train Acc. 0.9032 | Validation Acc. 0.8969 \n",
      "Epoch 00445 | Loss 0.2492 | Train Acc. 0.9006 | Validation Acc. 0.8969 \n",
      "Epoch 00450 | Loss 0.2514 | Train Acc. 0.9019 | Validation Acc. 0.8969 \n",
      "Epoch 00455 | Loss 0.2630 | Train Acc. 0.8955 | Validation Acc. 0.8866 \n",
      "Epoch 00460 | Loss 0.2546 | Train Acc. 0.8968 | Validation Acc. 0.8969 \n",
      "Epoch 00465 | Loss 0.2405 | Train Acc. 0.9019 | Validation Acc. 0.8866 \n",
      "Epoch 00470 | Loss 0.2550 | Train Acc. 0.8929 | Validation Acc. 0.8814 \n",
      "Epoch 00475 | Loss 0.2526 | Train Acc. 0.8916 | Validation Acc. 0.8918 \n",
      "Epoch 00480 | Loss 0.2493 | Train Acc. 0.8955 | Validation Acc. 0.8866 \n",
      "Epoch 00485 | Loss 0.2509 | Train Acc. 0.9006 | Validation Acc. 0.8866 \n",
      "Epoch 00490 | Loss 0.2443 | Train Acc. 0.8981 | Validation Acc. 0.8918 \n",
      "Epoch 00495 | Loss 0.2538 | Train Acc. 0.8968 | Validation Acc. 0.8918 \n",
      "Epoch 00500 | Loss 0.2552 | Train Acc. 0.8955 | Validation Acc. 0.8969 \n",
      "Epoch 00505 | Loss 0.2538 | Train Acc. 0.8955 | Validation Acc. 0.8814 \n",
      "Epoch 00510 | Loss 0.2483 | Train Acc. 0.8994 | Validation Acc. 0.8763 \n",
      "Epoch 00515 | Loss 0.2471 | Train Acc. 0.8903 | Validation Acc. 0.8969 \n",
      "Epoch 00520 | Loss 0.2552 | Train Acc. 0.9006 | Validation Acc. 0.8918 \n",
      "Epoch 00525 | Loss 0.2470 | Train Acc. 0.9110 | Validation Acc. 0.8918 \n",
      "Epoch 00530 | Loss 0.2455 | Train Acc. 0.9019 | Validation Acc. 0.8763 \n",
      "Epoch 00535 | Loss 0.2455 | Train Acc. 0.8929 | Validation Acc. 0.8969 \n",
      "Epoch 00540 | Loss 0.2517 | Train Acc. 0.9032 | Validation Acc. 0.8763 \n",
      "Epoch 00545 | Loss 0.2502 | Train Acc. 0.8916 | Validation Acc. 0.8866 \n",
      "Epoch 00550 | Loss 0.2495 | Train Acc. 0.8994 | Validation Acc. 0.8866 \n",
      "Epoch 00555 | Loss 0.2518 | Train Acc. 0.9032 | Validation Acc. 0.8866 \n",
      "Epoch 00560 | Loss 0.2643 | Train Acc. 0.8968 | Validation Acc. 0.9021 \n",
      "Epoch 00565 | Loss 0.2499 | Train Acc. 0.8994 | Validation Acc. 0.8918 \n",
      "Epoch 00570 | Loss 0.2467 | Train Acc. 0.9032 | Validation Acc. 0.9021 \n",
      "Epoch 00575 | Loss 0.2495 | Train Acc. 0.8929 | Validation Acc. 0.8969 \n",
      "Epoch 00580 | Loss 0.2572 | Train Acc. 0.8994 | Validation Acc. 0.8763 \n",
      "Epoch 00585 | Loss 0.2550 | Train Acc. 0.8852 | Validation Acc. 0.8814 \n",
      "Epoch 00590 | Loss 0.2514 | Train Acc. 0.8890 | Validation Acc. 0.8866 \n",
      "Epoch 00595 | Loss 0.2543 | Train Acc. 0.9071 | Validation Acc. 0.8814 \n",
      "Epoch 00600 | Loss 0.2513 | Train Acc. 0.8994 | Validation Acc. 0.8918 \n",
      "Epoch 00605 | Loss 0.2533 | Train Acc. 0.8968 | Validation Acc. 0.8918 \n",
      "Epoch 00610 | Loss 0.2493 | Train Acc. 0.8994 | Validation Acc. 0.9021 \n",
      "Epoch 00615 | Loss 0.2470 | Train Acc. 0.9006 | Validation Acc. 0.8711 \n",
      "Epoch 00620 | Loss 0.2525 | Train Acc. 0.8955 | Validation Acc. 0.8814 \n",
      "Epoch 00625 | Loss 0.2525 | Train Acc. 0.9071 | Validation Acc. 0.8763 \n",
      "Epoch 00630 | Loss 0.2435 | Train Acc. 0.8981 | Validation Acc. 0.8814 \n",
      "Epoch 00635 | Loss 0.2461 | Train Acc. 0.8903 | Validation Acc. 0.8814 \n",
      "Epoch 00640 | Loss 0.2531 | Train Acc. 0.8942 | Validation Acc. 0.8814 \n",
      "Epoch 00645 | Loss 0.2535 | Train Acc. 0.8994 | Validation Acc. 0.8814 \n",
      "Epoch 00650 | Loss 0.2661 | Train Acc. 0.8916 | Validation Acc. 0.8711 \n",
      "Epoch 00655 | Loss 0.2497 | Train Acc. 0.9019 | Validation Acc. 0.9021 \n",
      "Epoch 00660 | Loss 0.2510 | Train Acc. 0.8968 | Validation Acc. 0.8814 \n",
      "Epoch 00665 | Loss 0.2504 | Train Acc. 0.8903 | Validation Acc. 0.8866 \n",
      "Epoch 00670 | Loss 0.2505 | Train Acc. 0.9006 | Validation Acc. 0.8814 \n",
      "Epoch 00675 | Loss 0.2640 | Train Acc. 0.8877 | Validation Acc. 0.8866 \n",
      "Epoch 00680 | Loss 0.2496 | Train Acc. 0.9032 | Validation Acc. 0.8814 \n",
      "Epoch 00685 | Loss 0.2525 | Train Acc. 0.8916 | Validation Acc. 0.8866 \n",
      "Epoch 00690 | Loss 0.2482 | Train Acc. 0.9019 | Validation Acc. 0.9021 \n",
      "Epoch 00695 | Loss 0.2528 | Train Acc. 0.9006 | Validation Acc. 0.8918 \n",
      "Epoch 00700 | Loss 0.2555 | Train Acc. 0.8955 | Validation Acc. 0.8866 \n",
      "Epoch 00705 | Loss 0.2486 | Train Acc. 0.9058 | Validation Acc. 0.8969 \n",
      "Epoch 00710 | Loss 0.2520 | Train Acc. 0.8890 | Validation Acc. 0.8969 \n",
      "Epoch 00715 | Loss 0.2569 | Train Acc. 0.8955 | Validation Acc. 0.8866 \n",
      "Epoch 00720 | Loss 0.2529 | Train Acc. 0.9006 | Validation Acc. 0.9021 \n",
      "Epoch 00725 | Loss 0.2537 | Train Acc. 0.8890 | Validation Acc. 0.8866 \n",
      "Epoch 00730 | Loss 0.2482 | Train Acc. 0.8994 | Validation Acc. 0.8711 \n",
      "Epoch 00735 | Loss 0.2442 | Train Acc. 0.9019 | Validation Acc. 0.8763 \n",
      "Epoch 00740 | Loss 0.2606 | Train Acc. 0.8942 | Validation Acc. 0.8866 \n",
      "Epoch 00745 | Loss 0.2538 | Train Acc. 0.8903 | Validation Acc. 0.8866 \n",
      "Epoch 00750 | Loss 0.2617 | Train Acc. 0.8981 | Validation Acc. 0.8814 \n",
      "Epoch 00755 | Loss 0.2548 | Train Acc. 0.9019 | Validation Acc. 0.8918 \n",
      "Epoch 00760 | Loss 0.2538 | Train Acc. 0.8890 | Validation Acc. 0.8866 \n",
      "Epoch 00765 | Loss 0.2535 | Train Acc. 0.8955 | Validation Acc. 0.8814 \n",
      "Epoch 00770 | Loss 0.2607 | Train Acc. 0.8942 | Validation Acc. 0.8866 \n",
      "Epoch 00775 | Loss 0.2431 | Train Acc. 0.9084 | Validation Acc. 0.8969 \n",
      "Epoch 00780 | Loss 0.2514 | Train Acc. 0.9006 | Validation Acc. 0.8866 \n",
      "Epoch 00785 | Loss 0.2489 | Train Acc. 0.8929 | Validation Acc. 0.8918 \n",
      "Epoch 00790 | Loss 0.2502 | Train Acc. 0.8903 | Validation Acc. 0.9072 \n",
      "Epoch 00795 | Loss 0.2554 | Train Acc. 0.8916 | Validation Acc. 0.8866 \n",
      "Epoch 00800 | Loss 0.2499 | Train Acc. 0.9058 | Validation Acc. 0.8814 \n",
      "Epoch 00805 | Loss 0.2459 | Train Acc. 0.8929 | Validation Acc. 0.8763 \n",
      "Epoch 00810 | Loss 0.2434 | Train Acc. 0.8994 | Validation Acc. 0.8918 \n",
      "Epoch 00815 | Loss 0.2542 | Train Acc. 0.8968 | Validation Acc. 0.8969 \n",
      "Epoch 00820 | Loss 0.2526 | Train Acc. 0.8968 | Validation Acc. 0.8814 \n",
      "Epoch 00825 | Loss 0.2473 | Train Acc. 0.8994 | Validation Acc. 0.8814 \n",
      "Epoch 00830 | Loss 0.2529 | Train Acc. 0.8942 | Validation Acc. 0.8660 \n",
      "Epoch 00835 | Loss 0.2381 | Train Acc. 0.9045 | Validation Acc. 0.8918 \n",
      "Epoch 00840 | Loss 0.2536 | Train Acc. 0.8929 | Validation Acc. 0.8969 \n",
      "Epoch 00845 | Loss 0.2448 | Train Acc. 0.8994 | Validation Acc. 0.8866 \n",
      "Epoch 00850 | Loss 0.2556 | Train Acc. 0.9019 | Validation Acc. 0.8814 \n",
      "Epoch 00855 | Loss 0.2553 | Train Acc. 0.8955 | Validation Acc. 0.8814 \n",
      "Epoch 00860 | Loss 0.2510 | Train Acc. 0.8890 | Validation Acc. 0.9021 \n",
      "Epoch 00865 | Loss 0.2508 | Train Acc. 0.8865 | Validation Acc. 0.8866 \n",
      "Epoch 00870 | Loss 0.2579 | Train Acc. 0.8865 | Validation Acc. 0.8918 \n",
      "Epoch 00875 | Loss 0.2554 | Train Acc. 0.8955 | Validation Acc. 0.8969 \n",
      "Epoch 00880 | Loss 0.2435 | Train Acc. 0.9123 | Validation Acc. 0.8814 \n",
      "Epoch 00885 | Loss 0.2500 | Train Acc. 0.8942 | Validation Acc. 0.8866 \n",
      "Epoch 00890 | Loss 0.2528 | Train Acc. 0.8981 | Validation Acc. 0.8918 \n",
      "Epoch 00895 | Loss 0.2570 | Train Acc. 0.8929 | Validation Acc. 0.9021 \n",
      "Epoch 00900 | Loss 0.2482 | Train Acc. 0.8955 | Validation Acc. 0.8969 \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "Fold : 8 | Test Accuracy = 0.8692 | F1 = 0.8402 \n",
      "Total = 6.4Gb \t Reserved = 1.1Gb \t Allocated = 0.4Gb\n",
      "Clearing gpu memory\n",
      "Total = 6.4Gb \t Reserved = 0.3Gb \t Allocated = 0.3Gb\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1076, num_edges=18312,\n",
      "      ndata_schemes={'idx': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(5,), dtype=torch.int64), 'feat': Scheme(shape=(30459,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.6613 | Train Acc. 0.1368 | Validation Acc. 0.1907 \n",
      "Epoch 00005 | Loss 0.7890 | Train Acc. 0.7742 | Validation Acc. 0.2680 \n",
      "Epoch 00010 | Loss 0.6208 | Train Acc. 0.8077 | Validation Acc. 0.4381 \n",
      "Epoch 00015 | Loss 0.5283 | Train Acc. 0.8284 | Validation Acc. 0.7320 \n",
      "Epoch 00020 | Loss 0.4794 | Train Acc. 0.8465 | Validation Acc. 0.7784 \n",
      "Epoch 00025 | Loss 0.4252 | Train Acc. 0.8542 | Validation Acc. 0.8351 \n",
      "Epoch 00030 | Loss 0.4038 | Train Acc. 0.8529 | Validation Acc. 0.8351 \n",
      "Epoch 00035 | Loss 0.3731 | Train Acc. 0.8568 | Validation Acc. 0.8351 \n",
      "Epoch 00040 | Loss 0.3641 | Train Acc. 0.8568 | Validation Acc. 0.7835 \n",
      "Epoch 00045 | Loss 0.3456 | Train Acc. 0.8723 | Validation Acc. 0.7526 \n",
      "Epoch 00050 | Loss 0.3418 | Train Acc. 0.8658 | Validation Acc. 0.7268 \n",
      "Epoch 00055 | Loss 0.3390 | Train Acc. 0.8645 | Validation Acc. 0.7629 \n",
      "Epoch 00060 | Loss 0.3328 | Train Acc. 0.8645 | Validation Acc. 0.7577 \n",
      "Epoch 00065 | Loss 0.3278 | Train Acc. 0.8761 | Validation Acc. 0.7577 \n",
      "Epoch 00070 | Loss 0.3303 | Train Acc. 0.8697 | Validation Acc. 0.7474 \n",
      "Epoch 00075 | Loss 0.3269 | Train Acc. 0.8723 | Validation Acc. 0.7423 \n",
      "Epoch 00080 | Loss 0.3181 | Train Acc. 0.8774 | Validation Acc. 0.7320 \n",
      "Epoch 00085 | Loss 0.3101 | Train Acc. 0.8813 | Validation Acc. 0.7268 \n",
      "Epoch 00090 | Loss 0.3083 | Train Acc. 0.8800 | Validation Acc. 0.7113 \n",
      "Epoch 00095 | Loss 0.3117 | Train Acc. 0.8748 | Validation Acc. 0.7216 \n",
      "Epoch 00100 | Loss 0.3109 | Train Acc. 0.8710 | Validation Acc. 0.7577 \n",
      "Epoch 00105 | Loss 0.3034 | Train Acc. 0.8748 | Validation Acc. 0.7629 \n",
      "Epoch 00110 | Loss 0.3068 | Train Acc. 0.8710 | Validation Acc. 0.7629 \n",
      "Epoch 00115 | Loss 0.3041 | Train Acc. 0.8787 | Validation Acc. 0.7680 \n",
      "Epoch 00120 | Loss 0.2943 | Train Acc. 0.8852 | Validation Acc. 0.7784 \n",
      "Epoch 00125 | Loss 0.2900 | Train Acc. 0.8852 | Validation Acc. 0.7938 \n",
      "Epoch 00130 | Loss 0.3027 | Train Acc. 0.8723 | Validation Acc. 0.8041 \n",
      "Epoch 00135 | Loss 0.2898 | Train Acc. 0.8839 | Validation Acc. 0.8041 \n",
      "Epoch 00140 | Loss 0.2864 | Train Acc. 0.8826 | Validation Acc. 0.8093 \n",
      "Epoch 00145 | Loss 0.2885 | Train Acc. 0.8903 | Validation Acc. 0.8041 \n",
      "Epoch 00150 | Loss 0.2845 | Train Acc. 0.8877 | Validation Acc. 0.8299 \n",
      "Epoch 00155 | Loss 0.2813 | Train Acc. 0.8929 | Validation Acc. 0.8351 \n",
      "Epoch 00160 | Loss 0.2725 | Train Acc. 0.8826 | Validation Acc. 0.8557 \n",
      "Epoch 00165 | Loss 0.2720 | Train Acc. 0.8890 | Validation Acc. 0.8711 \n",
      "Epoch 00170 | Loss 0.2715 | Train Acc. 0.8813 | Validation Acc. 0.8763 \n",
      "Epoch 00175 | Loss 0.2685 | Train Acc. 0.8890 | Validation Acc. 0.8866 \n",
      "Epoch 00180 | Loss 0.2786 | Train Acc. 0.8852 | Validation Acc. 0.8711 \n",
      "Epoch 00185 | Loss 0.2756 | Train Acc. 0.8890 | Validation Acc. 0.8814 \n",
      "Epoch 00190 | Loss 0.2669 | Train Acc. 0.8916 | Validation Acc. 0.8814 \n",
      "Epoch 00195 | Loss 0.2664 | Train Acc. 0.8968 | Validation Acc. 0.8814 \n",
      "Epoch 00200 | Loss 0.2703 | Train Acc. 0.8916 | Validation Acc. 0.8814 \n",
      "Epoch 00205 | Loss 0.2661 | Train Acc. 0.8916 | Validation Acc. 0.8866 \n",
      "Epoch 00210 | Loss 0.2620 | Train Acc. 0.8916 | Validation Acc. 0.8866 \n",
      "Epoch 00215 | Loss 0.2712 | Train Acc. 0.8877 | Validation Acc. 0.8866 \n",
      "Epoch 00220 | Loss 0.2673 | Train Acc. 0.8981 | Validation Acc. 0.8763 \n",
      "Epoch 00225 | Loss 0.2638 | Train Acc. 0.8942 | Validation Acc. 0.8711 \n",
      "Epoch 00230 | Loss 0.2580 | Train Acc. 0.8929 | Validation Acc. 0.8763 \n",
      "Epoch 00235 | Loss 0.2522 | Train Acc. 0.8981 | Validation Acc. 0.8866 \n",
      "Epoch 00240 | Loss 0.2640 | Train Acc. 0.8942 | Validation Acc. 0.8814 \n",
      "Epoch 00245 | Loss 0.2531 | Train Acc. 0.9006 | Validation Acc. 0.8814 \n",
      "Epoch 00250 | Loss 0.2508 | Train Acc. 0.8981 | Validation Acc. 0.8814 \n",
      "Epoch 00255 | Loss 0.2656 | Train Acc. 0.8968 | Validation Acc. 0.8814 \n",
      "Epoch 00260 | Loss 0.2585 | Train Acc. 0.9006 | Validation Acc. 0.8814 \n",
      "Epoch 00265 | Loss 0.2605 | Train Acc. 0.8968 | Validation Acc. 0.8763 \n",
      "Epoch 00270 | Loss 0.2602 | Train Acc. 0.8890 | Validation Acc. 0.8763 \n",
      "Epoch 00275 | Loss 0.2484 | Train Acc. 0.8981 | Validation Acc. 0.8814 \n",
      "Epoch 00280 | Loss 0.2617 | Train Acc. 0.8942 | Validation Acc. 0.8763 \n",
      "Epoch 00285 | Loss 0.2510 | Train Acc. 0.9123 | Validation Acc. 0.8866 \n",
      "Epoch 00290 | Loss 0.2501 | Train Acc. 0.8916 | Validation Acc. 0.8969 \n",
      "Epoch 00295 | Loss 0.2467 | Train Acc. 0.9045 | Validation Acc. 0.8711 \n",
      "Epoch 00300 | Loss 0.2489 | Train Acc. 0.8981 | Validation Acc. 0.8763 \n",
      "Epoch 00305 | Loss 0.2544 | Train Acc. 0.8968 | Validation Acc. 0.8711 \n",
      "Epoch 00310 | Loss 0.2571 | Train Acc. 0.8955 | Validation Acc. 0.8814 \n",
      "Epoch 00315 | Loss 0.2445 | Train Acc. 0.8903 | Validation Acc. 0.8814 \n",
      "Epoch 00320 | Loss 0.2502 | Train Acc. 0.8968 | Validation Acc. 0.8763 \n",
      "Epoch 00325 | Loss 0.2459 | Train Acc. 0.9019 | Validation Acc. 0.8763 \n",
      "Epoch 00330 | Loss 0.2494 | Train Acc. 0.8942 | Validation Acc. 0.8763 \n",
      "Epoch 00335 | Loss 0.2436 | Train Acc. 0.8981 | Validation Acc. 0.8814 \n",
      "Epoch 00340 | Loss 0.2448 | Train Acc. 0.9019 | Validation Acc. 0.8814 \n",
      "Epoch 00345 | Loss 0.2491 | Train Acc. 0.9071 | Validation Acc. 0.8763 \n",
      "Epoch 00350 | Loss 0.2402 | Train Acc. 0.8955 | Validation Acc. 0.8918 \n",
      "Epoch 00355 | Loss 0.2488 | Train Acc. 0.8968 | Validation Acc. 0.8814 \n",
      "Epoch 00360 | Loss 0.2512 | Train Acc. 0.9019 | Validation Acc. 0.8866 \n",
      "Epoch 00365 | Loss 0.2479 | Train Acc. 0.8981 | Validation Acc. 0.8814 \n",
      "Epoch 00370 | Loss 0.2419 | Train Acc. 0.9045 | Validation Acc. 0.8763 \n",
      "Epoch 00375 | Loss 0.2422 | Train Acc. 0.9097 | Validation Acc. 0.8763 \n",
      "Epoch 00380 | Loss 0.2581 | Train Acc. 0.8877 | Validation Acc. 0.8711 \n",
      "Epoch 00385 | Loss 0.2475 | Train Acc. 0.9019 | Validation Acc. 0.8814 \n",
      "Epoch 00390 | Loss 0.2397 | Train Acc. 0.8968 | Validation Acc. 0.8866 \n",
      "Epoch 00395 | Loss 0.2382 | Train Acc. 0.8955 | Validation Acc. 0.8711 \n",
      "Epoch 00400 | Loss 0.2508 | Train Acc. 0.9006 | Validation Acc. 0.8711 \n",
      "Epoch 00405 | Loss 0.2491 | Train Acc. 0.8968 | Validation Acc. 0.8711 \n",
      "Epoch 00410 | Loss 0.2350 | Train Acc. 0.9032 | Validation Acc. 0.8505 \n",
      "Epoch 00415 | Loss 0.2450 | Train Acc. 0.9032 | Validation Acc. 0.8505 \n",
      "Epoch 00420 | Loss 0.2510 | Train Acc. 0.8929 | Validation Acc. 0.8711 \n",
      "Epoch 00425 | Loss 0.2488 | Train Acc. 0.9006 | Validation Acc. 0.8763 \n",
      "Epoch 00430 | Loss 0.2485 | Train Acc. 0.9006 | Validation Acc. 0.8711 \n",
      "Epoch 00435 | Loss 0.2496 | Train Acc. 0.9006 | Validation Acc. 0.8763 \n",
      "Epoch 00440 | Loss 0.2448 | Train Acc. 0.9058 | Validation Acc. 0.8763 \n",
      "Epoch 00445 | Loss 0.2430 | Train Acc. 0.8994 | Validation Acc. 0.8711 \n",
      "Epoch 00450 | Loss 0.2443 | Train Acc. 0.9058 | Validation Acc. 0.8660 \n",
      "Epoch 00455 | Loss 0.2414 | Train Acc. 0.8994 | Validation Acc. 0.8660 \n",
      "Epoch 00460 | Loss 0.2451 | Train Acc. 0.8994 | Validation Acc. 0.8763 \n",
      "Epoch 00465 | Loss 0.2527 | Train Acc. 0.8929 | Validation Acc. 0.8711 \n",
      "Epoch 00470 | Loss 0.2370 | Train Acc. 0.8994 | Validation Acc. 0.8763 \n",
      "Epoch 00475 | Loss 0.2411 | Train Acc. 0.8942 | Validation Acc. 0.8814 \n",
      "Epoch 00480 | Loss 0.2466 | Train Acc. 0.9032 | Validation Acc. 0.8660 \n",
      "Epoch 00485 | Loss 0.2503 | Train Acc. 0.9006 | Validation Acc. 0.8711 \n",
      "Epoch 00490 | Loss 0.2516 | Train Acc. 0.8968 | Validation Acc. 0.8711 \n",
      "Epoch 00495 | Loss 0.2418 | Train Acc. 0.9071 | Validation Acc. 0.8608 \n",
      "Epoch 00500 | Loss 0.2465 | Train Acc. 0.8968 | Validation Acc. 0.8763 \n",
      "Epoch 00505 | Loss 0.2394 | Train Acc. 0.8968 | Validation Acc. 0.8763 \n",
      "Epoch 00510 | Loss 0.2365 | Train Acc. 0.9045 | Validation Acc. 0.8608 \n",
      "Epoch 00515 | Loss 0.2531 | Train Acc. 0.9019 | Validation Acc. 0.8711 \n",
      "Epoch 00520 | Loss 0.2404 | Train Acc. 0.9045 | Validation Acc. 0.8660 \n",
      "Epoch 00525 | Loss 0.2513 | Train Acc. 0.8916 | Validation Acc. 0.8608 \n",
      "Epoch 00530 | Loss 0.2416 | Train Acc. 0.9058 | Validation Acc. 0.8660 \n",
      "Epoch 00535 | Loss 0.2400 | Train Acc. 0.9071 | Validation Acc. 0.8814 \n",
      "Epoch 00540 | Loss 0.2494 | Train Acc. 0.8994 | Validation Acc. 0.8608 \n",
      "Epoch 00545 | Loss 0.2448 | Train Acc. 0.8994 | Validation Acc. 0.8660 \n",
      "Epoch 00550 | Loss 0.2481 | Train Acc. 0.8981 | Validation Acc. 0.8660 \n",
      "Epoch 00555 | Loss 0.2489 | Train Acc. 0.9097 | Validation Acc. 0.8660 \n",
      "Epoch 00560 | Loss 0.2448 | Train Acc. 0.9032 | Validation Acc. 0.8763 \n",
      "Epoch 00565 | Loss 0.2490 | Train Acc. 0.8942 | Validation Acc. 0.8608 \n",
      "Epoch 00570 | Loss 0.2425 | Train Acc. 0.9032 | Validation Acc. 0.8608 \n",
      "Epoch 00575 | Loss 0.2432 | Train Acc. 0.8994 | Validation Acc. 0.8814 \n",
      "Epoch 00580 | Loss 0.2461 | Train Acc. 0.8994 | Validation Acc. 0.8660 \n",
      "Epoch 00585 | Loss 0.2447 | Train Acc. 0.9006 | Validation Acc. 0.8763 \n",
      "Epoch 00590 | Loss 0.2478 | Train Acc. 0.8929 | Validation Acc. 0.8557 \n",
      "Epoch 00595 | Loss 0.2475 | Train Acc. 0.9019 | Validation Acc. 0.8763 \n",
      "Epoch 00600 | Loss 0.2336 | Train Acc. 0.9097 | Validation Acc. 0.8814 \n",
      "Epoch 00605 | Loss 0.2324 | Train Acc. 0.9071 | Validation Acc. 0.8763 \n",
      "Epoch 00610 | Loss 0.2478 | Train Acc. 0.9006 | Validation Acc. 0.8711 \n",
      "Epoch 00615 | Loss 0.2527 | Train Acc. 0.8968 | Validation Acc. 0.8763 \n",
      "Epoch 00620 | Loss 0.2515 | Train Acc. 0.8929 | Validation Acc. 0.8608 \n",
      "Epoch 00625 | Loss 0.2465 | Train Acc. 0.8981 | Validation Acc. 0.8866 \n",
      "Epoch 00630 | Loss 0.2452 | Train Acc. 0.9006 | Validation Acc. 0.8711 \n",
      "Epoch 00635 | Loss 0.2498 | Train Acc. 0.8981 | Validation Acc. 0.8866 \n",
      "Epoch 00640 | Loss 0.2405 | Train Acc. 0.9071 | Validation Acc. 0.8711 \n",
      "Epoch 00645 | Loss 0.2437 | Train Acc. 0.9058 | Validation Acc. 0.8557 \n",
      "Epoch 00650 | Loss 0.2406 | Train Acc. 0.9006 | Validation Acc. 0.8763 \n",
      "Epoch 00655 | Loss 0.2440 | Train Acc. 0.9097 | Validation Acc. 0.8660 \n",
      "Epoch 00660 | Loss 0.2486 | Train Acc. 0.8942 | Validation Acc. 0.8660 \n",
      "Epoch 00665 | Loss 0.2520 | Train Acc. 0.9006 | Validation Acc. 0.8557 \n",
      "Epoch 00670 | Loss 0.2493 | Train Acc. 0.8942 | Validation Acc. 0.8711 \n",
      "Epoch 00675 | Loss 0.2476 | Train Acc. 0.9006 | Validation Acc. 0.8660 \n",
      "Epoch 00680 | Loss 0.2433 | Train Acc. 0.9032 | Validation Acc. 0.8711 \n",
      "Epoch 00685 | Loss 0.2437 | Train Acc. 0.9032 | Validation Acc. 0.8660 \n",
      "Epoch 00690 | Loss 0.2473 | Train Acc. 0.9019 | Validation Acc. 0.8660 \n",
      "Epoch 00695 | Loss 0.2359 | Train Acc. 0.9045 | Validation Acc. 0.8660 \n",
      "Epoch 00700 | Loss 0.2391 | Train Acc. 0.9032 | Validation Acc. 0.8763 \n",
      "Epoch 00705 | Loss 0.2413 | Train Acc. 0.8981 | Validation Acc. 0.8608 \n",
      "Epoch 00710 | Loss 0.2484 | Train Acc. 0.8968 | Validation Acc. 0.8608 \n",
      "Epoch 00715 | Loss 0.2518 | Train Acc. 0.8942 | Validation Acc. 0.8608 \n",
      "Epoch 00720 | Loss 0.2502 | Train Acc. 0.9006 | Validation Acc. 0.8763 \n",
      "Epoch 00725 | Loss 0.2473 | Train Acc. 0.9058 | Validation Acc. 0.8711 \n",
      "Epoch 00730 | Loss 0.2483 | Train Acc. 0.9045 | Validation Acc. 0.8608 \n",
      "Epoch 00735 | Loss 0.2395 | Train Acc. 0.9032 | Validation Acc. 0.8557 \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "Fold : 9 | Test Accuracy = 0.8598 | F1 = 0.8405 \n",
      "Total = 6.4Gb \t Reserved = 1.1Gb \t Allocated = 0.4Gb\n",
      "Clearing gpu memory\n",
      "Total = 6.4Gb \t Reserved = 0.3Gb \t Allocated = 0.3Gb\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1076, num_edges=18312,\n",
      "      ndata_schemes={'idx': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(5,), dtype=torch.int64), 'feat': Scheme(shape=(30459,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.5328 | Train Acc. 0.4000 | Validation Acc. 0.5722 \n",
      "Epoch 00005 | Loss 0.6879 | Train Acc. 0.8232 | Validation Acc. 0.4536 \n",
      "Epoch 00010 | Loss 0.5730 | Train Acc. 0.8245 | Validation Acc. 0.5567 \n",
      "Epoch 00015 | Loss 0.4944 | Train Acc. 0.8503 | Validation Acc. 0.6649 \n",
      "Epoch 00020 | Loss 0.4358 | Train Acc. 0.8335 | Validation Acc. 0.7577 \n",
      "Epoch 00025 | Loss 0.4154 | Train Acc. 0.8361 | Validation Acc. 0.8351 \n",
      "Epoch 00030 | Loss 0.3960 | Train Acc. 0.8542 | Validation Acc. 0.8247 \n",
      "Epoch 00035 | Loss 0.3657 | Train Acc. 0.8542 | Validation Acc. 0.8041 \n",
      "Epoch 00040 | Loss 0.3604 | Train Acc. 0.8465 | Validation Acc. 0.7680 \n",
      "Epoch 00045 | Loss 0.3529 | Train Acc. 0.8581 | Validation Acc. 0.7371 \n",
      "Epoch 00050 | Loss 0.3329 | Train Acc. 0.8800 | Validation Acc. 0.7216 \n",
      "Epoch 00055 | Loss 0.3381 | Train Acc. 0.8710 | Validation Acc. 0.7165 \n",
      "Epoch 00060 | Loss 0.3249 | Train Acc. 0.8748 | Validation Acc. 0.7216 \n",
      "Epoch 00065 | Loss 0.3259 | Train Acc. 0.8735 | Validation Acc. 0.7062 \n",
      "Epoch 00070 | Loss 0.3153 | Train Acc. 0.8710 | Validation Acc. 0.7474 \n",
      "Epoch 00075 | Loss 0.3134 | Train Acc. 0.8774 | Validation Acc. 0.7216 \n",
      "Epoch 00080 | Loss 0.3100 | Train Acc. 0.8761 | Validation Acc. 0.7268 \n",
      "Epoch 00085 | Loss 0.2996 | Train Acc. 0.8787 | Validation Acc. 0.7371 \n",
      "Epoch 00090 | Loss 0.2980 | Train Acc. 0.8813 | Validation Acc. 0.7268 \n",
      "Epoch 00095 | Loss 0.3029 | Train Acc. 0.8761 | Validation Acc. 0.7062 \n",
      "Epoch 00100 | Loss 0.2980 | Train Acc. 0.8748 | Validation Acc. 0.7371 \n",
      "Epoch 00105 | Loss 0.2988 | Train Acc. 0.8813 | Validation Acc. 0.7371 \n",
      "Epoch 00110 | Loss 0.2880 | Train Acc. 0.8877 | Validation Acc. 0.8247 \n",
      "Epoch 00115 | Loss 0.2897 | Train Acc. 0.8865 | Validation Acc. 0.8454 \n",
      "Epoch 00120 | Loss 0.2905 | Train Acc. 0.8787 | Validation Acc. 0.8299 \n",
      "Epoch 00125 | Loss 0.2835 | Train Acc. 0.8890 | Validation Acc. 0.8505 \n",
      "Epoch 00130 | Loss 0.2872 | Train Acc. 0.8852 | Validation Acc. 0.8660 \n",
      "Epoch 00135 | Loss 0.2749 | Train Acc. 0.8890 | Validation Acc. 0.8505 \n",
      "Epoch 00140 | Loss 0.2808 | Train Acc. 0.8877 | Validation Acc. 0.8454 \n",
      "Epoch 00145 | Loss 0.2725 | Train Acc. 0.8929 | Validation Acc. 0.8299 \n",
      "Epoch 00150 | Loss 0.2744 | Train Acc. 0.8877 | Validation Acc. 0.7938 \n",
      "Epoch 00155 | Loss 0.2720 | Train Acc. 0.8955 | Validation Acc. 0.8247 \n",
      "Epoch 00160 | Loss 0.2841 | Train Acc. 0.8852 | Validation Acc. 0.8505 \n",
      "Epoch 00165 | Loss 0.2704 | Train Acc. 0.8877 | Validation Acc. 0.8402 \n",
      "Epoch 00170 | Loss 0.2708 | Train Acc. 0.8929 | Validation Acc. 0.8402 \n",
      "Epoch 00175 | Loss 0.2622 | Train Acc. 0.9006 | Validation Acc. 0.8041 \n",
      "Epoch 00180 | Loss 0.2505 | Train Acc. 0.9045 | Validation Acc. 0.8093 \n",
      "Epoch 00185 | Loss 0.2624 | Train Acc. 0.8877 | Validation Acc. 0.8299 \n",
      "Epoch 00190 | Loss 0.2661 | Train Acc. 0.8955 | Validation Acc. 0.8247 \n",
      "Epoch 00195 | Loss 0.2626 | Train Acc. 0.8916 | Validation Acc. 0.7938 \n",
      "Epoch 00200 | Loss 0.2491 | Train Acc. 0.9045 | Validation Acc. 0.7835 \n",
      "Epoch 00205 | Loss 0.2572 | Train Acc. 0.8994 | Validation Acc. 0.8144 \n",
      "Epoch 00210 | Loss 0.2571 | Train Acc. 0.8955 | Validation Acc. 0.8351 \n",
      "Epoch 00215 | Loss 0.2484 | Train Acc. 0.8994 | Validation Acc. 0.8144 \n",
      "Epoch 00220 | Loss 0.2466 | Train Acc. 0.9071 | Validation Acc. 0.7990 \n",
      "Epoch 00225 | Loss 0.2489 | Train Acc. 0.9019 | Validation Acc. 0.8093 \n",
      "Epoch 00230 | Loss 0.2464 | Train Acc. 0.9032 | Validation Acc. 0.7680 \n",
      "Epoch 00235 | Loss 0.2521 | Train Acc. 0.8994 | Validation Acc. 0.7423 \n",
      "Epoch 00240 | Loss 0.2472 | Train Acc. 0.9058 | Validation Acc. 0.7423 \n",
      "Epoch 00245 | Loss 0.2403 | Train Acc. 0.9058 | Validation Acc. 0.7268 \n",
      "Epoch 00250 | Loss 0.2320 | Train Acc. 0.9123 | Validation Acc. 0.7165 \n",
      "Epoch 00255 | Loss 0.2326 | Train Acc. 0.9084 | Validation Acc. 0.7887 \n",
      "Epoch 00260 | Loss 0.2364 | Train Acc. 0.9084 | Validation Acc. 0.8144 \n",
      "Epoch 00265 | Loss 0.2442 | Train Acc. 0.9045 | Validation Acc. 0.8196 \n",
      "Epoch 00270 | Loss 0.2406 | Train Acc. 0.9032 | Validation Acc. 0.8351 \n",
      "Epoch 00275 | Loss 0.2461 | Train Acc. 0.9084 | Validation Acc. 0.8247 \n",
      "Epoch 00280 | Loss 0.2466 | Train Acc. 0.9006 | Validation Acc. 0.8402 \n",
      "Epoch 00285 | Loss 0.2366 | Train Acc. 0.9058 | Validation Acc. 0.8351 \n",
      "Epoch 00290 | Loss 0.2311 | Train Acc. 0.9058 | Validation Acc. 0.8402 \n",
      "Epoch 00295 | Loss 0.2368 | Train Acc. 0.9071 | Validation Acc. 0.8196 \n",
      "Epoch 00300 | Loss 0.2347 | Train Acc. 0.9084 | Validation Acc. 0.8402 \n",
      "Epoch 00305 | Loss 0.2430 | Train Acc. 0.8981 | Validation Acc. 0.8660 \n",
      "Epoch 00310 | Loss 0.2316 | Train Acc. 0.9084 | Validation Acc. 0.8505 \n",
      "Epoch 00315 | Loss 0.2435 | Train Acc. 0.9110 | Validation Acc. 0.8402 \n",
      "Epoch 00320 | Loss 0.2325 | Train Acc. 0.9148 | Validation Acc. 0.8402 \n",
      "Epoch 00325 | Loss 0.2318 | Train Acc. 0.9058 | Validation Acc. 0.8196 \n",
      "Epoch 00330 | Loss 0.2370 | Train Acc. 0.9071 | Validation Acc. 0.8299 \n",
      "Epoch 00335 | Loss 0.2395 | Train Acc. 0.9045 | Validation Acc. 0.8299 \n",
      "Epoch 00340 | Loss 0.2408 | Train Acc. 0.8942 | Validation Acc. 0.8299 \n",
      "Epoch 00345 | Loss 0.2288 | Train Acc. 0.9148 | Validation Acc. 0.8557 \n",
      "Epoch 00350 | Loss 0.2294 | Train Acc. 0.9123 | Validation Acc. 0.8351 \n",
      "Epoch 00355 | Loss 0.2375 | Train Acc. 0.8942 | Validation Acc. 0.8402 \n",
      "Epoch 00360 | Loss 0.2341 | Train Acc. 0.9071 | Validation Acc. 0.8866 \n",
      "Epoch 00365 | Loss 0.2388 | Train Acc. 0.9071 | Validation Acc. 0.8660 \n",
      "Epoch 00370 | Loss 0.2317 | Train Acc. 0.9097 | Validation Acc. 0.8557 \n",
      "Epoch 00375 | Loss 0.2348 | Train Acc. 0.9123 | Validation Acc. 0.8660 \n",
      "Epoch 00380 | Loss 0.2302 | Train Acc. 0.9071 | Validation Acc. 0.8557 \n",
      "Epoch 00385 | Loss 0.2372 | Train Acc. 0.9058 | Validation Acc. 0.8351 \n",
      "Epoch 00390 | Loss 0.2294 | Train Acc. 0.9174 | Validation Acc. 0.8454 \n",
      "Epoch 00395 | Loss 0.2320 | Train Acc. 0.9148 | Validation Acc. 0.8608 \n",
      "Epoch 00400 | Loss 0.2256 | Train Acc. 0.9174 | Validation Acc. 0.8660 \n",
      "Epoch 00405 | Loss 0.2354 | Train Acc. 0.9045 | Validation Acc. 0.8660 \n",
      "Epoch 00410 | Loss 0.2316 | Train Acc. 0.9084 | Validation Acc. 0.8711 \n",
      "Epoch 00415 | Loss 0.2247 | Train Acc. 0.9097 | Validation Acc. 0.8557 \n",
      "Epoch 00420 | Loss 0.2323 | Train Acc. 0.9045 | Validation Acc. 0.8608 \n",
      "Epoch 00425 | Loss 0.2288 | Train Acc. 0.9071 | Validation Acc. 0.8660 \n",
      "Epoch 00430 | Loss 0.2253 | Train Acc. 0.9071 | Validation Acc. 0.8711 \n",
      "Epoch 00435 | Loss 0.2282 | Train Acc. 0.9161 | Validation Acc. 0.8608 \n",
      "Epoch 00440 | Loss 0.2404 | Train Acc. 0.8968 | Validation Acc. 0.8763 \n",
      "Epoch 00445 | Loss 0.2333 | Train Acc. 0.9097 | Validation Acc. 0.8660 \n",
      "Epoch 00450 | Loss 0.2318 | Train Acc. 0.9135 | Validation Acc. 0.8711 \n",
      "Epoch 00455 | Loss 0.2262 | Train Acc. 0.9148 | Validation Acc. 0.8608 \n",
      "Epoch 00460 | Loss 0.2343 | Train Acc. 0.9045 | Validation Acc. 0.8711 \n",
      "Epoch 00465 | Loss 0.2297 | Train Acc. 0.9174 | Validation Acc. 0.8454 \n",
      "Epoch 00470 | Loss 0.2326 | Train Acc. 0.9161 | Validation Acc. 0.8660 \n",
      "Epoch 00475 | Loss 0.2258 | Train Acc. 0.9032 | Validation Acc. 0.8557 \n",
      "Epoch 00480 | Loss 0.2359 | Train Acc. 0.8994 | Validation Acc. 0.8711 \n",
      "Epoch 00485 | Loss 0.2306 | Train Acc. 0.9123 | Validation Acc. 0.8608 \n",
      "Epoch 00490 | Loss 0.2275 | Train Acc. 0.9161 | Validation Acc. 0.8557 \n",
      "Epoch 00495 | Loss 0.2372 | Train Acc. 0.8981 | Validation Acc. 0.8505 \n",
      "Epoch 00500 | Loss 0.2357 | Train Acc. 0.9097 | Validation Acc. 0.8608 \n",
      "Epoch 00505 | Loss 0.2324 | Train Acc. 0.9123 | Validation Acc. 0.8557 \n",
      "Epoch 00510 | Loss 0.2319 | Train Acc. 0.9097 | Validation Acc. 0.8557 \n",
      "Epoch 00515 | Loss 0.2287 | Train Acc. 0.9097 | Validation Acc. 0.8608 \n",
      "Epoch 00520 | Loss 0.2300 | Train Acc. 0.9019 | Validation Acc. 0.8557 \n",
      "Epoch 00525 | Loss 0.2355 | Train Acc. 0.9006 | Validation Acc. 0.8557 \n",
      "Epoch 00530 | Loss 0.2327 | Train Acc. 0.9032 | Validation Acc. 0.8505 \n",
      "Epoch 00535 | Loss 0.2236 | Train Acc. 0.9148 | Validation Acc. 0.8711 \n",
      "Epoch 00540 | Loss 0.2319 | Train Acc. 0.9058 | Validation Acc. 0.8608 \n",
      "Epoch 00545 | Loss 0.2314 | Train Acc. 0.9045 | Validation Acc. 0.8711 \n",
      "Epoch 00550 | Loss 0.2316 | Train Acc. 0.9110 | Validation Acc. 0.8557 \n",
      "Epoch 00555 | Loss 0.2468 | Train Acc. 0.8994 | Validation Acc. 0.8763 \n",
      "Epoch 00560 | Loss 0.2219 | Train Acc. 0.9226 | Validation Acc. 0.8557 \n",
      "Epoch 00565 | Loss 0.2322 | Train Acc. 0.9084 | Validation Acc. 0.8608 \n",
      "Epoch 00570 | Loss 0.2295 | Train Acc. 0.8994 | Validation Acc. 0.8660 \n",
      "Epoch 00575 | Loss 0.2397 | Train Acc. 0.9058 | Validation Acc. 0.8660 \n",
      "Epoch 00580 | Loss 0.2267 | Train Acc. 0.9045 | Validation Acc. 0.8711 \n",
      "Epoch 00585 | Loss 0.2345 | Train Acc. 0.9097 | Validation Acc. 0.8660 \n",
      "Epoch 00590 | Loss 0.2286 | Train Acc. 0.9123 | Validation Acc. 0.8660 \n",
      "Epoch 00595 | Loss 0.2296 | Train Acc. 0.9097 | Validation Acc. 0.8505 \n",
      "Epoch 00600 | Loss 0.2388 | Train Acc. 0.9084 | Validation Acc. 0.8454 \n",
      "Epoch 00605 | Loss 0.2346 | Train Acc. 0.9045 | Validation Acc. 0.8557 \n",
      "Epoch 00610 | Loss 0.2235 | Train Acc. 0.9123 | Validation Acc. 0.8608 \n",
      "Epoch 00615 | Loss 0.2344 | Train Acc. 0.9135 | Validation Acc. 0.8608 \n",
      "Epoch 00620 | Loss 0.2186 | Train Acc. 0.9187 | Validation Acc. 0.8660 \n",
      "Epoch 00625 | Loss 0.2313 | Train Acc. 0.9019 | Validation Acc. 0.8557 \n",
      "Epoch 00630 | Loss 0.2292 | Train Acc. 0.9097 | Validation Acc. 0.8711 \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "Fold : 10 | Test Accuracy = 0.8879 | F1 = 0.8742 \n",
      "Total = 6.4Gb \t Reserved = 1.1Gb \t Allocated = 0.3Gb\n",
      "Clearing gpu memory\n",
      "Total = 6.4Gb \t Reserved = 0.3Gb \t Allocated = 0.3Gb\n",
      "5 Fold Cross Validation Accuracy = 86.25 ± 1.91\n",
      "5 Fold Cross Validation F1 = 83.77 ± 2.30\n"
     ]
    }
   ],
   "source": [
    "!python MOGDx.py -i \"./../data/\" -o \"./Output/TCGA\" -mod mRNA RPPA -ld 32 16 --target \"paper_BRCA_Subtype_PAM50\" --index-col \"index\" --h-feats 32 --decoder-dim 64 --epochs 2000 --lr 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ae18a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Library Import \n",
      "\n",
      "usage: MOGDx.py [-h] [--epochs N] [--lr LR] [--patience PATIENCE] [--no-cuda]\n",
      "                [--no-output-plots] [--split-val] [--no-shuffle] [--psn-only]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                [--no-psn] [--val-split-size VAL_SPLIT_SIZE]\n",
      "                [--index-col INDEX_COL] [--n-splits N_SPLITS]\n",
      "                [--decoder-dim DECODER_DIM] --h-feats H_FEATS [H_FEATS ...] -i\n",
      "                INPUT -o OUTPUT -mod MODALITIES [MODALITIES ...] -ld\n",
      "                LATENT_DIM [LATENT_DIM ...] --target TARGET\n",
      "\n",
      "MOGDx\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --epochs N            number of epochs to train (default: 10)\n",
      "  --lr LR               learning rate (default: 1.0)\n",
      "  --patience PATIENCE   Early Stopping Patience (default: 100 batches of 5 ->\n",
      "                        equivalent of 100*5 = 500)\n",
      "  --no-cuda             disables CUDA training\n",
      "  --no-output-plots     Disables Confusion Matrix and TSNE plots\n",
      "  --split-val           Disable validation split on AE and GNN\n",
      "  --no-shuffle          Disable shuffling of index for K fold split\n",
      "  --psn-only            Dont train on any node features\n",
      "  --no-psn              Dont train on PSN (removal of edges)\n",
      "  --val-split-size VAL_SPLIT_SIZE\n",
      "                        Validation split of training set ineach k fold split.\n",
      "                        Default of 0.85 is 60/10/30 train/val/test with a 10\n",
      "                        fold split\n",
      "  --index-col INDEX_COL\n",
      "                        Name of column in input data which refers to\n",
      "                        index.Leave blank if none.\n",
      "  --n-splits N_SPLITS   Number of K-Foldsplits to use\n",
      "  --decoder-dim DECODER_DIM\n",
      "                        Integer specifying dim of common layer to all\n",
      "                        modalities\n",
      "  --h-feats H_FEATS [H_FEATS ...]\n",
      "                        Integer specifying hidden dim of GNNspecifying GNN\n",
      "                        layer size\n",
      "  -i INPUT, --input INPUT\n",
      "                        Path to the input data for the model to read\n",
      "  -o OUTPUT, --output OUTPUT\n",
      "                        Path to the directory to write output to\n",
      "  -mod MODALITIES [MODALITIES ...], --modalities MODALITIES [MODALITIES ...]\n",
      "                        List of themodalities to include in the integration\n",
      "  -ld LATENT_DIM [LATENT_DIM ...], --latent-dim LATENT_DIM [LATENT_DIM ...]\n",
      "                        List of integers corresponding to the length of hidden\n",
      "                        dims of each data modality\n",
      "  --target TARGET       Column name referring to thedisease classification\n",
      "                        label\n"
     ]
    }
   ],
   "source": [
    "! python MOGDx.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f859ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
