{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3032ad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Library Import \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys  \n",
    "sys.path.insert(0, './MAIN/')\n",
    "from utils import *\n",
    "from GNN_MME import *\n",
    "from train import *\n",
    "import preprocess_functions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import networkx as nx\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Finished Library Import \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4967dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = './../data/raw'\n",
    "snf_net = 'mRNA_RPPA_miRNA_DNAm_graph.graphml'\n",
    "index_col = 'index'\n",
    "target = 'paper_BRCA_Subtype_PAM50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee28437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Total = 6.4Gb \t Reserved = 1.6Gb \t Allocated = 1.4Gb\n",
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=1601, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=200000, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gcnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph with 1083 nodes and 10426 edges\n",
      "Epoch 00000 | Loss 1.5901 | Train Acc. 0.2673 | Validation Acc. 0.5172 \n",
      "Epoch 00005 | Loss 1.3223 | Train Acc. 0.5520 | Validation Acc. 0.5172 \n",
      "Epoch 00010 | Loss 1.1419 | Train Acc. 0.5665 | Validation Acc. 0.6954 \n",
      "Epoch 00015 | Loss 1.0380 | Train Acc. 0.5723 | Validation Acc. 0.6954 \n",
      "Epoch 00020 | Loss 0.9339 | Train Acc. 0.6069 | Validation Acc. 0.6897 \n",
      "Epoch 00025 | Loss 0.8735 | Train Acc. 0.6228 | Validation Acc. 0.5172 \n",
      "Epoch 00030 | Loss 0.8700 | Train Acc. 0.5983 | Validation Acc. 0.5172 \n",
      "Epoch 00035 | Loss 0.8444 | Train Acc. 0.6084 | Validation Acc. 0.5172 \n",
      "Epoch 00040 | Loss 0.7717 | Train Acc. 0.6387 | Validation Acc. 0.5172 \n",
      "Epoch 00045 | Loss 0.8075 | Train Acc. 0.6012 | Validation Acc. 0.5172 \n",
      "Epoch 00050 | Loss 0.7774 | Train Acc. 0.6142 | Validation Acc. 0.5402 \n",
      "Epoch 00055 | Loss 0.7564 | Train Acc. 0.6561 | Validation Acc. 0.6839 \n",
      "Epoch 00060 | Loss 0.7578 | Train Acc. 0.6387 | Validation Acc. 0.6552 \n",
      "Epoch 00065 | Loss 0.7493 | Train Acc. 0.6416 | Validation Acc. 0.6667 \n",
      "Epoch 00070 | Loss 0.7328 | Train Acc. 0.6228 | Validation Acc. 0.7471 \n",
      "Epoch 00075 | Loss 0.6765 | Train Acc. 0.6777 | Validation Acc. 0.8218 \n",
      "Epoch 00080 | Loss 0.7450 | Train Acc. 0.6402 | Validation Acc. 0.7184 \n",
      "Epoch 00085 | Loss 0.6934 | Train Acc. 0.6517 | Validation Acc. 0.8563 \n",
      "Epoch 00090 | Loss 0.7118 | Train Acc. 0.6532 | Validation Acc. 0.7414 \n",
      "Epoch 00095 | Loss 0.7012 | Train Acc. 0.6416 | Validation Acc. 0.8103 \n",
      "Epoch 00100 | Loss 0.7347 | Train Acc. 0.6301 | Validation Acc. 0.7644 \n",
      "Epoch 00105 | Loss 0.7165 | Train Acc. 0.6301 | Validation Acc. 0.8621 \n",
      "Epoch 00110 | Loss 0.6995 | Train Acc. 0.6676 | Validation Acc. 0.8563 \n",
      "Epoch 00115 | Loss 0.6880 | Train Acc. 0.6575 | Validation Acc. 0.8563 \n",
      "Epoch 00120 | Loss 0.6617 | Train Acc. 0.6532 | Validation Acc. 0.8448 \n",
      "Epoch 00125 | Loss 0.6914 | Train Acc. 0.6474 | Validation Acc. 0.6494 \n",
      "Epoch 00130 | Loss 0.6858 | Train Acc. 0.6286 | Validation Acc. 0.3908 \n",
      "Epoch 00135 | Loss 0.6921 | Train Acc. 0.6431 | Validation Acc. 0.8161 \n",
      "Epoch 00140 | Loss 0.6246 | Train Acc. 0.6777 | Validation Acc. 0.8506 \n",
      "Epoch 00145 | Loss 0.6840 | Train Acc. 0.6387 | Validation Acc. 0.8621 \n",
      "Epoch 00150 | Loss 0.6835 | Train Acc. 0.6532 | Validation Acc. 0.8621 \n",
      "Epoch 00155 | Loss 0.6873 | Train Acc. 0.6460 | Validation Acc. 0.8506 \n",
      "Epoch 00160 | Loss 0.6531 | Train Acc. 0.6561 | Validation Acc. 0.8218 \n",
      "Epoch 00165 | Loss 0.6698 | Train Acc. 0.6517 | Validation Acc. 0.6264 \n",
      "Epoch 00170 | Loss 0.7109 | Train Acc. 0.6445 | Validation Acc. 0.7759 \n",
      "Epoch 00175 | Loss 0.6530 | Train Acc. 0.6590 | Validation Acc. 0.8448 \n",
      "Epoch 00180 | Loss 0.6823 | Train Acc. 0.6445 | Validation Acc. 0.8391 \n",
      "Epoch 00185 | Loss 0.6326 | Train Acc. 0.6879 | Validation Acc. 0.8448 \n",
      "Epoch 00190 | Loss 0.6881 | Train Acc. 0.6214 | Validation Acc. 0.8506 \n",
      "Epoch 00195 | Loss 0.6631 | Train Acc. 0.6532 | Validation Acc. 0.8391 \n",
      "Epoch 00200 | Loss 0.6760 | Train Acc. 0.6402 | Validation Acc. 0.8621 \n",
      "Epoch 00205 | Loss 0.6564 | Train Acc. 0.6445 | Validation Acc. 0.8391 \n",
      "Epoch 00210 | Loss 0.6387 | Train Acc. 0.6604 | Validation Acc. 0.8563 \n",
      "Epoch 00215 | Loss 0.6358 | Train Acc. 0.6618 | Validation Acc. 0.8506 \n",
      "Epoch 00220 | Loss 0.7078 | Train Acc. 0.6474 | Validation Acc. 0.8506 \n",
      "Epoch 00225 | Loss 0.6286 | Train Acc. 0.6749 | Validation Acc. 0.8621 \n",
      "Epoch 00230 | Loss 0.6179 | Train Acc. 0.6980 | Validation Acc. 0.8448 \n",
      "Epoch 00235 | Loss 0.6462 | Train Acc. 0.6387 | Validation Acc. 0.8448 \n",
      "Epoch 00240 | Loss 0.6415 | Train Acc. 0.6575 | Validation Acc. 0.7989 \n",
      "Epoch 00245 | Loss 0.6507 | Train Acc. 0.6503 | Validation Acc. 0.8333 \n",
      "Epoch 00250 | Loss 0.6314 | Train Acc. 0.6777 | Validation Acc. 0.8391 \n",
      "Epoch 00255 | Loss 0.6612 | Train Acc. 0.6561 | Validation Acc. 0.8563 \n",
      "Epoch 00260 | Loss 0.6224 | Train Acc. 0.6691 | Validation Acc. 0.8621 \n",
      "Epoch 00265 | Loss 0.6769 | Train Acc. 0.6243 | Validation Acc. 0.8448 \n",
      "Epoch 00270 | Loss 0.6182 | Train Acc. 0.6821 | Validation Acc. 0.8391 \n",
      "Epoch 00275 | Loss 0.6325 | Train Acc. 0.6590 | Validation Acc. 0.8448 \n",
      "Epoch 00280 | Loss 0.6365 | Train Acc. 0.6647 | Validation Acc. 0.8391 \n",
      "Epoch 00285 | Loss 0.6455 | Train Acc. 0.6691 | Validation Acc. 0.8506 \n",
      "Epoch 00290 | Loss 0.6003 | Train Acc. 0.7038 | Validation Acc. 0.8391 \n",
      "Epoch 00295 | Loss 0.6814 | Train Acc. 0.6431 | Validation Acc. 0.8678 \n",
      "Epoch 00300 | Loss 0.6567 | Train Acc. 0.6474 | Validation Acc. 0.8621 \n",
      "Epoch 00305 | Loss 0.6685 | Train Acc. 0.6460 | Validation Acc. 0.8448 \n",
      "Epoch 00310 | Loss 0.6260 | Train Acc. 0.6777 | Validation Acc. 0.8448 \n",
      "Epoch 00315 | Loss 0.6765 | Train Acc. 0.6474 | Validation Acc. 0.8563 \n",
      "Epoch 00320 | Loss 0.6556 | Train Acc. 0.6445 | Validation Acc. 0.8506 \n",
      "Epoch 00325 | Loss 0.6440 | Train Acc. 0.6474 | Validation Acc. 0.8391 \n",
      "Epoch 00330 | Loss 0.6336 | Train Acc. 0.6618 | Validation Acc. 0.8448 \n",
      "Epoch 00335 | Loss 0.6400 | Train Acc. 0.6633 | Validation Acc. 0.8621 \n",
      "Epoch 00340 | Loss 0.6533 | Train Acc. 0.6474 | Validation Acc. 0.8793 \n",
      "Epoch 00345 | Loss 0.6975 | Train Acc. 0.6214 | Validation Acc. 0.8563 \n",
      "Epoch 00350 | Loss 0.6466 | Train Acc. 0.6792 | Validation Acc. 0.8563 \n",
      "Epoch 00355 | Loss 0.6019 | Train Acc. 0.6691 | Validation Acc. 0.8621 \n",
      "Epoch 00360 | Loss 0.6638 | Train Acc. 0.6387 | Validation Acc. 0.8506 \n",
      "Epoch 00365 | Loss 0.6359 | Train Acc. 0.6734 | Validation Acc. 0.8391 \n",
      "Epoch 00370 | Loss 0.6618 | Train Acc. 0.6445 | Validation Acc. 0.8563 \n",
      "Epoch 00375 | Loss 0.6298 | Train Acc. 0.6908 | Validation Acc. 0.8448 \n",
      "Epoch 00380 | Loss 0.6642 | Train Acc. 0.6387 | Validation Acc. 0.8448 \n",
      "Epoch 00385 | Loss 0.6488 | Train Acc. 0.6532 | Validation Acc. 0.8506 \n",
      "Epoch 00390 | Loss 0.6390 | Train Acc. 0.6561 | Validation Acc. 0.8506 \n",
      "Epoch 00395 | Loss 0.6445 | Train Acc. 0.6647 | Validation Acc. 0.8506 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00400 | Loss 0.6326 | Train Acc. 0.6532 | Validation Acc. 0.8506 \n",
      "Epoch 00405 | Loss 0.6346 | Train Acc. 0.6647 | Validation Acc. 0.8563 \n",
      "Epoch 00410 | Loss 0.6459 | Train Acc. 0.6647 | Validation Acc. 0.8563 \n",
      "Epoch 00415 | Loss 0.6712 | Train Acc. 0.6532 | Validation Acc. 0.8391 \n",
      "Epoch 00420 | Loss 0.6445 | Train Acc. 0.6662 | Validation Acc. 0.8391 \n",
      "Epoch 00425 | Loss 0.6234 | Train Acc. 0.6864 | Validation Acc. 0.8448 \n",
      "Epoch 00430 | Loss 0.6490 | Train Acc. 0.6488 | Validation Acc. 0.8563 \n",
      "Epoch 00435 | Loss 0.6713 | Train Acc. 0.6329 | Validation Acc. 0.8506 \n",
      "Epoch 00440 | Loss 0.6446 | Train Acc. 0.6561 | Validation Acc. 0.8391 \n",
      "Epoch 00445 | Loss 0.6298 | Train Acc. 0.6662 | Validation Acc. 0.8391 \n",
      "Epoch 00450 | Loss 0.6612 | Train Acc. 0.6647 | Validation Acc. 0.8391 \n",
      "Epoch 00455 | Loss 0.6187 | Train Acc. 0.6705 | Validation Acc. 0.8391 \n",
      "Epoch 00460 | Loss 0.6188 | Train Acc. 0.6676 | Validation Acc. 0.8448 \n",
      "Epoch 00465 | Loss 0.6251 | Train Acc. 0.6633 | Validation Acc. 0.8448 \n",
      "Epoch 00470 | Loss 0.6710 | Train Acc. 0.6604 | Validation Acc. 0.8448 \n",
      "Epoch 00475 | Loss 0.6829 | Train Acc. 0.6344 | Validation Acc. 0.8506 \n",
      "Epoch 00480 | Loss 0.6153 | Train Acc. 0.6821 | Validation Acc. 0.8563 \n",
      "Epoch 00485 | Loss 0.6179 | Train Acc. 0.6561 | Validation Acc. 0.8563 \n",
      "Epoch 00490 | Loss 0.6548 | Train Acc. 0.6590 | Validation Acc. 0.8506 \n",
      "Epoch 00495 | Loss 0.6878 | Train Acc. 0.6286 | Validation Acc. 0.8391 \n",
      "Epoch 00500 | Loss 0.6248 | Train Acc. 0.6676 | Validation Acc. 0.8391 \n",
      "Epoch 00505 | Loss 0.6121 | Train Acc. 0.6821 | Validation Acc. 0.8391 \n",
      "Epoch 00510 | Loss 0.6445 | Train Acc. 0.6618 | Validation Acc. 0.8391 \n",
      "Epoch 00515 | Loss 0.6645 | Train Acc. 0.6387 | Validation Acc. 0.8506 \n",
      "Epoch 00520 | Loss 0.6345 | Train Acc. 0.6749 | Validation Acc. 0.8563 \n",
      "Epoch 00525 | Loss 0.6309 | Train Acc. 0.6590 | Validation Acc. 0.8563 \n",
      "Epoch 00530 | Loss 0.6629 | Train Acc. 0.6546 | Validation Acc. 0.8563 \n",
      "Epoch 00535 | Loss 0.6233 | Train Acc. 0.6734 | Validation Acc. 0.8506 \n",
      "Epoch 00540 | Loss 0.6226 | Train Acc. 0.6749 | Validation Acc. 0.8506 \n",
      "Epoch 00545 | Loss 0.6856 | Train Acc. 0.6286 | Validation Acc. 0.8506 \n",
      "Epoch 00550 | Loss 0.6060 | Train Acc. 0.6734 | Validation Acc. 0.8563 \n",
      "Epoch 00555 | Loss 0.6733 | Train Acc. 0.6590 | Validation Acc. 0.8563 \n",
      "Epoch 00560 | Loss 0.6248 | Train Acc. 0.6676 | Validation Acc. 0.8563 \n",
      "Epoch 00565 | Loss 0.6294 | Train Acc. 0.6561 | Validation Acc. 0.8563 \n",
      "Epoch 00570 | Loss 0.6231 | Train Acc. 0.6763 | Validation Acc. 0.8563 \n",
      "Epoch 00575 | Loss 0.6852 | Train Acc. 0.6358 | Validation Acc. 0.8563 \n",
      "Epoch 00580 | Loss 0.6275 | Train Acc. 0.6604 | Validation Acc. 0.8563 \n",
      "Epoch 00585 | Loss 0.6083 | Train Acc. 0.6676 | Validation Acc. 0.8563 \n",
      "Epoch 00590 | Loss 0.6346 | Train Acc. 0.6662 | Validation Acc. 0.8563 \n",
      "Epoch 00595 | Loss 0.6178 | Train Acc. 0.6749 | Validation Acc. 0.8563 \n",
      "Epoch 00600 | Loss 0.6464 | Train Acc. 0.6416 | Validation Acc. 0.8563 \n",
      "Epoch 00605 | Loss 0.6678 | Train Acc. 0.6416 | Validation Acc. 0.8563 \n",
      "Epoch 00610 | Loss 0.6658 | Train Acc. 0.6402 | Validation Acc. 0.8563 \n",
      "Epoch 00615 | Loss 0.5911 | Train Acc. 0.6792 | Validation Acc. 0.8563 \n",
      "Epoch 00620 | Loss 0.6485 | Train Acc. 0.6402 | Validation Acc. 0.8563 \n",
      "Epoch 00625 | Loss 0.6157 | Train Acc. 0.6777 | Validation Acc. 0.8563 \n",
      "Epoch 00630 | Loss 0.6679 | Train Acc. 0.6618 | Validation Acc. 0.8563 \n",
      "Epoch 00635 | Loss 0.6355 | Train Acc. 0.6720 | Validation Acc. 0.8563 \n",
      "Epoch 00640 | Loss 0.6552 | Train Acc. 0.6503 | Validation Acc. 0.8563 \n",
      "Epoch 00645 | Loss 0.6197 | Train Acc. 0.6720 | Validation Acc. 0.8563 \n",
      "Epoch 00650 | Loss 0.6202 | Train Acc. 0.6618 | Validation Acc. 0.8563 \n",
      "Epoch 00655 | Loss 0.6730 | Train Acc. 0.6315 | Validation Acc. 0.8563 \n",
      "Epoch 00660 | Loss 0.6641 | Train Acc. 0.6618 | Validation Acc. 0.8563 \n",
      "Epoch 00665 | Loss 0.6048 | Train Acc. 0.6662 | Validation Acc. 0.8563 \n",
      "Epoch 00670 | Loss 0.6841 | Train Acc. 0.6257 | Validation Acc. 0.8563 \n",
      "Epoch 00675 | Loss 0.6513 | Train Acc. 0.6633 | Validation Acc. 0.8563 \n",
      "Epoch 00680 | Loss 0.6475 | Train Acc. 0.6604 | Validation Acc. 0.8563 \n",
      "Epoch 00685 | Loss 0.6938 | Train Acc. 0.6373 | Validation Acc. 0.8563 \n",
      "Epoch 00690 | Loss 0.6587 | Train Acc. 0.6431 | Validation Acc. 0.8563 \n",
      "Epoch 00695 | Loss 0.6357 | Train Acc. 0.6734 | Validation Acc. 0.8506 \n",
      "Epoch 00700 | Loss 0.6274 | Train Acc. 0.6850 | Validation Acc. 0.8506 \n",
      "Epoch 00705 | Loss 0.6640 | Train Acc. 0.6358 | Validation Acc. 0.8506 \n",
      "Epoch 00710 | Loss 0.6731 | Train Acc. 0.6431 | Validation Acc. 0.8506 \n",
      "Epoch 00715 | Loss 0.6674 | Train Acc. 0.6431 | Validation Acc. 0.8506 \n",
      "Epoch 00720 | Loss 0.6388 | Train Acc. 0.6618 | Validation Acc. 0.8506 \n",
      "Epoch 00725 | Loss 0.6134 | Train Acc. 0.6561 | Validation Acc. 0.8506 \n",
      "Epoch 00730 | Loss 0.6077 | Train Acc. 0.6864 | Validation Acc. 0.8506 \n",
      "Epoch 00735 | Loss 0.6357 | Train Acc. 0.6373 | Validation Acc. 0.8506 \n",
      "Epoch 00740 | Loss 0.6377 | Train Acc. 0.6431 | Validation Acc. 0.8506 \n",
      "Epoch 00745 | Loss 0.6756 | Train Acc. 0.6387 | Validation Acc. 0.8506 \n",
      "Epoch 00750 | Loss 0.6863 | Train Acc. 0.6373 | Validation Acc. 0.8506 \n",
      "Epoch 00755 | Loss 0.6343 | Train Acc. 0.6503 | Validation Acc. 0.8506 \n",
      "Epoch 00760 | Loss 0.6238 | Train Acc. 0.6734 | Validation Acc. 0.8506 \n",
      "Epoch 00765 | Loss 0.6246 | Train Acc. 0.6676 | Validation Acc. 0.8506 \n",
      "Epoch 00770 | Loss 0.6346 | Train Acc. 0.6575 | Validation Acc. 0.8506 \n",
      "Epoch 00775 | Loss 0.6828 | Train Acc. 0.6431 | Validation Acc. 0.8506 \n",
      "Epoch 00780 | Loss 0.6702 | Train Acc. 0.6460 | Validation Acc. 0.8506 \n",
      "Epoch 00785 | Loss 0.6212 | Train Acc. 0.6951 | Validation Acc. 0.8506 \n",
      "Epoch 00790 | Loss 0.6510 | Train Acc. 0.6546 | Validation Acc. 0.8506 \n",
      "Epoch 00795 | Loss 0.6544 | Train Acc. 0.6561 | Validation Acc. 0.8506 \n",
      "Epoch 00800 | Loss 0.6429 | Train Acc. 0.6763 | Validation Acc. 0.8506 \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 1 | Test Accuracy = 0.8618 | F1 = 0.8382 \n",
      "Total = 6.4Gb \t Reserved = 4.6Gb \t Allocated = 1.8Gb\n",
      "Clearing gpu memory\n",
      "Total = 6.4Gb \t Reserved = 2.1Gb \t Allocated = 1.8Gb\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=1601, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=200000, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gcnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph with 1083 nodes and 10426 edges\n",
      "Epoch 00000 | Loss 1.6019 | Train Acc. 0.2225 | Validation Acc. 0.1954 \n",
      "Epoch 00005 | Loss 1.3986 | Train Acc. 0.5014 | Validation Acc. 0.1954 \n",
      "Epoch 00010 | Loss 1.2299 | Train Acc. 0.5289 | Validation Acc. 0.1954 \n",
      "Epoch 00015 | Loss 1.1187 | Train Acc. 0.5217 | Validation Acc. 0.8506 \n",
      "Epoch 00020 | Loss 0.9892 | Train Acc. 0.5694 | Validation Acc. 0.4023 \n",
      "Epoch 00025 | Loss 0.9319 | Train Acc. 0.5824 | Validation Acc. 0.5920 \n",
      "Epoch 00030 | Loss 0.8628 | Train Acc. 0.6127 | Validation Acc. 0.7414 \n",
      "Epoch 00035 | Loss 0.8420 | Train Acc. 0.6272 | Validation Acc. 0.6897 \n",
      "Epoch 00040 | Loss 0.8099 | Train Acc. 0.6214 | Validation Acc. 0.6437 \n",
      "Epoch 00045 | Loss 0.7919 | Train Acc. 0.6243 | Validation Acc. 0.8276 \n",
      "Epoch 00050 | Loss 0.7791 | Train Acc. 0.6301 | Validation Acc. 0.5172 \n",
      "Epoch 00055 | Loss 0.7581 | Train Acc. 0.6373 | Validation Acc. 0.6207 \n",
      "Epoch 00060 | Loss 0.7226 | Train Acc. 0.6561 | Validation Acc. 0.8678 \n",
      "Epoch 00065 | Loss 0.7708 | Train Acc. 0.6127 | Validation Acc. 0.8678 \n",
      "Epoch 00070 | Loss 0.7328 | Train Acc. 0.6286 | Validation Acc. 0.8563 \n",
      "Epoch 00075 | Loss 0.6887 | Train Acc. 0.6662 | Validation Acc. 0.3736 \n",
      "Epoch 00080 | Loss 0.7323 | Train Acc. 0.6286 | Validation Acc. 0.3793 \n",
      "Epoch 00085 | Loss 0.7352 | Train Acc. 0.6113 | Validation Acc. 0.5345 \n",
      "Epoch 00090 | Loss 0.6994 | Train Acc. 0.6416 | Validation Acc. 0.8103 \n",
      "Epoch 00095 | Loss 0.7210 | Train Acc. 0.6243 | Validation Acc. 0.5172 \n",
      "Epoch 00100 | Loss 0.6708 | Train Acc. 0.6561 | Validation Acc. 0.5230 \n",
      "Epoch 00105 | Loss 0.6907 | Train Acc. 0.6488 | Validation Acc. 0.5977 \n",
      "Epoch 00110 | Loss 0.7118 | Train Acc. 0.6243 | Validation Acc. 0.8563 \n",
      "Epoch 00115 | Loss 0.6809 | Train Acc. 0.6532 | Validation Acc. 0.8851 \n",
      "Epoch 00120 | Loss 0.6552 | Train Acc. 0.6532 | Validation Acc. 0.7471 \n",
      "Epoch 00125 | Loss 0.6662 | Train Acc. 0.6662 | Validation Acc. 0.6724 \n",
      "Epoch 00130 | Loss 0.6816 | Train Acc. 0.6445 | Validation Acc. 0.7874 \n",
      "Epoch 00135 | Loss 0.6751 | Train Acc. 0.6561 | Validation Acc. 0.4885 \n",
      "Epoch 00140 | Loss 0.6811 | Train Acc. 0.6488 | Validation Acc. 0.7529 \n",
      "Epoch 00145 | Loss 0.6699 | Train Acc. 0.6517 | Validation Acc. 0.7644 \n",
      "Epoch 00150 | Loss 0.6530 | Train Acc. 0.6474 | Validation Acc. 0.8276 \n",
      "Epoch 00155 | Loss 0.6948 | Train Acc. 0.6373 | Validation Acc. 0.8448 \n",
      "Epoch 00160 | Loss 0.6271 | Train Acc. 0.6763 | Validation Acc. 0.7586 \n",
      "Epoch 00165 | Loss 0.6832 | Train Acc. 0.6387 | Validation Acc. 0.8103 \n",
      "Epoch 00170 | Loss 0.6801 | Train Acc. 0.6344 | Validation Acc. 0.6954 \n",
      "Epoch 00175 | Loss 0.6652 | Train Acc. 0.6474 | Validation Acc. 0.8736 \n",
      "Epoch 00180 | Loss 0.6950 | Train Acc. 0.6445 | Validation Acc. 0.8851 \n",
      "Epoch 00185 | Loss 0.6656 | Train Acc. 0.6517 | Validation Acc. 0.8563 \n",
      "Epoch 00190 | Loss 0.6653 | Train Acc. 0.6561 | Validation Acc. 0.7989 \n",
      "Epoch 00195 | Loss 0.6287 | Train Acc. 0.6676 | Validation Acc. 0.7414 \n",
      "Epoch 00200 | Loss 0.6357 | Train Acc. 0.6720 | Validation Acc. 0.8391 \n",
      "Epoch 00205 | Loss 0.6688 | Train Acc. 0.6604 | Validation Acc. 0.8621 \n",
      "Epoch 00210 | Loss 0.6593 | Train Acc. 0.6590 | Validation Acc. 0.8621 \n",
      "Epoch 00215 | Loss 0.6578 | Train Acc. 0.6546 | Validation Acc. 0.8851 \n",
      "Epoch 00220 | Loss 0.6467 | Train Acc. 0.6691 | Validation Acc. 0.7759 \n",
      "Epoch 00225 | Loss 0.6388 | Train Acc. 0.6532 | Validation Acc. 0.8678 \n",
      "Epoch 00230 | Loss 0.6849 | Train Acc. 0.6344 | Validation Acc. 0.8908 \n",
      "Epoch 00235 | Loss 0.6638 | Train Acc. 0.6488 | Validation Acc. 0.8506 \n",
      "Epoch 00240 | Loss 0.6158 | Train Acc. 0.6821 | Validation Acc. 0.8966 \n",
      "Epoch 00245 | Loss 0.6853 | Train Acc. 0.6272 | Validation Acc. 0.8793 \n",
      "Epoch 00250 | Loss 0.6503 | Train Acc. 0.6618 | Validation Acc. 0.8736 \n",
      "Epoch 00255 | Loss 0.6223 | Train Acc. 0.6691 | Validation Acc. 0.8736 \n",
      "Epoch 00260 | Loss 0.6638 | Train Acc. 0.6257 | Validation Acc. 0.8966 \n",
      "Epoch 00265 | Loss 0.6578 | Train Acc. 0.6402 | Validation Acc. 0.8736 \n",
      "Epoch 00270 | Loss 0.6466 | Train Acc. 0.6416 | Validation Acc. 0.8621 \n",
      "Epoch 00275 | Loss 0.6389 | Train Acc. 0.6561 | Validation Acc. 0.8218 \n",
      "Epoch 00280 | Loss 0.6680 | Train Acc. 0.6387 | Validation Acc. 0.8276 \n",
      "Epoch 00285 | Loss 0.6484 | Train Acc. 0.6749 | Validation Acc. 0.8736 \n",
      "Epoch 00290 | Loss 0.6312 | Train Acc. 0.6792 | Validation Acc. 0.8851 \n",
      "Epoch 00295 | Loss 0.6119 | Train Acc. 0.6922 | Validation Acc. 0.8793 \n",
      "Epoch 00300 | Loss 0.6752 | Train Acc. 0.6460 | Validation Acc. 0.8736 \n",
      "Epoch 00305 | Loss 0.6979 | Train Acc. 0.6474 | Validation Acc. 0.8908 \n",
      "Epoch 00310 | Loss 0.6588 | Train Acc. 0.6416 | Validation Acc. 0.8678 \n",
      "Epoch 00315 | Loss 0.6444 | Train Acc. 0.6575 | Validation Acc. 0.8793 \n",
      "Epoch 00320 | Loss 0.6218 | Train Acc. 0.6618 | Validation Acc. 0.8966 \n",
      "Epoch 00325 | Loss 0.6526 | Train Acc. 0.6734 | Validation Acc. 0.8908 \n",
      "Epoch 00330 | Loss 0.6245 | Train Acc. 0.6618 | Validation Acc. 0.8966 \n",
      "Epoch 00335 | Loss 0.6321 | Train Acc. 0.6691 | Validation Acc. 0.8966 \n",
      "Epoch 00340 | Loss 0.6196 | Train Acc. 0.6618 | Validation Acc. 0.8851 \n",
      "Epoch 00345 | Loss 0.6379 | Train Acc. 0.6821 | Validation Acc. 0.8621 \n",
      "Epoch 00350 | Loss 0.6003 | Train Acc. 0.6850 | Validation Acc. 0.8678 \n",
      "Epoch 00355 | Loss 0.6295 | Train Acc. 0.6618 | Validation Acc. 0.8736 \n",
      "Epoch 00360 | Loss 0.6723 | Train Acc. 0.6431 | Validation Acc. 0.8908 \n",
      "Epoch 00365 | Loss 0.6266 | Train Acc. 0.6662 | Validation Acc. 0.8793 \n",
      "Epoch 00370 | Loss 0.6503 | Train Acc. 0.6517 | Validation Acc. 0.8851 \n",
      "Epoch 00375 | Loss 0.6353 | Train Acc. 0.6604 | Validation Acc. 0.8966 \n",
      "Epoch 00380 | Loss 0.6307 | Train Acc. 0.6575 | Validation Acc. 0.8966 \n",
      "Epoch 00385 | Loss 0.6449 | Train Acc. 0.6575 | Validation Acc. 0.8966 \n",
      "Epoch 00390 | Loss 0.6389 | Train Acc. 0.6517 | Validation Acc. 0.8908 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00395 | Loss 0.6181 | Train Acc. 0.6647 | Validation Acc. 0.8966 \n",
      "Epoch 00400 | Loss 0.6294 | Train Acc. 0.6503 | Validation Acc. 0.8966 \n",
      "Epoch 00405 | Loss 0.6039 | Train Acc. 0.6850 | Validation Acc. 0.8966 \n",
      "Epoch 00410 | Loss 0.6337 | Train Acc. 0.6590 | Validation Acc. 0.8908 \n",
      "Epoch 00415 | Loss 0.6464 | Train Acc. 0.6416 | Validation Acc. 0.8851 \n",
      "Epoch 00420 | Loss 0.6369 | Train Acc. 0.6705 | Validation Acc. 0.8851 \n",
      "Epoch 00425 | Loss 0.6454 | Train Acc. 0.6835 | Validation Acc. 0.8851 \n",
      "Epoch 00430 | Loss 0.5811 | Train Acc. 0.7023 | Validation Acc. 0.8851 \n",
      "Epoch 00435 | Loss 0.6479 | Train Acc. 0.6329 | Validation Acc. 0.8851 \n",
      "Epoch 00440 | Loss 0.6758 | Train Acc. 0.6373 | Validation Acc. 0.8851 \n",
      "Epoch 00445 | Loss 0.6127 | Train Acc. 0.6980 | Validation Acc. 0.8851 \n",
      "Epoch 00450 | Loss 0.6836 | Train Acc. 0.6329 | Validation Acc. 0.8851 \n",
      "Epoch 00455 | Loss 0.6538 | Train Acc. 0.6474 | Validation Acc. 0.8851 \n",
      "Epoch 00460 | Loss 0.6427 | Train Acc. 0.6387 | Validation Acc. 0.8851 \n",
      "Epoch 00465 | Loss 0.6408 | Train Acc. 0.6705 | Validation Acc. 0.8851 \n",
      "Epoch 00470 | Loss 0.6456 | Train Acc. 0.6445 | Validation Acc. 0.8851 \n",
      "Epoch 00475 | Loss 0.6740 | Train Acc. 0.6503 | Validation Acc. 0.8851 \n",
      "Epoch 00480 | Loss 0.6145 | Train Acc. 0.6705 | Validation Acc. 0.8851 \n",
      "Epoch 00485 | Loss 0.6643 | Train Acc. 0.6358 | Validation Acc. 0.8851 \n",
      "Epoch 00490 | Loss 0.6625 | Train Acc. 0.6618 | Validation Acc. 0.8851 \n",
      "Epoch 00495 | Loss 0.6268 | Train Acc. 0.6792 | Validation Acc. 0.8851 \n",
      "Epoch 00500 | Loss 0.6558 | Train Acc. 0.6705 | Validation Acc. 0.8851 \n",
      "Epoch 00505 | Loss 0.6182 | Train Acc. 0.6734 | Validation Acc. 0.8851 \n",
      "Epoch 00510 | Loss 0.6380 | Train Acc. 0.6546 | Validation Acc. 0.8908 \n",
      "Epoch 00515 | Loss 0.6585 | Train Acc. 0.6633 | Validation Acc. 0.8908 \n",
      "Epoch 00520 | Loss 0.6361 | Train Acc. 0.6676 | Validation Acc. 0.8908 \n",
      "Epoch 00525 | Loss 0.6015 | Train Acc. 0.6691 | Validation Acc. 0.8908 \n",
      "Epoch 00530 | Loss 0.6237 | Train Acc. 0.6662 | Validation Acc. 0.8908 \n",
      "Epoch 00535 | Loss 0.7064 | Train Acc. 0.6228 | Validation Acc. 0.8908 \n",
      "Epoch 00540 | Loss 0.6272 | Train Acc. 0.6734 | Validation Acc. 0.8908 \n",
      "Epoch 00545 | Loss 0.6483 | Train Acc. 0.6590 | Validation Acc. 0.8908 \n",
      "Epoch 00550 | Loss 0.6864 | Train Acc. 0.6315 | Validation Acc. 0.8908 \n",
      "Epoch 00555 | Loss 0.6104 | Train Acc. 0.6879 | Validation Acc. 0.8908 \n",
      "Epoch 00560 | Loss 0.6207 | Train Acc. 0.6864 | Validation Acc. 0.8908 \n",
      "Epoch 00565 | Loss 0.6440 | Train Acc. 0.6618 | Validation Acc. 0.8908 \n",
      "Epoch 00570 | Loss 0.6514 | Train Acc. 0.6532 | Validation Acc. 0.8908 \n",
      "Epoch 00575 | Loss 0.6237 | Train Acc. 0.6763 | Validation Acc. 0.8908 \n",
      "Epoch 00580 | Loss 0.6601 | Train Acc. 0.6358 | Validation Acc. 0.8908 \n",
      "Epoch 00585 | Loss 0.6432 | Train Acc. 0.6777 | Validation Acc. 0.8908 \n",
      "Epoch 00590 | Loss 0.6403 | Train Acc. 0.6474 | Validation Acc. 0.8908 \n",
      "Epoch 00595 | Loss 0.6021 | Train Acc. 0.6777 | Validation Acc. 0.8908 \n",
      "Epoch 00600 | Loss 0.6354 | Train Acc. 0.6488 | Validation Acc. 0.8908 \n",
      "Epoch 00605 | Loss 0.6187 | Train Acc. 0.6662 | Validation Acc. 0.8908 \n",
      "Epoch 00610 | Loss 0.6409 | Train Acc. 0.6590 | Validation Acc. 0.8908 \n",
      "Epoch 00615 | Loss 0.6193 | Train Acc. 0.6488 | Validation Acc. 0.8908 \n",
      "Epoch 00620 | Loss 0.5960 | Train Acc. 0.6633 | Validation Acc. 0.8908 \n",
      "Epoch 00625 | Loss 0.6192 | Train Acc. 0.6734 | Validation Acc. 0.8908 \n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') # Get GPU device name, else use CPU\n",
    "print(\"Using %s device\" % device)\n",
    "get_gpu_memory()\n",
    "\n",
    "datModalities , meta = data_parsing(data_input , ['mRNA' , 'RPPA'  , 'miRNA' , 'DNAm' ] , target , index_col)\n",
    "\n",
    "graph_file = data_input + '/../Networks/' + snf_net\n",
    "g = nx.read_graphml(graph_file)\n",
    "'''g = network_from_csv('./../data/raw/mRNA_RPPA_graph.csv' , False)\n",
    "nx.set_node_attributes(g , meta.astype('category').cat.codes , 'label')\n",
    "\n",
    "graph_nodes= list(g.nodes)\n",
    "\n",
    "for node in graph_nodes : \n",
    "    if node in meta.index : \n",
    "        pass\n",
    "    else : \n",
    "        g.remove_node(node)'''\n",
    "\n",
    "meta = meta.loc[sorted(meta.index)]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5 , shuffle=True) \n",
    "\n",
    "print(skf)\n",
    "\n",
    "subjects_list = [list(set(g.nodes) & set(datModalities[mod].index)) for mod in datModalities]\n",
    "h = [torch.from_numpy(datModalities[mod].loc[subjects_list[i]].to_numpy(dtype=np.float32)).to(device) for i , mod in enumerate(datModalities) ]\n",
    "MME_input_shapes = [ datModalities[mod].shape[1] for mod in datModalities]\n",
    "\n",
    "del datModalities\n",
    "gc.collect()\n",
    "\n",
    "labels = F.one_hot(torch.Tensor(list(meta.astype('category').cat.codes)).to(torch.int64)).to(device)\n",
    "output_metrics = []\n",
    "test_logits = []\n",
    "test_labels = []\n",
    "for i, (train_index, test_index) in enumerate(skf.split(meta.index, meta)) :\n",
    "\n",
    "    model = GCN_MME(MME_input_shapes , [32 , 16 , 16, 32] , 64 , [32]  , len(meta.unique())).to(device)\n",
    "    print(model)\n",
    "    print(g)\n",
    "\n",
    "    train_index , val_index = train_test_split(\n",
    "        train_index, train_size=0.8, test_size=None, stratify=meta.iloc[train_index]\n",
    "        )\n",
    "\n",
    "    loss_plot = train(g, h , subjects_list , train_index , val_index , device ,  model , labels , 2000 , 1e-3 , 100)\n",
    "    plt.title(f'Loss for split {i}')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    test_output_metrics = evaluate(test_index , device , g , h , subjects_list , model , labels )\n",
    "\n",
    "    print(\n",
    "        \"Fold : {:01d} | Test Accuracy = {:.4f} | F1 = {:.4f} \".format(\n",
    "        i+1 , test_output_metrics[1] , test_output_metrics[2] )\n",
    "    )\n",
    "    \n",
    "    test_logits.extend(test_output_metrics[-1][test_index])\n",
    "    test_labels.extend(labels[test_index])\n",
    "    \n",
    "    output_metrics.append(test_output_metrics)\n",
    "    if i == 0 : \n",
    "        best_model = model\n",
    "        best_idx = i\n",
    "    elif output_metrics[best_idx][1] < test_output_metrics[1] : \n",
    "        best_model = model\n",
    "        best_idx   = i\n",
    "\n",
    "    get_gpu_memory()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print('Clearing gpu memory')\n",
    "    get_gpu_memory()\n",
    "\n",
    "test_logits = torch.stack(test_logits)\n",
    "test_labels = torch.stack(test_labels)\n",
    "    \n",
    "accuracy = []\n",
    "F1 = []\n",
    "i = 0\n",
    "for metric in output_metrics :\n",
    "    \n",
    "    accuracy.append(metric[1])\n",
    "    F1.append(metric[2])\n",
    "\n",
    "\n",
    "print(\"%i Fold Cross Validation Accuracy = %2.2f \\u00B1 %2.2f\" %(5 , np.mean(accuracy)*100 , np.std(accuracy)*100))\n",
    "print(\"%i Fold Cross Validation F1 = %2.2f \\u00B1 %2.2f\" %(5 , np.mean(F1)*100 , np.std(F1)*100))\n",
    "\n",
    "confusion_matrix(test_logits , test_labels , meta.astype('category').cat.categories)\n",
    "plt.title('Test Accuracy = %2.1f %%' % (np.mean(accuracy)*100))\n",
    "\n",
    "precision_recall_plot , all_predictions_conf = AUROC(test_logits, test_labels , meta)\n",
    "\n",
    "node_predictions = []\n",
    "display_label = meta.astype('category').cat.categories\n",
    "for pred in all_predictions_conf.argmax(1)  : \n",
    "    node_predictions.append(display_label[pred])\n",
    "\n",
    "tst = pd.DataFrame({'Actual' : meta.loc[list(nx.get_node_attributes(g, 'idx').keys())] , 'Predicted' : node_predictions})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
