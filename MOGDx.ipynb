{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d5186b",
   "metadata": {},
   "source": [
    "# MOGDx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3032ad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Library Import \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys  \n",
    "sys.path.insert(0, './MAIN/')\n",
    "from utils import *\n",
    "from GNN_MME import *\n",
    "from train import *\n",
    "import preprocess_functions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import networkx as nx\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Finished Library Import \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4967dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = './../data/raw'\n",
    "snf_net = 'mRNA_RPPA_DNAm_graph.graphml'\n",
    "index_col = 'index'\n",
    "target = 'paper_BRCA_Subtype_PAM50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee28437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Total = 6.4Gb \t Reserved = 0.0Gb \t Allocated = 0.0Gb\n",
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=200000, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gcnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph with 1083 nodes and 10380 edges\n",
      "Epoch 00000 | Loss 1.6017 | Train Acc. 0.2428 | Validation Acc. 0.0345 \n",
      "Epoch 00005 | Loss 1.3830 | Train Acc. 0.5275 | Validation Acc. 0.6322 \n",
      "Epoch 00010 | Loss 1.2176 | Train Acc. 0.5448 | Validation Acc. 0.4253 \n",
      "Epoch 00015 | Loss 1.0303 | Train Acc. 0.5535 | Validation Acc. 0.4483 \n",
      "Epoch 00020 | Loss 0.9927 | Train Acc. 0.5491 | Validation Acc. 0.6264 \n",
      "Epoch 00025 | Loss 1.0034 | Train Acc. 0.5491 | Validation Acc. 0.6552 \n",
      "Epoch 00030 | Loss 0.8901 | Train Acc. 0.5954 | Validation Acc. 0.5172 \n",
      "Epoch 00035 | Loss 0.8444 | Train Acc. 0.6243 | Validation Acc. 0.5172 \n",
      "Epoch 00040 | Loss 0.8360 | Train Acc. 0.6286 | Validation Acc. 0.5172 \n",
      "Epoch 00045 | Loss 0.8040 | Train Acc. 0.6329 | Validation Acc. 0.5172 \n",
      "Epoch 00050 | Loss 0.7946 | Train Acc. 0.6156 | Validation Acc. 0.5172 \n",
      "Epoch 00055 | Loss 0.7923 | Train Acc. 0.6257 | Validation Acc. 0.5632 \n",
      "Epoch 00060 | Loss 0.7757 | Train Acc. 0.6488 | Validation Acc. 0.6494 \n",
      "Epoch 00065 | Loss 0.7805 | Train Acc. 0.6272 | Validation Acc. 0.6552 \n",
      "Epoch 00070 | Loss 0.7565 | Train Acc. 0.6488 | Validation Acc. 0.5690 \n",
      "Epoch 00075 | Loss 0.7725 | Train Acc. 0.6069 | Validation Acc. 0.6494 \n",
      "Epoch 00080 | Loss 0.7466 | Train Acc. 0.6127 | Validation Acc. 0.7011 \n",
      "Epoch 00085 | Loss 0.7345 | Train Acc. 0.6431 | Validation Acc. 0.6724 \n",
      "Epoch 00090 | Loss 0.7344 | Train Acc. 0.6460 | Validation Acc. 0.6724 \n",
      "Epoch 00095 | Loss 0.7289 | Train Acc. 0.6171 | Validation Acc. 0.6782 \n",
      "Epoch 00100 | Loss 0.7547 | Train Acc. 0.6315 | Validation Acc. 0.6724 \n",
      "Epoch 00105 | Loss 0.6980 | Train Acc. 0.6618 | Validation Acc. 0.8448 \n",
      "Epoch 00110 | Loss 0.7207 | Train Acc. 0.6257 | Validation Acc. 0.8678 \n",
      "Epoch 00115 | Loss 0.7261 | Train Acc. 0.6532 | Validation Acc. 0.8563 \n",
      "Epoch 00120 | Loss 0.7389 | Train Acc. 0.6402 | Validation Acc. 0.7126 \n",
      "Epoch 00125 | Loss 0.6734 | Train Acc. 0.6647 | Validation Acc. 0.6609 \n",
      "Epoch 00130 | Loss 0.6958 | Train Acc. 0.6517 | Validation Acc. 0.6437 \n",
      "Epoch 00135 | Loss 0.6894 | Train Acc. 0.6503 | Validation Acc. 0.6092 \n",
      "Epoch 00140 | Loss 0.7128 | Train Acc. 0.6243 | Validation Acc. 0.8563 \n",
      "Epoch 00145 | Loss 0.7100 | Train Acc. 0.6488 | Validation Acc. 0.4828 \n",
      "Epoch 00150 | Loss 0.6960 | Train Acc. 0.6445 | Validation Acc. 0.4943 \n",
      "Epoch 00155 | Loss 0.7465 | Train Acc. 0.6069 | Validation Acc. 0.8851 \n",
      "Epoch 00160 | Loss 0.6895 | Train Acc. 0.6488 | Validation Acc. 0.6724 \n",
      "Epoch 00165 | Loss 0.7101 | Train Acc. 0.6532 | Validation Acc. 0.6839 \n",
      "Epoch 00170 | Loss 0.7175 | Train Acc. 0.6142 | Validation Acc. 0.6839 \n",
      "Epoch 00175 | Loss 0.7111 | Train Acc. 0.6301 | Validation Acc. 0.8046 \n",
      "Epoch 00180 | Loss 0.6591 | Train Acc. 0.6763 | Validation Acc. 0.8161 \n",
      "Epoch 00185 | Loss 0.6821 | Train Acc. 0.6488 | Validation Acc. 0.8391 \n",
      "Epoch 00190 | Loss 0.6857 | Train Acc. 0.6257 | Validation Acc. 0.7586 \n",
      "Epoch 00195 | Loss 0.6976 | Train Acc. 0.6402 | Validation Acc. 0.8563 \n",
      "Epoch 00200 | Loss 0.6681 | Train Acc. 0.6546 | Validation Acc. 0.8621 \n",
      "Epoch 00205 | Loss 0.7175 | Train Acc. 0.6301 | Validation Acc. 0.8793 \n",
      "Epoch 00210 | Loss 0.7015 | Train Acc. 0.6358 | Validation Acc. 0.8621 \n",
      "Epoch 00215 | Loss 0.6832 | Train Acc. 0.6402 | Validation Acc. 0.8103 \n",
      "Epoch 00220 | Loss 0.7045 | Train Acc. 0.6228 | Validation Acc. 0.7989 \n",
      "Epoch 00225 | Loss 0.7232 | Train Acc. 0.6373 | Validation Acc. 0.7759 \n",
      "Epoch 00230 | Loss 0.6966 | Train Acc. 0.6358 | Validation Acc. 0.6667 \n",
      "Epoch 00235 | Loss 0.6778 | Train Acc. 0.6532 | Validation Acc. 0.7701 \n",
      "Epoch 00240 | Loss 0.6745 | Train Acc. 0.6676 | Validation Acc. 0.8276 \n",
      "Epoch 00245 | Loss 0.6715 | Train Acc. 0.6532 | Validation Acc. 0.8563 \n",
      "Epoch 00250 | Loss 0.6678 | Train Acc. 0.6488 | Validation Acc. 0.7816 \n",
      "Epoch 00255 | Loss 0.6890 | Train Acc. 0.6431 | Validation Acc. 0.8621 \n",
      "Epoch 00260 | Loss 0.7212 | Train Acc. 0.6055 | Validation Acc. 0.8563 \n",
      "Epoch 00265 | Loss 0.6486 | Train Acc. 0.6532 | Validation Acc. 0.8563 \n",
      "Epoch 00270 | Loss 0.6876 | Train Acc. 0.6488 | Validation Acc. 0.8678 \n",
      "Epoch 00275 | Loss 0.7011 | Train Acc. 0.6315 | Validation Acc. 0.8736 \n",
      "Epoch 00280 | Loss 0.7008 | Train Acc. 0.6387 | Validation Acc. 0.8563 \n",
      "Epoch 00285 | Loss 0.6590 | Train Acc. 0.6431 | Validation Acc. 0.8621 \n",
      "Epoch 00290 | Loss 0.6676 | Train Acc. 0.6590 | Validation Acc. 0.8506 \n",
      "Epoch 00295 | Loss 0.6952 | Train Acc. 0.6329 | Validation Acc. 0.7759 \n",
      "Epoch 00300 | Loss 0.6653 | Train Acc. 0.6488 | Validation Acc. 0.7586 \n",
      "Epoch 00305 | Loss 0.7035 | Train Acc. 0.6416 | Validation Acc. 0.7989 \n",
      "Epoch 00310 | Loss 0.6877 | Train Acc. 0.6358 | Validation Acc. 0.8621 \n",
      "Epoch 00315 | Loss 0.6655 | Train Acc. 0.6561 | Validation Acc. 0.8506 \n",
      "Epoch 00320 | Loss 0.6351 | Train Acc. 0.6879 | Validation Acc. 0.8563 \n",
      "Epoch 00325 | Loss 0.6562 | Train Acc. 0.6763 | Validation Acc. 0.8621 \n",
      "Epoch 00330 | Loss 0.6495 | Train Acc. 0.6604 | Validation Acc. 0.8736 \n",
      "Epoch 00335 | Loss 0.6623 | Train Acc. 0.6503 | Validation Acc. 0.8621 \n",
      "Epoch 00340 | Loss 0.6840 | Train Acc. 0.6431 | Validation Acc. 0.8506 \n",
      "Epoch 00345 | Loss 0.7082 | Train Acc. 0.6431 | Validation Acc. 0.8563 \n",
      "Epoch 00350 | Loss 0.6844 | Train Acc. 0.6358 | Validation Acc. 0.8563 \n",
      "Epoch 00355 | Loss 0.6893 | Train Acc. 0.6373 | Validation Acc. 0.8563 \n",
      "Epoch 00360 | Loss 0.6825 | Train Acc. 0.6431 | Validation Acc. 0.8621 \n",
      "Epoch 00365 | Loss 0.6653 | Train Acc. 0.6358 | Validation Acc. 0.8621 \n",
      "Epoch 00370 | Loss 0.6687 | Train Acc. 0.6618 | Validation Acc. 0.8563 \n",
      "Epoch 00375 | Loss 0.6812 | Train Acc. 0.6460 | Validation Acc. 0.8506 \n",
      "Epoch 00380 | Loss 0.6819 | Train Acc. 0.6720 | Validation Acc. 0.8736 \n",
      "Epoch 00385 | Loss 0.6627 | Train Acc. 0.6618 | Validation Acc. 0.8678 \n",
      "Epoch 00390 | Loss 0.7316 | Train Acc. 0.6199 | Validation Acc. 0.8563 \n",
      "Epoch 00395 | Loss 0.6586 | Train Acc. 0.6575 | Validation Acc. 0.8563 \n",
      "Epoch 00400 | Loss 0.7063 | Train Acc. 0.6171 | Validation Acc. 0.8678 \n",
      "Epoch 00405 | Loss 0.6625 | Train Acc. 0.6763 | Validation Acc. 0.8621 \n",
      "Epoch 00410 | Loss 0.6683 | Train Acc. 0.6575 | Validation Acc. 0.8621 \n",
      "Epoch 00415 | Loss 0.6976 | Train Acc. 0.6243 | Validation Acc. 0.8621 \n",
      "Epoch 00420 | Loss 0.6500 | Train Acc. 0.6618 | Validation Acc. 0.8621 \n",
      "Epoch 00425 | Loss 0.6914 | Train Acc. 0.6546 | Validation Acc. 0.8621 \n",
      "Epoch 00430 | Loss 0.6944 | Train Acc. 0.6315 | Validation Acc. 0.8621 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00435 | Loss 0.6793 | Train Acc. 0.6575 | Validation Acc. 0.8621 \n",
      "Epoch 00440 | Loss 0.7156 | Train Acc. 0.6358 | Validation Acc. 0.8621 \n",
      "Epoch 00445 | Loss 0.6543 | Train Acc. 0.6474 | Validation Acc. 0.8621 \n",
      "Epoch 00450 | Loss 0.6476 | Train Acc. 0.6720 | Validation Acc. 0.8621 \n",
      "Epoch 00455 | Loss 0.6657 | Train Acc. 0.6532 | Validation Acc. 0.8621 \n",
      "Epoch 00460 | Loss 0.6763 | Train Acc. 0.6517 | Validation Acc. 0.8621 \n",
      "Epoch 00465 | Loss 0.6754 | Train Acc. 0.6387 | Validation Acc. 0.8621 \n",
      "Epoch 00470 | Loss 0.6477 | Train Acc. 0.6503 | Validation Acc. 0.8563 \n",
      "Epoch 00475 | Loss 0.6951 | Train Acc. 0.6416 | Validation Acc. 0.8563 \n",
      "Epoch 00480 | Loss 0.6805 | Train Acc. 0.6445 | Validation Acc. 0.8563 \n",
      "Epoch 00485 | Loss 0.6481 | Train Acc. 0.6763 | Validation Acc. 0.8621 \n",
      "Epoch 00490 | Loss 0.6469 | Train Acc. 0.6662 | Validation Acc. 0.8678 \n",
      "Epoch 00495 | Loss 0.6668 | Train Acc. 0.6402 | Validation Acc. 0.8621 \n",
      "Epoch 00500 | Loss 0.7113 | Train Acc. 0.6301 | Validation Acc. 0.8678 \n",
      "Epoch 00505 | Loss 0.6469 | Train Acc. 0.6691 | Validation Acc. 0.8621 \n",
      "Epoch 00510 | Loss 0.6845 | Train Acc. 0.6460 | Validation Acc. 0.8563 \n",
      "Epoch 00515 | Loss 0.6531 | Train Acc. 0.6604 | Validation Acc. 0.8621 \n",
      "Epoch 00520 | Loss 0.6711 | Train Acc. 0.6647 | Validation Acc. 0.8621 \n",
      "Epoch 00525 | Loss 0.6941 | Train Acc. 0.6546 | Validation Acc. 0.8621 \n",
      "Epoch 00530 | Loss 0.6762 | Train Acc. 0.6749 | Validation Acc. 0.8621 \n",
      "Epoch 00535 | Loss 0.6874 | Train Acc. 0.6777 | Validation Acc. 0.8621 \n",
      "Epoch 00540 | Loss 0.6619 | Train Acc. 0.6647 | Validation Acc. 0.8621 \n",
      "Epoch 00545 | Loss 0.6954 | Train Acc. 0.6387 | Validation Acc. 0.8621 \n",
      "Epoch 00550 | Loss 0.6921 | Train Acc. 0.6387 | Validation Acc. 0.8621 \n",
      "Epoch 00555 | Loss 0.6639 | Train Acc. 0.6503 | Validation Acc. 0.8621 \n",
      "Epoch 00560 | Loss 0.6398 | Train Acc. 0.6662 | Validation Acc. 0.8621 \n",
      "Epoch 00565 | Loss 0.7068 | Train Acc. 0.6358 | Validation Acc. 0.8621 \n",
      "Epoch 00570 | Loss 0.6716 | Train Acc. 0.6546 | Validation Acc. 0.8621 \n",
      "Epoch 00575 | Loss 0.6730 | Train Acc. 0.6676 | Validation Acc. 0.8621 \n",
      "Epoch 00580 | Loss 0.6976 | Train Acc. 0.6257 | Validation Acc. 0.8621 \n",
      "Epoch 00585 | Loss 0.6714 | Train Acc. 0.6517 | Validation Acc. 0.8621 \n",
      "Epoch 00590 | Loss 0.6662 | Train Acc. 0.6647 | Validation Acc. 0.8621 \n",
      "Epoch 00595 | Loss 0.6694 | Train Acc. 0.6676 | Validation Acc. 0.8621 \n",
      "Epoch 00600 | Loss 0.6710 | Train Acc. 0.6546 | Validation Acc. 0.8621 \n",
      "Epoch 00605 | Loss 0.6982 | Train Acc. 0.6416 | Validation Acc. 0.8621 \n",
      "Epoch 00610 | Loss 0.6894 | Train Acc. 0.6517 | Validation Acc. 0.8621 \n",
      "Epoch 00615 | Loss 0.7098 | Train Acc. 0.6517 | Validation Acc. 0.8621 \n",
      "Epoch 00620 | Loss 0.6831 | Train Acc. 0.6474 | Validation Acc. 0.8621 \n",
      "Epoch 00625 | Loss 0.6153 | Train Acc. 0.6893 | Validation Acc. 0.8621 \n",
      "Epoch 00630 | Loss 0.6473 | Train Acc. 0.6720 | Validation Acc. 0.8621 \n",
      "Epoch 00635 | Loss 0.6676 | Train Acc. 0.6517 | Validation Acc. 0.8621 \n",
      "Epoch 00640 | Loss 0.6774 | Train Acc. 0.6604 | Validation Acc. 0.8621 \n",
      "Epoch 00645 | Loss 0.6651 | Train Acc. 0.6590 | Validation Acc. 0.8621 \n",
      "Epoch 00650 | Loss 0.6768 | Train Acc. 0.6445 | Validation Acc. 0.8621 \n",
      "Epoch 00655 | Loss 0.6745 | Train Acc. 0.6575 | Validation Acc. 0.8621 \n",
      "Epoch 00660 | Loss 0.6632 | Train Acc. 0.6561 | Validation Acc. 0.8621 \n",
      "Epoch 00665 | Loss 0.6964 | Train Acc. 0.6488 | Validation Acc. 0.8621 \n",
      "Epoch 00670 | Loss 0.7222 | Train Acc. 0.6358 | Validation Acc. 0.8621 \n",
      "Epoch 00675 | Loss 0.6905 | Train Acc. 0.6561 | Validation Acc. 0.8621 \n",
      "Epoch 00680 | Loss 0.6531 | Train Acc. 0.6763 | Validation Acc. 0.8621 \n",
      "Epoch 00685 | Loss 0.6435 | Train Acc. 0.6618 | Validation Acc. 0.8621 \n",
      "Epoch 00690 | Loss 0.6492 | Train Acc. 0.6792 | Validation Acc. 0.8621 \n",
      "Epoch 00695 | Loss 0.6685 | Train Acc. 0.6575 | Validation Acc. 0.8621 \n",
      "Epoch 00700 | Loss 0.6951 | Train Acc. 0.6344 | Validation Acc. 0.8621 \n",
      "Epoch 00705 | Loss 0.6733 | Train Acc. 0.6561 | Validation Acc. 0.8621 \n",
      "Epoch 00710 | Loss 0.7177 | Train Acc. 0.6445 | Validation Acc. 0.8621 \n",
      "Epoch 00715 | Loss 0.6835 | Train Acc. 0.6532 | Validation Acc. 0.8621 \n",
      "Epoch 00720 | Loss 0.6767 | Train Acc. 0.6416 | Validation Acc. 0.8621 \n",
      "Epoch 00725 | Loss 0.7151 | Train Acc. 0.6329 | Validation Acc. 0.8621 \n",
      "Epoch 00730 | Loss 0.6388 | Train Acc. 0.6705 | Validation Acc. 0.8621 \n",
      "Epoch 00735 | Loss 0.6477 | Train Acc. 0.6705 | Validation Acc. 0.8621 \n",
      "Epoch 00740 | Loss 0.6880 | Train Acc. 0.6445 | Validation Acc. 0.8621 \n",
      "Epoch 00745 | Loss 0.6824 | Train Acc. 0.6431 | Validation Acc. 0.8621 \n",
      "Epoch 00750 | Loss 0.6541 | Train Acc. 0.6749 | Validation Acc. 0.8621 \n",
      "Epoch 00755 | Loss 0.6657 | Train Acc. 0.6460 | Validation Acc. 0.8621 \n",
      "Epoch 00760 | Loss 0.7085 | Train Acc. 0.6315 | Validation Acc. 0.8621 \n",
      "Epoch 00765 | Loss 0.6313 | Train Acc. 0.6749 | Validation Acc. 0.8621 \n",
      "Epoch 00770 | Loss 0.6563 | Train Acc. 0.6488 | Validation Acc. 0.8621 \n",
      "Epoch 00775 | Loss 0.6729 | Train Acc. 0.6517 | Validation Acc. 0.8621 \n",
      "Epoch 00780 | Loss 0.6762 | Train Acc. 0.6445 | Validation Acc. 0.8621 \n",
      "Epoch 00785 | Loss 0.6582 | Train Acc. 0.6720 | Validation Acc. 0.8621 \n",
      "Epoch 00790 | Loss 0.6419 | Train Acc. 0.6806 | Validation Acc. 0.8621 \n",
      "Epoch 00795 | Loss 0.7097 | Train Acc. 0.6199 | Validation Acc. 0.8621 \n",
      "Epoch 00800 | Loss 0.6588 | Train Acc. 0.6546 | Validation Acc. 0.8621 \n",
      "Epoch 00805 | Loss 0.7022 | Train Acc. 0.6532 | Validation Acc. 0.8621 \n",
      "Epoch 00810 | Loss 0.6810 | Train Acc. 0.6517 | Validation Acc. 0.8621 \n",
      "Epoch 00815 | Loss 0.6907 | Train Acc. 0.6113 | Validation Acc. 0.8621 \n",
      "Epoch 00820 | Loss 0.6731 | Train Acc. 0.6517 | Validation Acc. 0.8621 \n",
      "Epoch 00825 | Loss 0.6349 | Train Acc. 0.6720 | Validation Acc. 0.8621 \n",
      "Epoch 00830 | Loss 0.6671 | Train Acc. 0.6691 | Validation Acc. 0.8621 \n",
      "Epoch 00835 | Loss 0.6378 | Train Acc. 0.6705 | Validation Acc. 0.8621 \n",
      "Epoch 00840 | Loss 0.6720 | Train Acc. 0.6705 | Validation Acc. 0.8621 \n",
      "Epoch 00845 | Loss 0.6857 | Train Acc. 0.6315 | Validation Acc. 0.8621 \n",
      "Epoch 00850 | Loss 0.6923 | Train Acc. 0.6431 | Validation Acc. 0.8621 \n",
      "Epoch 00855 | Loss 0.6643 | Train Acc. 0.6618 | Validation Acc. 0.8621 \n",
      "Epoch 00860 | Loss 0.6575 | Train Acc. 0.6532 | Validation Acc. 0.8621 \n",
      "Epoch 00865 | Loss 0.6685 | Train Acc. 0.6517 | Validation Acc. 0.8621 \n",
      "Epoch 00870 | Loss 0.7021 | Train Acc. 0.6315 | Validation Acc. 0.8621 \n",
      "Epoch 00875 | Loss 0.6919 | Train Acc. 0.6142 | Validation Acc. 0.8621 \n",
      "Epoch 00880 | Loss 0.6440 | Train Acc. 0.6691 | Validation Acc. 0.8621 \n",
      "Epoch 00885 | Loss 0.6569 | Train Acc. 0.6763 | Validation Acc. 0.8621 \n",
      "Epoch 00890 | Loss 0.6752 | Train Acc. 0.6315 | Validation Acc. 0.8621 \n",
      "Epoch 00895 | Loss 0.6734 | Train Acc. 0.6575 | Validation Acc. 0.8621 \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 1 | Test Accuracy = 0.8387 | F1 = 0.7975 \n",
      "Total = 6.4Gb \t Reserved = 4.0Gb \t Allocated = 1.7Gb\n",
      "Clearing gpu memory\n",
      "Total = 6.4Gb \t Reserved = 1.7Gb \t Allocated = 1.7Gb\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=29995, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=32, bias=True)\n",
      "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=464, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Encoder(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=200000, out_features=500, bias=True)\n",
      "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Linear(in_features=500, out_features=16, bias=True)\n",
      "        (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gcnlayers): ModuleList(\n",
      "    (0): GraphConv(in=64, out=32, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=32, out=5, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph with 1083 nodes and 10380 edges\n",
      "Epoch 00000 | Loss 1.5811 | Train Acc. 0.2977 | Validation Acc. 0.5172 \n",
      "Epoch 00005 | Loss 1.3251 | Train Acc. 0.4451 | Validation Acc. 0.6609 \n",
      "Epoch 00010 | Loss 1.1676 | Train Acc. 0.4812 | Validation Acc. 0.1782 \n",
      "Epoch 00015 | Loss 1.0575 | Train Acc. 0.5318 | Validation Acc. 0.1782 \n",
      "Epoch 00020 | Loss 0.9983 | Train Acc. 0.5882 | Validation Acc. 0.7126 \n",
      "Epoch 00025 | Loss 0.8917 | Train Acc. 0.5968 | Validation Acc. 0.7069 \n",
      "Epoch 00030 | Loss 0.8823 | Train Acc. 0.6113 | Validation Acc. 0.5172 \n",
      "Epoch 00035 | Loss 0.8984 | Train Acc. 0.6026 | Validation Acc. 0.5172 \n",
      "Epoch 00040 | Loss 0.7909 | Train Acc. 0.6474 | Validation Acc. 0.5172 \n",
      "Epoch 00045 | Loss 0.8002 | Train Acc. 0.6272 | Validation Acc. 0.5172 \n",
      "Epoch 00050 | Loss 0.8188 | Train Acc. 0.6026 | Validation Acc. 0.5172 \n",
      "Epoch 00055 | Loss 0.7644 | Train Acc. 0.6358 | Validation Acc. 0.5172 \n",
      "Epoch 00060 | Loss 0.7880 | Train Acc. 0.6329 | Validation Acc. 0.6034 \n",
      "Epoch 00065 | Loss 0.8064 | Train Acc. 0.5968 | Validation Acc. 0.6494 \n",
      "Epoch 00070 | Loss 0.7793 | Train Acc. 0.6373 | Validation Acc. 0.6322 \n",
      "Epoch 00075 | Loss 0.7508 | Train Acc. 0.6315 | Validation Acc. 0.5862 \n",
      "Epoch 00080 | Loss 0.7852 | Train Acc. 0.6286 | Validation Acc. 0.7701 \n",
      "Epoch 00085 | Loss 0.7506 | Train Acc. 0.6373 | Validation Acc. 0.8046 \n",
      "Epoch 00090 | Loss 0.7652 | Train Acc. 0.6185 | Validation Acc. 0.6609 \n",
      "Epoch 00095 | Loss 0.7331 | Train Acc. 0.6272 | Validation Acc. 0.6437 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 37>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(g)\n\u001b[0;32m     43\u001b[0m train_index , val_index \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     44\u001b[0m     train_index, train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stratify\u001b[38;5;241m=\u001b[39mmeta\u001b[38;5;241m.\u001b[39miloc[train_index]\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 47\u001b[0m loss_plot \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubjects_list\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss for split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     49\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\OneDrive - University of Edinburgh\\PhD_Research\\Year3\\Python\\MOGDx3.0\\MOGDx\\./MAIN\\train.py:33\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(g, h, subjects_list, train_split, val_split, device, model, labels, epochs, lr, patience)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     31\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 33\u001b[0m     logits  \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubjects_list\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fcn(logits[train_split], labels[train_split]\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     37\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(logits[train_split], \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\OneDrive - University of Edinburgh\\PhD_Research\\Year3\\Python\\MOGDx3.0\\MOGDx\\./MAIN\\GNN_MME.py:92\u001b[0m, in \u001b[0;36mGCN_MME.forward\u001b[1;34m(self, g, h, subjects_list, device)\u001b[0m\n\u001b[0;32m     88\u001b[0m     node_features \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m decoded_imputed\n\u001b[0;32m     90\u001b[0m node_features \u001b[38;5;241m=\u001b[39m node_features\u001b[38;5;241m/\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mdgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_attrs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43midx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     93\u001b[0m g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m node_features\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layers \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers) : \n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dgl\\convert.py:1357\u001b[0m, in \u001b[0;36mfrom_networkx\u001b[1;34m(nx_graph, node_attrs, edge_attrs, edge_id_attr_name, idtype, device)\u001b[0m\n\u001b[0;32m   1354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nx_graph\u001b[38;5;241m.\u001b[39mis_directed():\n\u001b[0;32m   1355\u001b[0m     nx_graph \u001b[38;5;241m=\u001b[39m nx_graph\u001b[38;5;241m.\u001b[39mto_directed()\n\u001b[1;32m-> 1357\u001b[0m (sparse_fmt, arrays), urange, vrange \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraphdata2tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnx_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_id_attr_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_id_attr_name\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1361\u001b[0m g \u001b[38;5;241m=\u001b[39m create_from_edges(sparse_fmt, arrays, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_N\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_E\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_N\u001b[39m\u001b[38;5;124m\"\u001b[39m, urange, vrange)\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;66;03m# nx_graph.edges(data=True) returns src, dst, attr_dict\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dgl\\utils\\data.py:218\u001b[0m, in \u001b[0;36mgraphdata2tensors\u001b[1;34m(data, idtype, bipartite, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m         src, dst \u001b[38;5;241m=\u001b[39m networkxbipartite2tensors(\n\u001b[0;32m    211\u001b[0m             data,\n\u001b[0;32m    212\u001b[0m             idtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m             edge_id_attr_name\u001b[38;5;241m=\u001b[39medge_id_attr_name,\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m         src, dst \u001b[38;5;241m=\u001b[39m \u001b[43mnetworkx2tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_id_attr_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_id_attr_name\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     data \u001b[38;5;241m=\u001b[39m SparseAdjTuple(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m, (src, dst))\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dgl\\utils\\data.py:89\u001b[0m, in \u001b[0;36mnetworkx2tensor\u001b[1;34m(nx_graph, idtype, edge_id_attr_name)\u001b[0m\n\u001b[0;32m     86\u001b[0m     nx_graph \u001b[38;5;241m=\u001b[39m nx_graph\u001b[38;5;241m.\u001b[39mto_directed()\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Relabel nodes using consecutive integers\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m nx_graph \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_node_labels_to_integers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnx_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msorted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m has_edge_id \u001b[38;5;241m=\u001b[39m edge_id_attr_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_edge_id:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\networkx\\utils\\backends.py:412\u001b[0m, in \u001b[0;36m_dispatch.__call__\u001b[1;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backends:\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;66;03m# Fast path if no backends are installed\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\networkx\\relabel.py:281\u001b[0m, in \u001b[0;36mconvert_node_labels_to_integers\u001b[1;34m(G, first_label, ordering, label_attribute)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown node ordering: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mordering\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 281\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[43mrelabel_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# create node attribute with the old label\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_attribute \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\networkx\\utils\\backends.py:412\u001b[0m, in \u001b[0;36m_dispatch.__call__\u001b[1;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backends:\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;66;03m# Fast path if no backends are installed\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\networkx\\relabel.py:123\u001b[0m, in \u001b[0;36mrelabel_nodes\u001b[1;34m(G, mapping, copy)\u001b[0m\n\u001b[0;32m    120\u001b[0m m \u001b[38;5;241m=\u001b[39m {n: mapping(n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m G} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(mapping) \u001b[38;5;28;01melse\u001b[39;00m mapping\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_relabel_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _relabel_inplace(G, m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\networkx\\relabel.py:216\u001b[0m, in \u001b[0;36m_relabel_copy\u001b[1;34m(G, mapping)\u001b[0m\n\u001b[0;32m    214\u001b[0m     H\u001b[38;5;241m.\u001b[39madd_edges_from(new_edges)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 216\u001b[0m     \u001b[43mH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_edges_from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m H\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mupdate(G\u001b[38;5;241m.\u001b[39mgraph)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m H\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\networkx\\classes\\digraph.py:768\u001b[0m, in \u001b[0;36mDiGraph.add_edges_from\u001b[1;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_edges_from\u001b[39m(\u001b[38;5;28mself\u001b[39m, ebunch_to_add, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattr):\n\u001b[0;32m    714\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add all the edges in ebunch_to_add.\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03m    >>> G.add_edges_from(list((5, n) for n in G.nodes))\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m ebunch_to_add:\n\u001b[0;32m    769\u001b[0m         ne \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(e)\n\u001b[0;32m    770\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ne \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\networkx\\relabel.py:216\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    214\u001b[0m     H\u001b[38;5;241m.\u001b[39madd_edges_from(new_edges)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 216\u001b[0m     H\u001b[38;5;241m.\u001b[39madd_edges_from(\n\u001b[0;32m    217\u001b[0m         (mapping\u001b[38;5;241m.\u001b[39mget(n1, n1), mapping\u001b[38;5;241m.\u001b[39mget(n2, n2), d\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (n1, n2, d) \u001b[38;5;129;01min\u001b[39;00m G\u001b[38;5;241m.\u001b[39medges(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    220\u001b[0m H\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mupdate(G\u001b[38;5;241m.\u001b[39mgraph)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m H\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')# Get GPU device name, else use CPU\n",
    "print(\"Using %s device\" % device)\n",
    "get_gpu_memory()\n",
    "\n",
    "datModalities , meta = data_parsing(data_input , ['mRNA' , 'RPPA' , 'DNAm' ] , target , index_col)\n",
    "\n",
    "graph_file = data_input + '/../Networks/' + snf_net\n",
    "g = nx.read_graphml(graph_file)\n",
    "'''g = network_from_csv('./../data/raw/mRNA_RPPA_graph.csv' , False)\n",
    "nx.set_node_attributes(g , meta.astype('category').cat.codes , 'label')\n",
    "\n",
    "graph_nodes= list(g.nodes)\n",
    "\n",
    "for node in graph_nodes : \n",
    "    if node in meta.index : \n",
    "        pass\n",
    "    else : \n",
    "        g.remove_node(node)'''\n",
    "\n",
    "meta = meta.loc[sorted(meta.index)]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5 , shuffle=True) \n",
    "\n",
    "print(skf)\n",
    "\n",
    "subjects_list = [list(set(g.nodes) & set(datModalities[mod].index)) for mod in datModalities]\n",
    "h = [torch.from_numpy(datModalities[mod].loc[subjects_list[i]].to_numpy(dtype=np.float32)).to(device) for i , mod in enumerate(datModalities) ]\n",
    "MME_input_shapes = [ datModalities[mod].shape[1] for mod in datModalities]\n",
    "\n",
    "del datModalities\n",
    "gc.collect()\n",
    "\n",
    "labels = F.one_hot(torch.Tensor(list(meta.astype('category').cat.codes)).to(torch.int64)).to(device)\n",
    "output_metrics = []\n",
    "test_logits = []\n",
    "test_labels = []\n",
    "for i, (train_index, test_index) in enumerate(skf.split(meta.index, meta)) :\n",
    "\n",
    "    model = GCN_MME(MME_input_shapes , [32 , 16 , 32] , 64 , [32]  , len(meta.unique())).to(device)\n",
    "    print(model)\n",
    "    print(g)\n",
    "\n",
    "    train_index , val_index = train_test_split(\n",
    "        train_index, train_size=0.8, test_size=None, stratify=meta.iloc[train_index]\n",
    "        )\n",
    "\n",
    "    loss_plot = train(g, h , subjects_list , train_index , val_index , device ,  model , labels , 2000 , 1e-3 , 100)\n",
    "    plt.title(f'Loss for split {i}')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    test_output_metrics = evaluate(test_index , device , g , h , subjects_list , model , labels )\n",
    "\n",
    "    print(\n",
    "        \"Fold : {:01d} | Test Accuracy = {:.4f} | F1 = {:.4f} \".format(\n",
    "        i+1 , test_output_metrics[1] , test_output_metrics[2] )\n",
    "    )\n",
    "    \n",
    "    test_logits.extend(test_output_metrics[-1][test_index])\n",
    "    test_labels.extend(labels[test_index])\n",
    "    \n",
    "    output_metrics.append(test_output_metrics)\n",
    "    if i == 0 : \n",
    "        best_model = model\n",
    "        best_idx = i\n",
    "    elif output_metrics[best_idx][1] < test_output_metrics[1] : \n",
    "        best_model = model\n",
    "        best_idx   = i\n",
    "\n",
    "    get_gpu_memory()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print('Clearing gpu memory')\n",
    "    get_gpu_memory()\n",
    "\n",
    "test_logits = torch.stack(test_logits)\n",
    "test_labels = torch.stack(test_labels)\n",
    "    \n",
    "accuracy = []\n",
    "F1 = []\n",
    "i = 0\n",
    "for metric in output_metrics :\n",
    "    \n",
    "    accuracy.append(metric[1])\n",
    "    F1.append(metric[2])\n",
    "\n",
    "\n",
    "print(\"%i Fold Cross Validation Accuracy = %2.2f \\u00B1 %2.2f\" %(5 , np.mean(accuracy)*100 , np.std(accuracy)*100))\n",
    "print(\"%i Fold Cross Validation F1 = %2.2f \\u00B1 %2.2f\" %(5 , np.mean(F1)*100 , np.std(F1)*100))\n",
    "\n",
    "confusion_matrix(test_logits , test_labels , meta.astype('category').cat.categories)\n",
    "plt.title('Test Accuracy = %2.1f %%' % (np.mean(accuracy)*100))\n",
    "\n",
    "precision_recall_plot , all_predictions_conf = AUROC(test_logits, test_labels , meta)\n",
    "\n",
    "node_predictions = []\n",
    "display_label = meta.astype('category').cat.categories\n",
    "for pred in all_predictions_conf.argmax(1)  : \n",
    "    node_predictions.append(display_label[pred])\n",
    "\n",
    "tst = pd.DataFrame({'Actual' : meta.loc[list(nx.get_node_attributes(g, 'idx').keys())] , 'Predicted' : node_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ebebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
